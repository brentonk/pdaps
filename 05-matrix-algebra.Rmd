# Matrix Algebra: A Crash Course {#matrix}

*Some material in this chapter is adapted from notes [Hye Young You](https://hyeyoungyou.com) wrote for the math boot camp for the political science PhD program at Vanderbilt.*

Matrix algebra is an essential tool for understanding multivariate statistics.  You are probably already familiar with matrices, at least informally.  The data representations we have worked with so far---each row an observation, each column a variable---are formatted like matrices.

An introductory treatment of matrix algebra is a semester-long college course.  We don't have that long, or even half that long.  This chapter gives you the *bare minimum* you need to understand to get up and running with the matrix algebra we need for OLS with multiple covariates.  If you want to use advanced statistical methods in your research and haven't previously taken a matrix algebra or linear algebra course, I recommend taking some time this summer to catch up.  For example, MIT has its undergraduate linear algebra course [available online](https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/index.htm), including video lectures.


## Vector Operations

A *vector* is an ordered array.  To denote a vector $v$ of $k$ elements, we write $\mathbf{v} = (v_1, v_2, \ldots, v_k)$, or sometimes
$$
\mathbf{v} = \begin{pmatrix} v_1 \\ v_2 \\ \vdots \\ v_k \end{pmatrix}.
$$
Notice the convention of using a lowercase bold letter to denote a vector.  We will usually be dealing with vectors of real numbers.  To denote the fact that $\mathbf{v}$ is a vector of $k$ real numbers, we write $\mathbf{v} \in \mathbb{R}^k$.

A vector can be multiplied by a scalar $c \in \mathbb{R}$, producing what you would expect:
$$
c \mathbf{v} = \begin{pmatrix} c v_1 \\ c v_2 \\ \vdots \\ c v_k \end{pmatrix}
$$
You can also add and subtract two vectors of the same length.[^recycling]
$$
\begin{aligned}
\mathbf{u} + \mathbf{v} &= \begin{pmatrix}
  u_1 + v_1 \\ u_2 + v_2 \\ \vdots \\ u_k + v_k
\end{pmatrix}, \\
\mathbf{u} - \mathbf{v} &= \begin{pmatrix}
  u_1 - v_1 \\ u_2 - v_2 \\ \vdots \\ u_k - v_k
\end{pmatrix}.
\end{aligned}
$$

[^recycling]: R will let you add and subtract vectors of different lengths, via a technique called "recycling".  For example `c(1, 0) + c(1, 2, 3, 4)` will produce `c(2, 2, 4, 4)`.  This is kosher in R, but not in mathematical derivations.

A special vector is the *zero vector*, which contains---you guessed it---all zeroes.  We write $\mathbf{0}_k$ to denote the zero vector of length $k$.  When the length of the zero vector is clear from the context, we may just write $\mathbf{0}$.

The last important vector operation is the *dot product*.  The dot product of $\mathbf{u}$ and $\mathbf{v}$, written $\mathbf{u} \cdot \mathbf{v}$, is the sum of the products of the entries:
$$
\mathbf{u} \cdot \mathbf{v}
=
u_1 v_1 + u_2 v_2 + \cdots + u_k v_k
=
\sum_{m=1}^k u_m v_m.
$$

An important concept for regression analysis is the linear independence of a collection of vectors.  Let $\mathbf{v}_1, \ldots, \mathbf{v}_J$ be a collection of $J$ vectors, each of length $k$.  We call $\mathbf{u}$ a *linear combination* of $\mathbf{v}_1, \ldots, \mathbf{v}_J$ if there exist real numbers $c_1, \ldots, c_J$ such that
$$
\mathbf{u} = c_1 \mathbf{v}_1 + \cdots + c_J \mathbf{v}_J = \sum_{j=1}^J c_j \mathbf{v}_j.
$$
A collection of vectors is *linearly independent* if the only solution to
$$
c_1 \mathbf{v}_1 + \cdots + c_J \mathbf{v}_J = \mathbf{0}
$$
is $c_1 = 0, \ldots, c_J = 0$.  Otherwise, we call the vectors *linearly dependent*.  Some fun facts about linear independence:

*   If any vector in $\mathbf{v}_1, \ldots, \mathbf{v}_J$ is a linear combination of the others, then these vectors are linearly dependent.

*   A collection of $J$ vectors of length $k$ cannot be linearly independent if $J > k$.  In other words, given vectors of length $k$, the most that can be linearly independent of each other is $k$.

*   If any $\mathbf{v}_j = \mathbf{0}$, then $\mathbf{v}_1, \ldots, \mathbf{v}_J$ are linearly dependent.  (Why?)

Examples:
$$
\begin{gathered}
\mathbf{v}_1 = \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix},
\mathbf{v}_2 = \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix},
\mathbf{v}_3 = \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}; \\
\mathbf{v}_1 = \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix},
\mathbf{v}_2 = \begin{pmatrix} 14 \\ 12 \\ 0 \end{pmatrix},
\mathbf{v}_3 = \begin{pmatrix} 0 \\ -1 \\ 0 \end{pmatrix}; \\
\mathbf{v}_1 = \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix},
\mathbf{v}_2 = \begin{pmatrix} 1 \\ 4 \\ 9 \end{pmatrix},
\mathbf{v}_3 = \begin{pmatrix} 1 \\ 8 \\ 27 \end{pmatrix}.
\end{gathered}
$$


## Matrix Operations

A matrix is a two-dimensional array of numbers, with entries in rows and columns.  We call a matrix with $n$ rows and $m$ columns an $n \times m$ matrix.  For example, the following is a $2 \times 3$ matrix:
$$
\mathbf{A}
=
\begin{bmatrix}
  99 & 73 & 2 \\
  13 & 40 & 41
\end{bmatrix}
$$
Notice the convention of using an uppercase bold letter to denote a matrix.  Given a matrix $\mathbf{A}$, we usually write $a_{ij}$ to denote the entry in the $i$'th row and $j$'th column.  In the above example, we have $a_{13} = 2$.

You can think of a vector $\mathbf{v} \in \mathbb{R}^k$ as a $1 \times k$ *row matrix* or as a $k \times 1$ *column matrix*.  Throughout this book, I will treat vectors as column matrices unless otherwise noted.

Like vectors, matrices can be multipled by a scalar $c \in \mathbb{R}$.
$$
c \mathbf{A} =
\begin{bmatrix}
  c a_{11} & c a_{12} & \cdots & c a_{1m} \\
  c a_{21} & c a_{22} & \cdots & c a_{2m} \\
  \vdots & \vdots & \ddots & \vdots \\
  c a_{n1} & c a_{n2} & \cdots & c a_{nm}
\end{bmatrix}
$$

Matrices of the same dimension (i.e., both with the same number of rows $n$ and columns $m$) can be added ...
$$
\mathbf{A} + \mathbf{B} =
\begin{bmatrix}
  a_{11} + b_{11} & a_{12} + b_{12} & \cdots & a_{1m} + b_{1m} \\
  a_{21} + b_{21} & a_{22} + b_{22} & \cdots & a_{2m} + b_{2m} \\
  \vdots & \vdots & \ddots & \vdots \\
  a_{n1} + b_{n1} & a_{n2} + b_{n2} & \cdots & a_{nm} + b_{nm} \\
\end{bmatrix}
$$
... and subtracted ...
$$
\mathbf{A} - \mathbf{B} =
\begin{bmatrix}
  a_{11} - b_{11} & a_{12} - b_{12} & \cdots & a_{1m} - b_{1m} \\
  a_{21} - b_{21} & a_{22} - b_{22} & \cdots & a_{2m} - b_{2m} \\
  \vdots & \vdots & \ddots & \vdots \\
  a_{n1} - b_{n1} & a_{n2} - b_{n2} & \cdots & a_{nm} - b_{nm} \\
\end{bmatrix}
$$

Sometimes you will want to "rotate" an $n \times m$ matrix into an $m \times n$ one, so that the first row becomes the first column, the second row becomes the second column, and so on.  This is called the *transpose*.  I write the transpose of $\mathbf{A}$ as $\mathbf{A}^\top$, though you will often also see it written $\mathbf{A}'$.  For example:
$$
\mathbf{A}
=
\begin{bmatrix}
  99 & 73 & 2 \\
  13 & 40 & 41
\end{bmatrix}
\qquad
\Leftrightarrow
\qquad
\mathbf{A}^\top =
\begin{bmatrix}
  99 & 13 \\
  73 & 40 \\
  2 & 41
\end{bmatrix}
$$
Some of the most commonly invoked properties of the transpose are:
$$
\begin{aligned}
(\mathbf{A}^\top)^\top &= \mathbf{A}, \\
(c \mathbf{A})^\top &= c \mathbf{A}^\top, \\
(\mathbf{A} + \mathbf{B})^\top &= \mathbf{A}^\top + \mathbf{B}^\top, \\
(\mathbf{A} - \mathbf{B})^\top &= \mathbf{A}^\top - \mathbf{B}^\top.
\end{aligned}
$$

A matrix is *square* if it has the same number of rows as columns, i.e., it is $n \times n$.  Every matrix is special, but some kinds of square matrix are *especially* special.

*   A *symmetric* matrix is equal to its transpose: $\mathbf{A} = \mathbf{A}^\top$.  Example: $$
\begin{bmatrix}
  1 & 10 & 100 \\
  10 & 2 & 0.1 \\
  100 & 0.1 & 3
\end{bmatrix}$$

*   A *diagonal* matrix contains zeroes everywhere except along the main diagonal: if $i \neq j$, then $a_{ij} = 0$.  A diagonal matrix is symmetric by definition.  Example: $$
\begin{bmatrix}
  1 & 0 & 0 \\
  0 & 2 & 0 \\
  0 & 0 & 3
\end{bmatrix}$$

*   The $n \times n$ *identity* matrix, written $\mathbf{I}_n$ (or just $\mathbf{I}$ when the size is clear from context), is the $n \times n$ diagonal matrix where each diagonal entry is 1.  Example: $$
\begin{bmatrix}
  1 & 0 & 0 \\
  0 & 1 & 0 \\
  0 & 0 & 1
\end{bmatrix}$$

And last we come to matrix multiplication.  Whereas matrix addition and subtraction are pretty intuitive, matrix multiplication is not.  Let $\mathbf{A}$ be an $n \times m$ matrix and $\mathbf{B}$ be an $m \times p$ matrix.  (Notice that the number of columns of $\mathbf{A}$ must match the number of rows of $\mathbf{B}$.)  Then $\mathbf{C} = \mathbf{A} \mathbf{B}$ is an $n \times p$ matrix whose $ij$'th element is the dot product of the $i$'th row of $\mathbf{A}$ and the $j$'th column of $\mathbf{B}$:
$$
c_{ij} = a_{i1} b_{1j} + a_{i2} b_{2j} + \cdots + a_{im} b_{mj}.
$$
Some examples might make this more clear.
$$
\begin{gathered}
\mathbf{A} = \begin{bmatrix}
  2 & 10 \\
  0 & 1 \\
  -1 & 5
\end{bmatrix},
\mathbf{B} = \begin{bmatrix}
  1 & 4 \\
  -1 & 10
\end{bmatrix} \\
\mathbf{A} \mathbf{B} = \begin{bmatrix}
  2 \cdot 1 + 10 \cdot (-1) & 2 \cdot 4 + 10 \cdot 10 \\
  0 \cdot 1 + 1 \cdot (-1) & 0 \cdot 4 + 1 \cdot 10 \\
  (-1) \cdot 1 + 5 \cdot (-1) & (-1) \cdot 4 + 5 \cdot 10
\end{bmatrix}
= \begin{bmatrix}
  -8 & 108 \\
  -1 & 10 \\
  -6 & 46
\end{bmatrix}
\end{gathered}
$$
And here's one that you'll start seeing a lot of soon.
$$
\begin{gathered}
\mathbf{A} = \begin{bmatrix}
  1 & x_{11} & x_{12} \\
  1 & x_{21} & x_{22} \\
  & \vdots \\
  1 & x_{N1} & x_{N2}
\end{bmatrix},
\mathbf{B} = \begin{bmatrix}
  \beta_0 \\
  \beta_1 \\
  \beta_2
\end{bmatrix} \\
\mathbf{A} \mathbf{B} = \begin{bmatrix}
  \beta_0 + \beta_1 x_{11} + \beta_2 x_{12} \\
  \beta_0 + \beta_1 x_{21} + \beta_2 x_{22} \\
  \vdots \\
  \beta_0 + \beta_1 x_{N1} + \beta_2 x_{N2}
\end{bmatrix}
\end{gathered}
$$

Some important properties of matrix multiplication:

*   Matrix multiplication is associative: $(\mathbf{A} \mathbf{B}) \mathbf{C} = \mathbf{A} (\mathbf{B} \mathbf{C})$.

*   Matrix multiplication is distributive: $\mathbf{A} (\mathbf{B} + \mathbf{C}) = \mathbf{A} \mathbf{B} + \mathbf{A} \mathbf{C}$.

*   For any $n \times m$ matrix $\mathbf{A}$, we have $\mathbf{A} \mathbf{I}_m = \mathbf{I}_n \mathbf{A} = \mathbf{A}$.  In this way, the identity matrix is kind of like the matrix equivalent of the number one.  (More on this when we get to matrix inversion.)

*   Matrix multiplication is *not* commutative.  In other words, $\mathbf{A} \mathbf{B} \neq \mathbf{B} \mathbf{A}$ except in very special cases (e.g., one of them is the identity matrix).

    This is obvious when we're dealing with non-square matrices.  Let $\mathbf{A}$ be $n \times m$ and $\mathbf{B}$ be $m \times p$, so that $\mathbf{A} \mathbf{B}$ exists.  Then $\mathbf{B} \mathbf{A}$ doesn't even exist unless $n = p$.  Even then, if $n \neq m$, then $\mathbf{A} \mathbf{B}$ is $n \times n$ and $\mathbf{B} \mathbf{A}$ is $m \times m$, so they can't possibly be the same.
    
    For an example that $\mathbf{A} \mathbf{B} \neq \mathbf{B} \mathbf{A}$ even for square matrices: $$\begin{gathered}
    \mathbf{A} = \begin{bmatrix}
      1 & 0 \\
      2 & 0
    \end{bmatrix},
    \mathbf{B} = \begin{bmatrix}
      1 & 0 \\
      0 & 0
    \end{bmatrix}, \\
    \mathbf{A} \mathbf{B} = \begin{bmatrix}
      1 & 0 \\
      2 & 0
    \end{bmatrix},
    \mathbf{B} \mathbf{A} = \begin{bmatrix}
      1 & 0 \\
      0 & 0
    \end{bmatrix}.
    \end{gathered}
    $$

*   The transpose of the product is the product of the transposes ... but the other way around: $(\mathbf{A} \mathbf{B})^\top = \mathbf{B}^\top \mathbf{A}^\top$.

    This is intuitive, if you think about it.  Suppose $\mathbf{A}$ is $n \times m$ and $\mathbf{B}$ is $m \times p$.  Then $\mathbf{A} \mathbf{B}$ is $n \times p$, so $(\mathbf{A} \mathbf{B})^\top$ should be $p \times n$.  Therefore, $\mathbf{B}^\top$ must come first.


## Matrix Inversion

Coming soon!


## Solving Linear Systems

Coming soon!


## Appendix: Matrix Operations in R

Coming soon!
