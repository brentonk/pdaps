<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>13 Binary Response Models | Practical Data Analysis for Political Scientists</title>
  <meta name="description" content="These are the course notes for Brenton Kenkel’s course PSCI 8357: Statistics for Political Research II. They may or may not also be useful in their own right." />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="13 Binary Response Models | Practical Data Analysis for Political Scientists" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="These are the course notes for Brenton Kenkel’s course PSCI 8357: Statistics for Political Research II. They may or may not also be useful in their own right." />
  <meta name="github-repo" content="brentonk/pdaps" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="13 Binary Response Models | Practical Data Analysis for Political Scientists" />
  
  <meta name="twitter:description" content="These are the course notes for Brenton Kenkel’s course PSCI 8357: Statistics for Political Research II. They may or may not also be useful in their own right." />
  

<meta name="author" content="Brenton Kenkel" />


<meta name="date" content="2020-03-30" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="panel.html"/>
<link rel="next" href="references.html"/>
<script src="libs/header-attrs-2.1/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Practical Data Analysis for Political Scientists</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About This Book</a></li>
<li class="chapter" data-level="2" data-path="programming.html"><a href="programming.html"><i class="fa fa-check"></i><b>2</b> Principles of Programming</a>
<ul>
<li class="chapter" data-level="2.1" data-path="programming.html"><a href="programming.html#write-programs-for-people-not-computers"><i class="fa fa-check"></i><b>2.1</b> Write Programs for People, Not Computers</a></li>
<li class="chapter" data-level="2.2" data-path="programming.html"><a href="programming.html#let-the-computer-do-the-work"><i class="fa fa-check"></i><b>2.2</b> Let the Computer Do the Work</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>3</b> Working with Data</a>
<ul>
<li class="chapter" data-level="3.1" data-path="data.html"><a href="data.html#loading"><i class="fa fa-check"></i><b>3.1</b> Loading</a></li>
<li class="chapter" data-level="3.2" data-path="data.html"><a href="data.html#tidying"><i class="fa fa-check"></i><b>3.2</b> Tidying</a></li>
<li class="chapter" data-level="3.3" data-path="data.html"><a href="data.html#transforming-and-aggregating"><i class="fa fa-check"></i><b>3.3</b> Transforming and Aggregating</a></li>
<li class="chapter" data-level="3.4" data-path="data.html"><a href="data.html#merging"><i class="fa fa-check"></i><b>3.4</b> Merging</a></li>
<li class="chapter" data-level="3.5" data-path="data.html"><a href="data.html#appendix-creating-the-example-data"><i class="fa fa-check"></i><b>3.5</b> Appendix: Creating the Example Data</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="visualization.html"><a href="visualization.html"><i class="fa fa-check"></i><b>4</b> Data Visualization</a>
<ul>
<li class="chapter" data-level="4.1" data-path="visualization.html"><a href="visualization.html#basic-plots"><i class="fa fa-check"></i><b>4.1</b> Basic Plots</a></li>
<li class="chapter" data-level="4.2" data-path="visualization.html"><a href="visualization.html#saving-plots"><i class="fa fa-check"></i><b>4.2</b> Saving Plots</a></li>
<li class="chapter" data-level="4.3" data-path="visualization.html"><a href="visualization.html#faceting"><i class="fa fa-check"></i><b>4.3</b> Faceting</a></li>
<li class="chapter" data-level="4.4" data-path="visualization.html"><a href="visualization.html#aesthetics"><i class="fa fa-check"></i><b>4.4</b> Aesthetics</a></li>
<li class="chapter" data-level="4.5" data-path="visualization.html"><a href="visualization.html#appendix-creating-the-example-data-1"><i class="fa fa-check"></i><b>4.5</b> Appendix: Creating the Example Data</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="bivariate.html"><a href="bivariate.html"><i class="fa fa-check"></i><b>5</b> Bivariate Regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="bivariate.html"><a href="bivariate.html#probability"><i class="fa fa-check"></i><b>5.1</b> Probability Refresher</a></li>
<li class="chapter" data-level="5.2" data-path="bivariate.html"><a href="bivariate.html#the-linear-model"><i class="fa fa-check"></i><b>5.2</b> The Linear Model</a></li>
<li class="chapter" data-level="5.3" data-path="bivariate.html"><a href="bivariate.html#least-squares"><i class="fa fa-check"></i><b>5.3</b> Least Squares</a></li>
<li class="chapter" data-level="5.4" data-path="bivariate.html"><a href="bivariate.html#properties"><i class="fa fa-check"></i><b>5.4</b> Properties</a></li>
<li class="chapter" data-level="5.5" data-path="bivariate.html"><a href="bivariate.html#appendix-regression-in-r"><i class="fa fa-check"></i><b>5.5</b> Appendix: Regression in R</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="matrix.html"><a href="matrix.html"><i class="fa fa-check"></i><b>6</b> Matrix Algebra: A Crash Course</a>
<ul>
<li class="chapter" data-level="6.1" data-path="matrix.html"><a href="matrix.html#vector-operations"><i class="fa fa-check"></i><b>6.1</b> Vector Operations</a></li>
<li class="chapter" data-level="6.2" data-path="matrix.html"><a href="matrix.html#matrix-operations"><i class="fa fa-check"></i><b>6.2</b> Matrix Operations</a></li>
<li class="chapter" data-level="6.3" data-path="matrix.html"><a href="matrix.html#matrix-inversion"><i class="fa fa-check"></i><b>6.3</b> Matrix Inversion</a></li>
<li class="chapter" data-level="6.4" data-path="matrix.html"><a href="matrix.html#solving-linear-systems"><i class="fa fa-check"></i><b>6.4</b> Solving Linear Systems</a></li>
<li class="chapter" data-level="6.5" data-path="matrix.html"><a href="matrix.html#appendix-matrices-in-r"><i class="fa fa-check"></i><b>6.5</b> Appendix: Matrices in R</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ols-matrix.html"><a href="ols-matrix.html"><i class="fa fa-check"></i><b>7</b> Reintroduction to the Linear Model</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ols-matrix.html"><a href="ols-matrix.html#the-linear-model-in-matrix-form"><i class="fa fa-check"></i><b>7.1</b> The Linear Model in Matrix Form</a></li>
<li class="chapter" data-level="7.2" data-path="ols-matrix.html"><a href="ols-matrix.html#the-ols-estimator"><i class="fa fa-check"></i><b>7.2</b> The OLS Estimator</a></li>
<li class="chapter" data-level="7.3" data-path="ols-matrix.html"><a href="ols-matrix.html#vector-valued-random-variables"><i class="fa fa-check"></i><b>7.3</b> Vector-Valued Random Variables</a></li>
<li class="chapter" data-level="7.4" data-path="ols-matrix.html"><a href="ols-matrix.html#properties-of-ols"><i class="fa fa-check"></i><b>7.4</b> Properties of OLS</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="specification.html"><a href="specification.html"><i class="fa fa-check"></i><b>8</b> Specification Issues</a>
<ul>
<li class="chapter" data-level="8.1" data-path="specification.html"><a href="specification.html#categorical-variables"><i class="fa fa-check"></i><b>8.1</b> Categorical Variables</a></li>
<li class="chapter" data-level="8.2" data-path="specification.html"><a href="specification.html#interaction-terms"><i class="fa fa-check"></i><b>8.2</b> Interaction Terms</a></li>
<li class="chapter" data-level="8.3" data-path="specification.html"><a href="specification.html#quadratic-terms"><i class="fa fa-check"></i><b>8.3</b> Quadratic Terms</a></li>
<li class="chapter" data-level="8.4" data-path="specification.html"><a href="specification.html#logarithmic-models"><i class="fa fa-check"></i><b>8.4</b> Logarithmic Models</a></li>
<li class="chapter" data-level="8.5" data-path="specification.html"><a href="specification.html#appendix-nonstandard-specifications-in-r"><i class="fa fa-check"></i><b>8.5</b> Appendix: Nonstandard Specifications in R</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="specification.html"><a href="specification.html#categorical-variables-1"><i class="fa fa-check"></i><b>8.5.1</b> Categorical Variables</a></li>
<li class="chapter" data-level="8.5.2" data-path="specification.html"><a href="specification.html#interaction-terms-1"><i class="fa fa-check"></i><b>8.5.2</b> Interaction Terms</a></li>
<li class="chapter" data-level="8.5.3" data-path="specification.html"><a href="specification.html#quadratic-and-logarithmic-models"><i class="fa fa-check"></i><b>8.5.3</b> Quadratic and Logarithmic Models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>9</b> Drawing Inferences</a>
<ul>
<li class="chapter" data-level="9.1" data-path="inference.html"><a href="inference.html#the-basics-of-hypothesis-testing"><i class="fa fa-check"></i><b>9.1</b> The Basics of Hypothesis Testing</a></li>
<li class="chapter" data-level="9.2" data-path="inference.html"><a href="inference.html#variance-of-ols"><i class="fa fa-check"></i><b>9.2</b> Variance of OLS</a></li>
<li class="chapter" data-level="9.3" data-path="inference.html"><a href="inference.html#single-variable-hypotheses"><i class="fa fa-check"></i><b>9.3</b> Single Variable Hypotheses</a></li>
<li class="chapter" data-level="9.4" data-path="inference.html"><a href="inference.html#multiple-variable-hypotheses"><i class="fa fa-check"></i><b>9.4</b> Multiple Variable Hypotheses</a></li>
<li class="chapter" data-level="9.5" data-path="inference.html"><a href="inference.html#appendix-full-derivation-of-ols-variance"><i class="fa fa-check"></i><b>9.5</b> Appendix: Full Derivation of OLS Variance</a></li>
<li class="chapter" data-level="9.6" data-path="inference.html"><a href="inference.html#appendix-regression-inference-in-r"><i class="fa fa-check"></i><b>9.6</b> Appendix: Regression Inference in R</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="nonspherical.html"><a href="nonspherical.html"><i class="fa fa-check"></i><b>10</b> Non-Spherical Errors</a>
<ul>
<li class="chapter" data-level="10.1" data-path="nonspherical.html"><a href="nonspherical.html#generalized-least-squares"><i class="fa fa-check"></i><b>10.1</b> Generalized Least Squares</a></li>
<li class="chapter" data-level="10.2" data-path="nonspherical.html"><a href="nonspherical.html#detecting-heteroskedasticity"><i class="fa fa-check"></i><b>10.2</b> Detecting Heteroskedasticity</a></li>
<li class="chapter" data-level="10.3" data-path="nonspherical.html"><a href="nonspherical.html#heteroskedasticity-of-unknown-form"><i class="fa fa-check"></i><b>10.3</b> Heteroskedasticity of Unknown Form</a></li>
<li class="chapter" data-level="10.4" data-path="nonspherical.html"><a href="nonspherical.html#appendix-implementation-in-r"><i class="fa fa-check"></i><b>10.4</b> Appendix: Implementation in R</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="nonspherical.html"><a href="nonspherical.html#generalized-least-squares-1"><i class="fa fa-check"></i><b>10.4.1</b> Generalized Least Squares</a></li>
<li class="chapter" data-level="10.4.2" data-path="nonspherical.html"><a href="nonspherical.html#breusch-pagan-test"><i class="fa fa-check"></i><b>10.4.2</b> Breusch-Pagan Test</a></li>
<li class="chapter" data-level="10.4.3" data-path="nonspherical.html"><a href="nonspherical.html#heteroskedasticity-consistent-standard-errors"><i class="fa fa-check"></i><b>10.4.3</b> Heteroskedasticity-Consistent Standard Errors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="crisis.html"><a href="crisis.html"><i class="fa fa-check"></i><b>11</b> The Statistical Crisis in Science</a>
<ul>
<li class="chapter" data-level="11.1" data-path="crisis.html"><a href="crisis.html#publication-bias"><i class="fa fa-check"></i><b>11.1</b> Publication Bias</a></li>
<li class="chapter" data-level="11.2" data-path="crisis.html"><a href="crisis.html#p-hacking"><i class="fa fa-check"></i><b>11.2</b> <span class="math inline">\(p\)</span>-Hacking</a></li>
<li class="chapter" data-level="11.3" data-path="crisis.html"><a href="crisis.html#what-to-do"><i class="fa fa-check"></i><b>11.3</b> What to Do</a></li>
<li class="chapter" data-level="11.4" data-path="crisis.html"><a href="crisis.html#appendix-r-simulation"><i class="fa fa-check"></i><b>11.4</b> Appendix: R Simulation</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="panel.html"><a href="panel.html"><i class="fa fa-check"></i><b>12</b> Clustered and Panel Data</a>
<ul>
<li class="chapter" data-level="12.1" data-path="panel.html"><a href="panel.html#the-linear-model-with-grouped-data"><i class="fa fa-check"></i><b>12.1</b> The Linear Model with Grouped Data</a></li>
<li class="chapter" data-level="12.2" data-path="panel.html"><a href="panel.html#autocorrelation-within-groups"><i class="fa fa-check"></i><b>12.2</b> Autocorrelation within Groups</a></li>
<li class="chapter" data-level="12.3" data-path="panel.html"><a href="panel.html#clustered-standard-errors"><i class="fa fa-check"></i><b>12.3</b> Clustered Standard Errors</a></li>
<li class="chapter" data-level="12.4" data-path="panel.html"><a href="panel.html#random-effects"><i class="fa fa-check"></i><b>12.4</b> Random Effects</a></li>
<li class="chapter" data-level="12.5" data-path="panel.html"><a href="panel.html#fixed-effects"><i class="fa fa-check"></i><b>12.5</b> Fixed Effects</a></li>
<li class="chapter" data-level="12.6" data-path="panel.html"><a href="panel.html#appendix-implementation-in-r-1"><i class="fa fa-check"></i><b>12.6</b> Appendix: Implementation in R</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="logit.html"><a href="logit.html"><i class="fa fa-check"></i><b>13</b> Binary Response Models</a>
<ul>
<li class="chapter" data-level="13.1" data-path="logit.html"><a href="logit.html#the-linear-probability-model"><i class="fa fa-check"></i><b>13.1</b> The Linear Probability Model</a></li>
<li class="chapter" data-level="13.2" data-path="logit.html"><a href="logit.html#the-logistic-regression-model"><i class="fa fa-check"></i><b>13.2</b> The Logistic Regression Model</a></li>
<li class="chapter" data-level="13.3" data-path="logit.html"><a href="logit.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>13.3</b> Maximum Likelihood Estimation</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="logit.html"><a href="logit.html#properties-of-logarithms"><i class="fa fa-check"></i><b>13.3.1</b> Properties of logarithms</a></li>
<li class="chapter" data-level="13.3.2" data-path="logit.html"><a href="logit.html#bernoulli-trials"><i class="fa fa-check"></i><b>13.3.2</b> Bernoulli trials</a></li>
<li class="chapter" data-level="13.3.3" data-path="logit.html"><a href="logit.html#uniform-draws"><i class="fa fa-check"></i><b>13.3.3</b> Uniform draws</a></li>
<li class="chapter" data-level="13.3.4" data-path="logit.html"><a href="logit.html#logistic-regression"><i class="fa fa-check"></i><b>13.3.4</b> Logistic regression</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="logit.html"><a href="logit.html#special-considerations"><i class="fa fa-check"></i><b>13.4</b> Special Considerations</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="logit.html"><a href="logit.html#separation"><i class="fa fa-check"></i><b>13.4.1</b> Separation</a></li>
<li class="chapter" data-level="13.4.2" data-path="logit.html"><a href="logit.html#clustered-standard-errors-1"><i class="fa fa-check"></i><b>13.4.2</b> Clustered Standard Errors</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="logit.html"><a href="logit.html#appendix-implementation-in-r-2"><i class="fa fa-check"></i><b>13.5</b> Appendix: Implementation in R</a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="logit.html"><a href="logit.html#logistic-regression-1"><i class="fa fa-check"></i><b>13.5.1</b> Logistic Regression</a></li>
<li class="chapter" data-level="13.5.2" data-path="logit.html"><a href="logit.html#maximum-likelihood-estimation-1"><i class="fa fa-check"></i><b>13.5.2</b> Maximum Likelihood Estimation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Practical Data Analysis for Political Scientists</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="logit" class="section level1" number="13">
<h1><span class="header-section-number">13</span> Binary Response Models</h1>
<p>Many of the political phenomena we’d like to explain are dichotomous:</p>
<ul>
<li><p>Whether someone votes.</p></li>
<li><p>Whether someones votes for a particular candidate.</p></li>
<li><p>Whether war occurs.</p></li>
</ul>
<p>For a long time, the conventional wisdom among political scientists was that special statistical models were required to analyze binary responses like these.
In this section of the course, we’ll discuss the reasoning behind this claim, work with the fancier statistical models that are purportedly necessary for binary responses, and discuss why the conventional wisdom about linear models might be wrong.</p>
<div id="the-linear-probability-model" class="section level2" number="13.1">
<h2><span class="header-section-number">13.1</span> The Linear Probability Model</h2>
<p>So far when encountering binary responses, we have just modeled them as linear functions of covariates.
For example, think of voting for Trump as a function of the years of education one has received:
<span class="math display">\[
\text{Trump}_n = \beta_0 + \beta_1 \text{Education}_n + \epsilon_n.
\]</span>
In this model, <span class="math inline">\(\beta_0\)</span> represents the probability that someone with zero years of education would vote for Trump, and <span class="math inline">\(\beta_1\)</span> is the marginal effect of an additional year of education.</p>
<p>Suppose we yielded the following estimates from this model:
<span class="math display">\[
\text{Trump}_n = 1.4 - 0.075 \text{Education}_n + \epsilon_n.
\]</span>
Most Americans have between 8 and 16 years of formal education.
According to this regression equation, we should expect about 80% of those with only an eighth-grade education to vote for Trump, while we should expect only 20% of those with a college degree to vote for Trump.
These sound like reasonable numbers!</p>
<p>The problem is, not <em>every</em> American’s educational attainment lies within this range.
Some Americans only have four years of formal education, and this model would predict that 110% of them voted for Trump.
Other Americans are crazy enough to pursue 20+ years of education, and this model would predict that –10% or fewer of them voted for Trump.</p>
<p>This illustrates one problem with linear probability models: they can produce impossible predicted probabilities, outside the <span class="math inline">\([0, 1]\)</span> range that all probabilities must lie within.
In practice, such predictions usually occur for observations far away from the central tendency of the data.
(In fact, as you proved on a previous problem set, the predicted probability for an observation with an average value of all covariates must equal the sample proportion of 1s in the response.)</p>
<p>We can see this problem in action when we draw the OLS line through the scatterplot of some (hypothetical) data on educational attainment and vote choice.</p>
<p><img src="pdaps_files/figure-html/binary-scatterplot-1.png" width="672" /></p>
<p>Another way to put all this is that the assumption of constant marginal effects doesn’t make sense for a binary response.
If someone already has a 99% chance of voting for Trump, the most you could possibly raise their chance of voting for him is 1%.
This puts a cap on the magnitude of the effect of any given variable.
An intervention that raises an <em>average</em> voter’s chance of voting for Trump by 5% cannot possibly have the same effect on our hypothetical Trump die-hard.
This is in contrast to settings with continuous outcomes.
A job training program that raises an average person’s annual earnings by 2% is <em>unlikely</em> to have the same effect on Jeff Bezos, but it is at least <em>conceivable</em> it would do so.</p>
<p>The idea behind logistic regression (and its close friend probit regression) is to model the conditional expectation in a way that gets around these issues.
These models always produce sensible predicted probabilities, and they assume lower-magnitude marginal effects for observations at the extremes.
Instead of regression lines, they draw nice curves like the one below.</p>
<p><img src="pdaps_files/figure-html/binary-scatterplot-redux-1.png" width="672" /></p>
</div>
<div id="the-logistic-regression-model" class="section level2" number="13.2">
<h2><span class="header-section-number">13.2</span> The Logistic Regression Model</h2>
<p>For a binary response, <span class="math inline">\(Y_n \in \{0, 1\}\)</span>, we can think of the linear probability model as the following:
<span class="math display">\[
\Pr(Y_n = 1) = \mathbf{x}_n \cdot \beta.
\]</span>
Logistic regression replaces the linear formula on the right-hand side with a nonlinear S-curve, as illustrated above.
The logistic regression model is
<span class="math display">\[
\Pr(Y_n = 1) = \frac{e^{\mathbf{x}_n \cdot \beta}}{1 + e^{\mathbf{x}_n \cdot \beta}} = \Lambda(\mathbf{x}_n \cdot \beta),
\]</span>
where <span class="math inline">\(\Lambda(z) = e^z / (1 + e^z)\)</span>.</p>
<p>How did we get the above formula?
We can tell a little story to provide a foundation for the logistic regression model.
For each observation <span class="math inline">\(n = 1, \ldots, N\)</span>, let <span class="math inline">\(Y_n^*\)</span> denote the <em>latent</em> (unobservable) propensity to choose <span class="math inline">\(Y_n = 1\)</span>.
We assume that the latent propensity follows a linear model,
<span class="math display">\[
Y_n^* = \mathbf{x}_n \cdot \beta + \epsilon_n,
\]</span>
where <span class="math inline">\(\epsilon_n\)</span> is independent and identically distributed across observations.
Furthermore, assume <span class="math inline">\(\epsilon_n\)</span> has a logistic distribution, so that <span class="math inline">\(\Pr(\epsilon_n &lt; z) = \Lambda(z)\)</span> for all real numbers <span class="math inline">\(z\)</span>.
Finally, we assume that we observe <span class="math inline">\(Y_n = 1\)</span> if and only if the latent propensity is non-negative:
<span class="math display">\[
Y_n = \begin{cases}
0 &amp; Y_n^* &lt; 0, \\
1 &amp; Y_n^* \geq 0.
\end{cases}
\]</span>
The logistic regression model follows from this combination of assumptions.</p>
<p>I hope you notice that the assumptions we’ve made on the latent propensity <span class="math inline">\(Y_n^*\)</span> are even stronger than the usual linear model assumptions.
We assumed that <span class="math inline">\(\epsilon_n\)</span> was independent across observations, whereas the usual linear model allows for autocorrelation.
We assumed that <span class="math inline">\(\epsilon_n\)</span> was identically distributed across observations, whereas the usual linear model allows for heteroskedasticity.<a href="#fn34" class="footnote-ref" id="fnref34"><sup>34</sup></a>
Finally, we assumed that <span class="math inline">\(\epsilon_n\)</span> has a logistic distribution, whereas the usual linear model does not assume a specific distributional form for the error term.
The verisimilitude of the S-curve comes at the cost of forcing us to make many more assumptions than we do with a linear model.</p>
<p>Probit regression is the same as logistic regression, except we assume that the error term <span class="math inline">\(\epsilon_n\)</span> in the latent propensity equation has a standard normal distribution.
This results in the model
<span class="math display">\[
\Pr(Y_n = 1) = \Phi(\mathbf{x}_n \cdot \beta),
\]</span>
where <span class="math inline">\(\Phi\)</span> is the standard normal CDF.
In practice, logit and probit almost always yield nearly identical results in terms of predicted probabilities and marginal effects.
From here on we will keep working with the logistic model, solely because the logistic CDF is easier to work with mathematically than the normal CDF.</p>
<p>One of the nice things about the linear probability model is that it’s easy to calculate marginal effects.
The estimated marginal effect of <span class="math inline">\(x_{nk}\)</span> on <span class="math inline">\(\Pr(Y_n = 1)\)</span> is simply its coefficient, <span class="math inline">\(\beta_k\)</span>.
It is not so simple with logistic regression.
First we need the derivative of the logistic CDF.
Surely you remember the quotient rule, which gives us
<span class="math display">\[
\Lambda&#39;(z)
= \frac{e^z (1 + e^z) - e^z (e^z)}{(1 + e^z)^2}
= \frac{e^z}{1 + e^z} \times \frac{1}{1 + e^z}
= \Lambda(z) [1 - \Lambda(z)].
\]</span>
And since you surely also remember the chain rule, you can calculate the marginal effect of <span class="math inline">\(x_{nk}\)</span> on <span class="math inline">\(\Pr(Y_n = 1)\)</span> as
<span class="math display">\[
\begin{aligned}
\frac{\partial \Pr(Y_n = 1)}{x_{nk}}
&amp;= \Lambda&#39;(\mathbf{x}_k \cdot \beta) \frac{\partial [\mathbf{x}_k \cdot \beta]}{x_{nk}} \\
&amp;= \Lambda(\mathbf{x}_k \cdot \beta) [1 - \Lambda(\mathbf{x}_k \cdot \beta)] \beta_k \\
&amp;= \Pr(Y_n = 1) \Pr(Y_n = 0) \beta_k.
\end{aligned}
\]</span>
This gives us the properties we wanted—that the marginal effect of a variable is lowest in magnitude for those observations that are already highly likely or highly unlikely to have <span class="math inline">\(Y_n = 1\)</span>.</p>
<ul>
<li><p>If <span class="math inline">\(\Pr(Y_n = 1) \approx 0\)</span>, then the marginal effect of <span class="math inline">\(x_{nk}\)</span> is approximately zero.</p></li>
<li><p>If <span class="math inline">\(\Pr(Y_n = 1) = 0.5\)</span>, then the marginal effect of <span class="math inline">\(x_{nk}\)</span> is <span class="math inline">\(\beta_k / 4\)</span>.</p></li>
<li><p>If <span class="math inline">\(\Pr(Y_n = 1) \approx 1\)</span>, then the marginal effect of <span class="math inline">\(x_{nk}\)</span> is approximately zero.</p></li>
</ul>
<p>This gives us a helpful rule for making sense of logistic regression output.
You can divide each coefficient by 4, and that gives you its marginal effect on an observation that’s at the 50-50 point.</p>
<p><img src="pdaps_files/figure-html/logit-margeff-1.png" width="672" /></p>
<p>Briefly — for a probit model, the marginal effect is
<span class="math display">\[
\frac{\partial \Pr(Y_n = 1)}{x_{nk}} = \phi(\mathbf{x}_n \cdot \beta) \beta_k,
\]</span>
where <span class="math inline">\(\phi\)</span> is the standard normal PDF.
Since <span class="math inline">\(\phi(0) \approx 0.4\)</span>, the marginal effect for an observation at the 50-50 point is roughly 0.4 times its coefficient.</p>
<p>For either logit or probit, if you want to calculate the average marginal effect of <span class="math inline">\(X_k\)</span> across your dataset, you may be tempted to calculate the marginal effect for an “average” observation—one with mean or median values for all the covariates.
Resist the temptation.
In both cases, the marginal effect is a nonlinear function of <span class="math inline">\(x_{nk}\)</span>, and a not-so-fun fact about nonlinear functions is that <span class="math inline">\(E[f(z)] \neq f(E[z])\)</span>.
If you want the average marginal effect, you need to calculate it individually for each observation and then average them:
<span class="math display">\[
\text{average m.e. of $X_k$}
= \frac{\beta_k}{N} \sum_{i=1}^N \Lambda(\mathbf{x}_n \cdot \beta) [1 - \Lambda(\mathbf{x}_n \cdot \beta)]
\]</span></p>
<p>For binary covariates, we will usually focus on the first difference rather than the marginal effect, as it doesn’t make much sense to take a derivative with respect to a dichotomous indicator.
Assuming <span class="math inline">\(X_k\)</span> is binary, the first difference for the <span class="math inline">\(n\)</span>’th observation is
<span class="math display">\[
\Pr(Y_n = 1) - \Pr(Y_n = 0) =
\Lambda(\tilde{\mathbf{x}}_n \cdot \tilde{\beta} + \beta_k)
- \Lambda(\tilde{\mathbf{x}}_n \cdot \tilde{\beta}),
\]</span>
where <span class="math inline">\(\tilde{\mathbf{x}}_n\)</span> is the vector of all covariates but the <span class="math inline">\(k\)</span>’th, and <span class="math inline">\(\tilde{\beta}\)</span> is the vector of all coefficients but the <span class="math inline">\(k\)</span>’th.
As with marginal effects, to calculate the average first difference across the data, you must calculate it for each observation individually and then average them—no shortcuts.</p>
</div>
<div id="maximum-likelihood-estimation" class="section level2" number="13.3">
<h2><span class="header-section-number">13.3</span> Maximum Likelihood Estimation</h2>
<p>Remember how we derived the ordinary least squares estimator for the linear model: we decided our goal was to minimize the sum of squared residuals, then we found the formula (the function of <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{Y}\)</span>) that accomplished this task.
Even though we started with a geometrical purpose (minimizing squared errors), we saw that the resulting estimator had various good statistical properties, such as unbiasedness, consistency, and asymptotic normality.</p>
<p>We typically don’t use least squares estimators for nonlinear models like logistic regression.<a href="#fn35" class="footnote-ref" id="fnref35"><sup>35</sup></a>
Instead, we typically use <em>maximum likelihood estimation</em>.
We won’t go into the deeper mathematical details behind maximum likelihood estimation; for that I recommend <span class="citation">Davidson and MacKinnon (<a href="#ref-davidson1993estimation" role="doc-biblioref">1993</a>)</span>.</p>
<p>What is important to know generally about maximum likelihood estimation, or MLE, is the following:</p>
<ul>
<li><p>To derive a maximum likelihood estimator, you must specify a <em>full probability model</em> for the process that generates your data.
This entails specifying distributional families for all random variables.</p>
<p>What this means is most easily illustrated by contrast with the ordinary linear model.
With the linear model, when we wrote <span class="math inline">\(Y_n = \mathbf{x}_n \cdot \beta + \epsilon_n\)</span>, all we assumed about <span class="math inline">\(\epsilon_n\)</span> was that it satisfied strict exogeneity and (sometimes) spherical errors.
We didn’t assume it had a normal (or any other) distribution when we derived the OLS and GLS estimators.
But if we wanted to estimate the linear model via maximum likelihood, we would have to specify a distribution for the error term.<a href="#fn36" class="footnote-ref" id="fnref36"><sup>36</sup></a></p></li>
<li><p>The general idea behind maximum likelihood estimation is to find the model parameters under which our observed data is most likely to have occurred.
If you draw a random sample of Democratic primary voters, and 68% of them tell you that they prefer Joe Biden over Bernie Sanders, it is <em>possible</em> that in fact Biden’s support in the population is 10%, or that it’s 90%.
However, we’d be much more likely to draw the sample that we did if the true population proportion is 68% than if it were 10% or 90%.
We will rely on this kind of reasoning to develop maximum likelihood estimators.</p></li>
<li><p>As long as your model of the data-generating process is correctly specified, the maximum likelihood estimator is <em>consistent</em>, <em>asymptotically normal</em>, and <em>asymptotically efficient</em>.
This means that in sufficiently large samples:</p>
<ul>
<li><p>The maximum likelihood estimate from any given sample is likely to be close to the population parameter value.</p></li>
<li><p>The distribution of maximum likelihood estimates across samples is approximately normal.</p></li>
<li><p>No other consistent estimator will have smaller standard errors.</p></li>
</ul></li>
</ul>
<p>Maximum likelihood estimation is built around the <em>likelihood function</em>.
Let <span class="math inline">\(\mathbf{Z}\)</span> denote the data we observe, and let <span class="math inline">\(\theta\)</span> denote the parameters we want to estimate.
In the regression context, <span class="math inline">\(\mathbf{Z}\)</span> would consist of the response <span class="math inline">\(\mathbf{Y}\)</span> and the covariates <span class="math inline">\(\mathbf{X}\)</span>, and <span class="math inline">\(\theta\)</span> would consist of the coefficients <span class="math inline">\(\beta\)</span>.
The likelihood function, which I’ll denote <span class="math inline">\(\ell(\theta, \mathbf{Z})\)</span>, is the probability that we would observe the given data, <span class="math inline">\(\mathbf{Z}\)</span>, if the parameters of the model were <span class="math inline">\(\theta\)</span>.
The maximum likelihood estimator is the value of <span class="math inline">\(\theta\)</span> at which <span class="math inline">\(\ell\)</span> reaches its maximum, given the data <span class="math inline">\(\mathbf{Z}\)</span>; it is the parameter value under which we have the highest chance of observing our actual data.
In case this sounds either trivial or incomprehensible, let’s make it concrete by working through some simple examples.</p>
<div id="properties-of-logarithms" class="section level3" number="13.3.1">
<h3><span class="header-section-number">13.3.1</span> Properties of logarithms</h3>
<p>As you’ll see below, we often run into the natural logarithm in maximum likelihood estimation.
So let’s briefly refresh ourselves on the most important properties of the natural logarithm.</p>
<p>Remember that the natural logarithm is the inverse of the natural exponent.
For all <span class="math inline">\(s \in \mathbb{R}\)</span> and <span class="math inline">\(t &gt; 0\)</span>,
<span class="math display">\[
\log (t) = s \quad \Leftrightarrow \quad e^s = t,
\]</span>
where <span class="math inline">\(e\)</span> is Euler’s number, <span class="math inline">\(e \approx 2.71828\)</span>.
Exponents and logarithms have some helpful properties that will become second nature once you work with them enough.</p>
<table>
<thead>
<tr class="header">
<th>Exponent Property</th>
<th>Logarithm Property</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(e^{a + b} = e^a \cdot e^b\)</span></td>
<td><span class="math inline">\(\log (ab) = \log(a) + \log(b)\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(e^{a - b} = \frac{e^a}{e^b}\)</span></td>
<td><span class="math inline">\(\log (\frac{a}{b}) = \log(a) - \log(b)\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(e^{ab} = (e^a)^b\)</span></td>
<td><span class="math inline">\(\log (a^b) = b \log (a)\)</span></td>
</tr>
</tbody>
</table>
<p>The natural logarithm plays a special role in differential calculus as well.
Remember the power rule,
<span class="math display">\[
\frac{d x^n}{d x} = n x^{n-1}.
\]</span>
If we apply the power rule in reverse, we see that
<span class="math display">\[
\begin{aligned}
\frac{d}{dx} \left[ \frac{x^3}{3} \right] &amp;= x^2, \\
\frac{d}{dx} \left[ \frac{x^2}{2} \right] &amp;= x^1, \\
\frac{d}{dx} \left[ \frac{x^1}{1} \right] &amp;= x^0 (= 1) \\
\frac{d}{dx} \left[ \text{???} \vphantom{\frac{x^0}{0}} \right] &amp;= x^{-1} (= \frac{1}{x}) \\
\frac{d}{dx} \left[ \frac{x^{-1}}{-1} \right] &amp;= x^{-2} (= \frac{1}{x^2}).
\end{aligned}
\]</span>
If we tried to fill in according to the pattern to find the function whose derivative is <span class="math inline">\(x^{-1}\)</span>, we would end up with <span class="math inline">\(\frac{x^0}{0}\)</span>, better known as <span class="math inline">\(\frac{1}{0}\)</span>, which can’t possibly be right.
It turns out instead that the natural logarithm plays this role:
<span class="math display">\[
\frac{d \log(x)}{dx} = \frac{1}{x}.
\]</span></p>
<p>We will often encounter expressions of the form <span class="math inline">\(\log(f(x))\)</span>, where <span class="math inline">\(f(x)\)</span> is some differentiable function of <span class="math inline">\(x\)</span>.
In this case, applying the chain rule, we have
<span class="math display">\[
\frac{d \log(f(x))}{dx} = \frac{\frac{d f(x)}{dx}}{f(x)}.
\]</span>
This is another one of those operations that will become second nature over time.</p>
</div>
<div id="bernoulli-trials" class="section level3" number="13.3.2">
<h3><span class="header-section-number">13.3.2</span> Bernoulli trials</h3>
<p>Assume we observe a sequence of independent random variables <span class="math inline">\(Z_1, \ldots, Z_N\)</span>, where each <span class="math inline">\(Z_n \sim \text{Bernoulli}(p)\)</span>.
Our goal is to estimate <span class="math inline">\(p\)</span>, the population probability of a “success” <span class="math inline">\(Z_n = 1\)</span> as opposed to a “failure” <span class="math inline">\(Z_n = 0\)</span>.</p>
<p>Suppose we only had a single observation, <span class="math inline">\(N = 1\)</span>.
What would the likelihood function, <span class="math inline">\(\ell(p, Z_1)\)</span>, look like?
Well, there are two possible samples: <span class="math inline">\(Z_1 = 0\)</span> or <span class="math inline">\(Z_1 = 1\)</span>.
If the population parameter is <span class="math inline">\(p\)</span> and our observed sample is <span class="math inline">\(Z_1 = 0\)</span>, the probability of observing this sample is <span class="math inline">\(1 - p\)</span>.
Conversely, if our observed sample is <span class="math inline">\(Z_1 = 1\)</span>, the probability of observing this sample is <span class="math inline">\(p\)</span>.
Hence, the likelihood function is
<span class="math display">\[
\ell(p, Z_1) = \begin{cases}
1 - p &amp; Z_1 = 0, \\
p &amp; Z_1 = 1.
\end{cases}
\]</span>
Now let’s think about choosing <span class="math inline">\(p\)</span> to <em>maximize</em> the likelihood function.
If <span class="math inline">\(Z_1 = 0\)</span>, then the likelihood function is strictly decreasing in <span class="math inline">\(p\)</span>, so we want to choose the smallest value possible: <span class="math inline">\(p = 0\)</span>.
If <span class="math inline">\(Z_1 = 1\)</span>, we are have the opposite: the likelihood function is strictly increasing in <span class="math inline">\(p\)</span>, and thus is maximized at <span class="math inline">\(p = 1\)</span>.
Altogether, we have the maximum likelihood estimator:
<span class="math display">\[
\hat{p}_{\text{MLE}}(Z_1) = \begin{cases}
0 &amp; Z_1 = 0, \\
1 &amp; Z_1 = 1,
\end{cases}
\]</span>
or, more simply, <span class="math inline">\(\hat{p}_{\text{MLE}}(Z_1) = Z_1\)</span>.</p>
<p>Now let’s think about the more general case, where <span class="math inline">\(\mathbf{Z} = (Z_1, \ldots, Z_N)\)</span>.
Since each observation is independent, the likelihood function is multiplicative:
<span class="math display">\[
\ell(p, \mathbf{Z})
= \Pr(Z = Z_1) \times \Pr(Z = Z_2) \times \cdots \times \Pr(Z = Z_N)
= \prod_{n=1}^N \Pr(Z = Z_n).
\]</span>
For those observations where <span class="math inline">\(Z_n = 0\)</span>, the probability of observing this outcome is <span class="math inline">\(1 - p\)</span>.
For those where <span class="math inline">\(Z_n = 1\)</span>, the probability is <span class="math inline">\(p\)</span>.
So we can further write
<span class="math display">\[
\ell(p, \mathbf{Z})
= \prod_{n : Y_N = 0} (1 - p) \prod_{n : Y_N = 1} p
= (1 - p)^{N - N_1} p^{N_1},
\]</span>
where <span class="math inline">\(N_1\)</span> is the number of successes, <span class="math inline">\(N_1 = \sum_{n=1}^N Y_n\)</span>.</p>
<p>We want to maximize <span class="math inline">\(\ell\)</span> with respect to <span class="math inline">\(p\)</span>.
That is, we want to find the value <span class="math inline">\(\hat{p}\)</span> such that
<span class="math display">\[
\ell(\hat{p}, \mathbf{Z}) \geq \ell(p, \mathbf{Z}) \qquad \text{for all $p \in [0, 1]$.}
\]</span>
This is a maximization problem.
We solve maximization problems the same way we solved the minimization problem of OLS—by finding the point at which the derivative equals zero.
Here, we want to find the point at which the derivative of the likelihood function with respect to <span class="math inline">\(p\)</span> equals zero.</p>
<p>Taking derivatives of products is annoying.
It’s way easier to take derivatives of sums, because the derivative of the sum is the sum of the derivatives.
Therefore, likelihood-based statistical methods usually work with the <em>log-likelihood</em>, the natural logarithm of the likelihood function.
Because the logarithm is a strictly increasing function, it is maximized at the same point as the likelihood function.
Consequently, we will look for the value <span class="math inline">\(\hat{p}\)</span> such that
<span class="math display">\[
\log \ell(\hat{p}, \mathbf{Z}) \geq \log \ell(p, \mathbf{Z}) \qquad \text{for all $p \in [0, 1]$},
\]</span>
which we will find by solving for <span class="math inline">\(\hat{p}\)</span> in
<span class="math display">\[
\frac{\partial \log \ell(\hat{p}, \mathbf{Z})}{\partial p} = 0.
\]</span></p>
<p>For the Bernoulli problem, we have
<span class="math display">\[
\begin{aligned}
\log \ell(p, \mathbf{Z}) &amp;= \log \left[ (1 - p)^{N - N_1} p^{N_1} \right] \\
&amp;= (N - N_1) \log(1 - p) + N_1 \log(p)
\end{aligned}
\]</span>
and therefore
<span class="math display">\[
\begin{aligned}
\frac{\partial \log \ell(p, \mathbf{Z})}{\partial p}
&amp;= \frac{-(N - N_1)}{1 - p} + \frac{N_1}{p}.
\end{aligned}
\]</span>
We solve for the maximum likelihood estimate by setting the above expression to equal zero, giving us the condition
<span class="math display">\[
\begin{aligned}
\frac{N_1}{\hat{p}} = \frac{N - N_1}{1 - \hat{p}}.
&amp;\quad\Leftrightarrow\quad N_1 - N_1 \hat{p} = N \hat{p} - N_1 \hat{p} \\
&amp;\quad\Leftrightarrow\quad \hat{p} = \frac{N_1}{N}.
\end{aligned}
\]</span>
Finally, therefore, the maximum likelihood estimator is the sample proportion of successes,
<span class="math display">\[
\hat{p}_{\text{MLE}}(\mathbf{Z}) = \frac{\sum_{n=1}^N Z_n}{N}.
\]</span></p>
<p>All that seems like a <em>lot</em> of effort to come to the conclusion that we should estimate the parameter of the Bernoulli distribution via the sample proportion of successes.
Didn’t we already know that?
Yes, we did, but this is a relatively easy example for working through the mechanics of deriving a maximum likelihood estimator.
Additionally, we’ll return to the Bernoulli example once we develop the maximum likelihood estimator for binary response models.
Now, we’ll turn to a less trivial example, where the maximum likelihood estimator will differ from what other estimation approaches would tell us to do.</p>
</div>
<div id="uniform-draws" class="section level3" number="13.3.3">
<h3><span class="header-section-number">13.3.3</span> Uniform draws</h3>
<p>Assume we observe a sequence of independent random variables <span class="math inline">\(Z_1, \ldots, Z_N\)</span>, where each <span class="math inline">\(Z_n \sim U[0, \theta]\)</span>.
Our goal is to estimate <span class="math inline">\(\theta\)</span>, the upper bound of the support of this uniform distribution.
Remember that the probability density function of <span class="math inline">\(U[0, \theta]\)</span> is
<span class="math display">\[
f_\theta(z) = \begin{cases}
0 &amp; z &lt; 0, \\
\frac{1}{\theta} &amp; 0 \leq z \leq \theta, \\
0 &amp; z &gt; \theta.
\end{cases}
\]</span></p>
<p><img src="pdaps_files/figure-html/uniform-dens-1.png" width="672" /></p>
<p>There is a simple unbiased estimator of <span class="math inline">\(\theta\)</span> available to us.
Let <span class="math inline">\(\bar{Z}\)</span> denote our sample mean, <span class="math inline">\(\bar{Z} = \frac{1}{N} \sum_{n=1}^N Z_n\)</span>.
Then <span class="math inline">\(\hat{\theta}_{\text{MM}}(\mathbf{Z}) = 2 \bar{Z}\)</span> is an unbiased estimator of <span class="math inline">\(\theta\)</span>.<a href="#fn37" class="footnote-ref" id="fnref37"><sup>37</sup></a>
This is true because (1) the population mean of <span class="math inline">\(U[0, \theta]\)</span> is <span class="math inline">\(\frac{\theta}{2}\)</span> and (2) the sample mean is an unbiased estimator of the population mean, and thus we have
<span class="math display">\[
E[\hat{\theta}_{\text{MM}}(\mathbf{Z})]
= E[2 \bar{Z}]
= 2 E[\bar{Z}]
= \theta.
\]</span></p>
<p>Here we see why an unbiased estimator is not always best.
Suppose we observe the following data with <span class="math inline">\(N = 3\)</span>:
<span class="math display">\[
Z_1 = 1, \quad Z_2 = 2, \quad Z_3 = 9.
\]</span>
We have the sample mean <span class="math inline">\(\bar{Z} = 4\)</span>, so our unbiased estimate of the population parameter <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\hat{\theta}_{\text{MM}}(\mathbf{Z}) = 8\)</span>.
But this is crazy, because this data cannot possibly have been drawn from <span class="math inline">\(U[0, 8]\)</span>!
If this were the true distribution, then it would be impossible to observe <span class="math inline">\(Z_3 = 9\)</span>.</p>
<p>Let’s see how we would estimate <span class="math inline">\(\theta\)</span> via maximum likelihood.
This time our data is continuous, so the likelihood function gives us the density of the data given the parameters:
<span class="math display">\[
\begin{aligned}
\ell(\theta, \mathbf{Z})
&amp;= f_\theta(Z_1) \times f_\theta(Z_2) \times \cdots \times f_\theta(Z_n) \\
&amp;= \prod_{n=1}^N f_\theta(Z_n).
\end{aligned}
\]</span>
Substituting in the definition of the <span class="math inline">\(U[0, \theta]\)</span> density from above, we have
<span class="math display">\[
\ell(\theta, \mathbf{Z}) = \begin{cases}
0 &amp; \text{any $Z_n &lt; 0$ or $Z_n &gt; \theta$}, \\
\frac{1}{\theta^N} &amp; \text{all $0 \leq Z_n \leq \theta$}.
\end{cases}
\]</span>
This time, we don’t even need to work with the log-likelihood or take derivatives to see that the likelihood will be maximized at <span class="math inline">\(\theta = \max \{Z_1, \ldots, Z_N\}\)</span>.
If we return to our example with <span class="math inline">\(\mathbf{Z} = (1, 2, 9)\)</span>, the likelihood function looks like this:</p>
<p><img src="pdaps_files/figure-html/lik-unif-1.png" width="672" /></p>
<p>Our maximum likelihood estimator for <span class="math inline">\(\theta\)</span> in this model is therefore
<span class="math display">\[
\hat{\theta}_{\text{MLE}}(\mathbf{Z}) = \max \{ Z_1, \ldots, Z_N \}.
\]</span>
This is a biased estimator.
To see why, imagine taking numerous different samples from <span class="math inline">\(U[0, \theta]\)</span> and calculating the MLE for each of them.
We will almost always have <span class="math inline">\(\hat{\theta}_{\text{MLE}} &lt; \theta\)</span>, and we will never have <span class="math inline">\(\hat{\theta}_{\text{MLE}} &gt; \theta\)</span>.
Consequently, the MLE will on average underestimate the true value of <span class="math inline">\(\theta\)</span>.
Nonetheless, the MLE has two advantages over the unbiased estimator we considered.
First, it will never yield an estimate that would be impossible given the observed data.
Second, as our sample size grows large, even though the MLE is biased, it will on average be closer to the true <span class="math inline">\(\theta\)</span> than the unbiased estimate will be.
This is because maximum likelihood estimation is asymptotically efficient.</p>
<p>I want to stress one more important thing that this example highlights.
However you choose to model the data-generating process, there are multiple distinct ways to estimate the parameters of that process.
We already saw this in the linear model context, where OLS and GLS are distinct estimators of the same linear model, each of which is preferable in different circumstances.
You may sometimes hear political scientists refer to “maximum likelihood models”, but this is a misnomer: maximum likelihood is just one method among many to estimate the parameters of a fully specified statistical model.
It happens to have quite good properties for doing so, and hence is usually the default choice, but that doesn’t mean maximum likelihood is embedded in the definition of the model itself.</p>
</div>
<div id="logistic-regression" class="section level3" number="13.3.4">
<h3><span class="header-section-number">13.3.4</span> Logistic regression</h3>
<p>Now let’s return to the logistic regression model.
Here we observe a sequence of outcomes, <span class="math inline">\(Y_1, \ldots, Y_N\)</span>, where each <span class="math inline">\(Y_n \in \{0, 1\}\)</span>, and a sequence of covariate vectors, <span class="math inline">\(\mathbf{x}_1, \ldots, \mathbf{x}_N\)</span>.
Our goal is to estimate <span class="math inline">\(\beta\)</span>, the coefficients associated with each entry in <span class="math inline">\(\mathbf{x}_n\)</span>.</p>
<p>Remember that under the logistic regression model,
<span class="math display">\[
\Pr(Y_n = 1 \,|\, \mathbf{x}_n)
= \frac{e^{\mathbf{x}_n \cdot \beta}}{1 + e^{\mathbf{x}_n \cdot \beta}}
= \Lambda(\mathbf{x}_n \cdot \beta).
\]</span>
By the same token, <span class="math inline">\(\Pr(Y_n = 0 \,|\, \mathbf{x}_n) = 1 - \Lambda(\mathbf{x}_n \cdot \beta)\)</span>.
Following the same logic as in the case of a sequence of Bernoulli random variables, this means the likelihood function for the logistic regression model is a big ugly product,
<span class="math display">\[
\ell(\beta, \mathbf{Y}, \mathbf{X})
= \prod_{n : Y_n = 0} (1 - \Lambda(\mathbf{x}_n \cdot \beta)) \prod_{n : Y_n = 1} \Lambda(\mathbf{x}_n \cdot \beta).
\]</span>
We will make this easier to work with by taking the log-likelihood,
<span class="math display">\[
\begin{aligned}
\log \ell(\beta, \mathbf{Y}, \mathbf{X})
&amp;= \sum_{n : Y_n = 0} \log (1 - \Lambda(\mathbf{x}_n \cdot \beta)) + \sum_{n : Y_n = 1} \log \Lambda (\mathbf{x}_n \cdot \beta) \\
&amp;= \sum_{n=1}^N \left[ \vphantom{\sum} (1 - Y_n) \log (1 - \Lambda(\mathbf{x}_n \cdot \beta)) + Y_n \log \Lambda (\mathbf{x}_n \cdot \beta) \right]
\end{aligned}
\]</span></p>
<p>If you followed the Bernoulli example, you can guess what comes next.
The log-likelihood is maximized at the point where its partial derivatives all equal zero, so you can’t increase it further by moving in any direction.
So what we’ll do now is take the <em>gradient</em>, or the vector of all partial derivatives,
<span class="math display">\[
\nabla \log \ell(\beta, \mathbf{Y}, \mathbf{X})
= \left( \frac{\partial \log \ell(\beta, \mathbf{Y}, \mathbf{X})}{\partial \beta_1}, \cdots, \frac{\partial \log \ell(\beta, \mathbf{Y}, \mathbf{X})}{\partial \beta_K} \right).
\]</span>
Then we’ll find the maximum of the log-likelihood by setting <span class="math inline">\(\nabla \log \ell(\beta, \mathbf{X}, \mathbf{Y}) = \mathbf{0}\)</span> and solving for <span class="math inline">\(\beta\)</span> (<span class="math inline">\(K\)</span> equations in <span class="math inline">\(K\)</span> unknowns).</p>
<p>Only there’s one problem.
If you try to do that, you can’t explicitly solve for <span class="math inline">\(\beta\)</span> as a function of <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{Y}\)</span>.
There’s no way to finagle the condition such that <span class="math inline">\(\beta\)</span> is on one side and some function of <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{Y}\)</span> are on the other side, the way we did to derive the OLS estimator.
In other words, unlike with OLS and GLS, there’s no formula that gives us the logistic regression coefficients.</p>
<p>Then what is your statistical software doing when you run a logistic regression?
It uses an <em>iterative</em> solver to come up with a best guess for the maximum likelihood estimate.
Without going too deeply into the details, here is what an iterative solver does.</p>
<ol style="list-style-type: decimal">
<li><p>Start with an initial guess for <span class="math inline">\(\hat{\beta}\)</span>.
For example, you might use the OLS coefficients.</p></li>
<li><p>Repeat the following process until it terminates or you run out of patience:</p>
<ol style="list-style-type: lower-alpha">
<li><p>Check whether <span class="math inline">\(\nabla \log \ell(\hat{\beta}, \mathbf{Y}, \mathbf{X}) \approx \mathbf{0}\)</span>. If so, terminate. Otherwise, continue.</p></li>
<li><p>Update your current guess about <span class="math inline">\(\hat{\beta}\)</span> by nudging it in the direction of quickest ascent, namely the gradient <span class="math inline">\(\nabla \log \ell(\hat{\beta}, \mathbf{Y}, \mathbf{X})\)</span>.</p></li>
</ol></li>
</ol>
<p>The standard errors are then calculated using the matrix of second partial derivatives of the log-likelihood.
I will spare you the math here.
The important thing to know about the standard errors is that they are only “correct”, in the sense of properly estimating the degree of uncertainty in your model estimates, asymptotically.
The logistic regression coefficients themselves are consistent but not unbiased, meaning they are only guaranteed to be reliable in sufficiently large samples, and the same is true of the inferential statistics.
If you find yourself in the dubious situation of running a logistic regression on a very small sample—such as because a reviewer asked you do to so in an R&amp;R, because surely you wouldn’t do this voluntarily knowing what you know now—you should not rely on the default standard error computation when performing hypothesis tests.</p>
</div>
</div>
<div id="special-considerations" class="section level2" number="13.4">
<h2><span class="header-section-number">13.4</span> Special Considerations</h2>
<div id="separation" class="section level3" number="13.4.1">
<h3><span class="header-section-number">13.4.1</span> Separation</h3>
</div>
<div id="clustered-standard-errors-1" class="section level3" number="13.4.2">
<h3><span class="header-section-number">13.4.2</span> Clustered Standard Errors</h3>
</div>
</div>
<div id="appendix-implementation-in-r-2" class="section level2" number="13.5">
<h2><span class="header-section-number">13.5</span> Appendix: Implementation in R</h2>
<div id="logistic-regression-1" class="section level3" number="13.5.1">
<h3><span class="header-section-number">13.5.1</span> Logistic Regression</h3>
<p>To illustrate logistic regression in R, we will use data from the <strong>pscl</strong> package on how senators voted on the 2002 resolution to authorize military force against Iraq.</p>
<div class="sourceCode" id="cb358"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb358-1"><a href="logit.html#cb358-1"></a><span class="kw">library</span>(<span class="st">&quot;pscl&quot;</span>)</span>
<span id="cb358-2"><a href="logit.html#cb358-2"></a><span class="kw">head</span>(iraqVote)</span></code></pre></div>
<pre><code>##   y state.abb             name  rep state.name gorevote
## 1 1        AL  SESSIONS (R AL) TRUE    Alabama    41.59
## 2 1        AL    SHELBY (R AL) TRUE    Alabama    41.59
## 3 1        AK MURKOWSKI (R AK) TRUE     Alaska    27.67
## 4 1        AK   STEVENS (R AK) TRUE     Alaska    27.67
## 5 1        AZ       KYL (R AZ) TRUE    Arizona    44.67
## 6 1        AZ    MCCAIN (R AZ) TRUE    Arizona    44.67</code></pre>
<div class="sourceCode" id="cb360"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb360-1"><a href="logit.html#cb360-1"></a><span class="kw">summary</span>(iraqVote)</span></code></pre></div>
<pre><code>##        y          state.abb              name       rep         
##  Min.   :0.00   AK     : 2   AKAKA (D HI)  : 1   Mode :logical  
##  1st Qu.:1.00   AL     : 2   ALLARD (R CO) : 1   FALSE:51       
##  Median :1.00   AR     : 2   ALLEN (R VA)  : 1   TRUE :49       
##  Mean   :0.77   AZ     : 2   BAUCUS (D MT) : 1                  
##  3rd Qu.:1.00   CA     : 2   BAYH (D IN)   : 1                  
##  Max.   :1.00   CO     : 2   BENNETT (R UT): 1                  
##                 (Other):88   (Other)       :94                  
##   state.name           gorevote    
##  Length:100         Min.   :26.34  
##  Class :character   1st Qu.:40.91  
##  Mode  :character   Median :46.22  
##                     Mean   :45.23  
##                     3rd Qu.:50.60  
##                     Max.   :60.99  
## </code></pre>
<p>The poorly labeled <code>y</code> variable here is an indicator for voting in favor of the war.</p>
<p>To run a logistic regression model in R, you simply use the <code>glm()</code> command (stands for <em>generalized</em> linear model) much like you use <code>lm()</code> for ordinary or weighted least squares.
The only difference is that you have to add the <code>family</code> argument to tell R to run a logistic regression.
<code>glm()</code> runs all kinds of models; if you forget the <code>family</code> argument, it will run OLS by default.</p>
<div class="sourceCode" id="cb362"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb362-1"><a href="logit.html#cb362-1"></a>fit_logit &lt;-<span class="st"> </span><span class="kw">glm</span>(y <span class="op">~</span><span class="st"> </span>rep <span class="op">+</span><span class="st"> </span>gorevote,</span>
<span id="cb362-2"><a href="logit.html#cb362-2"></a>                 <span class="dt">data =</span> iraqVote,</span>
<span id="cb362-3"><a href="logit.html#cb362-3"></a>                 <span class="dt">family =</span> <span class="kw">binomial</span>(<span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>))</span>
<span id="cb362-4"><a href="logit.html#cb362-4"></a><span class="kw">summary</span>(fit_logit)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ rep + gorevote, family = binomial(link = &quot;logit&quot;), 
##     data = iraqVote)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.12054   0.07761   0.19676   0.59926   1.59277  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)  5.87859    2.27506   2.584  0.00977 **
## repTRUE      3.01881    1.07138   2.818  0.00484 **
## gorevote    -0.11322    0.04508  -2.512  0.01201 * 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 107.855  on 99  degrees of freedom
## Residual deviance:  71.884  on 97  degrees of freedom
## AIC: 77.884
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<p>To fit a probit model, you would instead use <code>binomial(link = "probit")</code>.</p>
<p>In the summary output, the things about deviances can be safely ignored.
The AIC is a general measure of model fit, which we may talk about in the unlikely event we get to talking about predictive modeling.
Lower values of the AIC indicate better fit.
The Fisher scoring iterations are the number of steps the fitting process took to reach the maximum of the log-likelihood function.</p>
<p>You can extract the predicted probability of <span class="math inline">\(Y_n = 1\)</span> for each observation according to the model using the <code>predict()'</code> function.</p>
<div class="sourceCode" id="cb364"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb364-1"><a href="logit.html#cb364-1"></a>pp_logit &lt;-<span class="st"> </span><span class="kw">predict</span>(fit_logit, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb364-2"><a href="logit.html#cb364-2"></a><span class="kw">head</span>(pp_logit)</span></code></pre></div>
<pre><code>##         1         2         3         4         5         6 
## 0.9850607 0.9850607 0.9968734 0.9968734 0.9789587 0.9789587</code></pre>
<p>If you don’t use the <code>type = "response"</code> argument, then the function will instead return the latent propensity estimates, <span class="math inline">\(\mathbf{x}_n \cdot \hat{\beta}\)</span>, which are almost never what you want.</p>
<p>We can use <code>predict()</code> to calculate average marginal effects and first differences “by hand”.
First let’s use our heuristic to calculate the marginal effect of a state’s Gore vote percentage in the 2000 election on a senator’s probability of voting for the war, assuming that senator would otherwise be 50-50 on voting for the war.</p>
<div class="sourceCode" id="cb366"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb366-1"><a href="logit.html#cb366-1"></a>cf_gore &lt;-<span class="st"> </span><span class="kw">coef</span>(fit_logit)[<span class="st">&quot;gorevote&quot;</span>]</span>
<span id="cb366-2"><a href="logit.html#cb366-2"></a>cf_gore <span class="op">*</span><span class="st"> </span><span class="fl">0.25</span></span></code></pre></div>
<pre><code>##   gorevote 
## -0.0283042</code></pre>
<p>Of course, we saw from the summary statistics that 77% of senators voted for the war.
The marginal effect on a senator who would otherwise be 77-23 in favor is slightly smaller in magnitude:</p>
<div class="sourceCode" id="cb368"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb368-1"><a href="logit.html#cb368-1"></a>cf_gore <span class="op">*</span><span class="st"> </span><span class="fl">0.77</span> <span class="op">*</span><span class="st"> </span><span class="fl">0.23</span></span></code></pre></div>
<pre><code>##   gorevote 
## -0.0200507</code></pre>
<p>We can calculate the individual marginal effects using the predicted probabilities.</p>
<div class="sourceCode" id="cb370"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb370-1"><a href="logit.html#cb370-1"></a>margeff_gore &lt;-<span class="st"> </span>cf_gore <span class="op">*</span><span class="st"> </span>pp_logit <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>pp_logit)</span>
<span id="cb370-2"><a href="logit.html#cb370-2"></a><span class="kw">head</span>(margeff_gore)</span></code></pre></div>
<pre><code>##             1             2             3             4             5 
## -0.0016661114 -0.0016661114 -0.0003528739 -0.0003528739 -0.0023321078 
##             6 
## -0.0023321078</code></pre>
<p>And our best estimate of the average marginal effect will be the average of the individual marginal effects:</p>
<div class="sourceCode" id="cb372"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb372-1"><a href="logit.html#cb372-1"></a><span class="kw">mean</span>(margeff_gore)</span></code></pre></div>
<pre><code>## [1] -0.01343623</code></pre>
<p>Notice that this is substantially smaller than our heuristics would have guessed, since many senators have a very high predicted probability of voting for the war (and thus a marginal effect close to zero).</p>
<p>It is a little more involved to do first differences for a categorical variable by hand.
(Luckily, we will soon see a way to do all of these much more easily.
But you should know how to do them the hard way too.)
First let’s create two synthetic datasets, one where every senator is a Republican and one where every one is a Democrat.</p>
<div class="sourceCode" id="cb374"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb374-1"><a href="logit.html#cb374-1"></a>iraq_rep &lt;-<span class="st"> </span>iraq_dem &lt;-<span class="st"> </span>iraqVote</span>
<span id="cb374-2"><a href="logit.html#cb374-2"></a>iraq_rep<span class="op">$</span>rep &lt;-<span class="st"> </span><span class="ot">TRUE</span></span>
<span id="cb374-3"><a href="logit.html#cb374-3"></a>iraq_dem<span class="op">$</span>rep &lt;-<span class="st"> </span><span class="ot">FALSE</span></span>
<span id="cb374-4"><a href="logit.html#cb374-4"></a><span class="kw">head</span>(iraq_rep)</span></code></pre></div>
<pre><code>##   y state.abb             name  rep state.name gorevote
## 1 1        AL  SESSIONS (R AL) TRUE    Alabama    41.59
## 2 1        AL    SHELBY (R AL) TRUE    Alabama    41.59
## 3 1        AK MURKOWSKI (R AK) TRUE     Alaska    27.67
## 4 1        AK   STEVENS (R AK) TRUE     Alaska    27.67
## 5 1        AZ       KYL (R AZ) TRUE    Arizona    44.67
## 6 1        AZ    MCCAIN (R AZ) TRUE    Arizona    44.67</code></pre>
<div class="sourceCode" id="cb376"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb376-1"><a href="logit.html#cb376-1"></a><span class="kw">head</span>(iraq_dem)</span></code></pre></div>
<pre><code>##   y state.abb             name   rep state.name gorevote
## 1 1        AL  SESSIONS (R AL) FALSE    Alabama    41.59
## 2 1        AL    SHELBY (R AL) FALSE    Alabama    41.59
## 3 1        AK MURKOWSKI (R AK) FALSE     Alaska    27.67
## 4 1        AK   STEVENS (R AK) FALSE     Alaska    27.67
## 5 1        AZ       KYL (R AZ) FALSE    Arizona    44.67
## 6 1        AZ    MCCAIN (R AZ) FALSE    Arizona    44.67</code></pre>
<p>Now we can calculate the average first difference between Republicans and Democrats using the predicted probabilities for each of these synthetic datasets.</p>
<div class="sourceCode" id="cb378"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb378-1"><a href="logit.html#cb378-1"></a>pp_logit_rep &lt;-<span class="st"> </span><span class="kw">predict</span>(fit_logit, <span class="dt">newdata =</span> iraq_rep, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb378-2"><a href="logit.html#cb378-2"></a>pp_logit_dem &lt;-<span class="st"> </span><span class="kw">predict</span>(fit_logit, <span class="dt">newdata =</span> iraq_dem, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb378-3"><a href="logit.html#cb378-3"></a><span class="kw">mean</span>(pp_logit_rep <span class="op">-</span><span class="st"> </span>pp_logit_dem)</span></code></pre></div>
<pre><code>## [1] 0.317038</code></pre>
<p>We can calculate average marginal effects and first differences much more easily using the <strong>margins</strong> package.
As a bonus, it calculates standard errors for us too.
(We’ll talk more about how to do this in the unit on computational methods.)</p>
<div class="sourceCode" id="cb380"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb380-1"><a href="logit.html#cb380-1"></a><span class="kw">library</span>(<span class="st">&quot;margins&quot;</span>)</span>
<span id="cb380-2"><a href="logit.html#cb380-2"></a>meff_logit &lt;-<span class="st"> </span><span class="kw">margins</span>(fit_logit)</span>
<span id="cb380-3"><a href="logit.html#cb380-3"></a><span class="kw">summary</span>(meff_logit)</span></code></pre></div>
<pre><code>##    factor     AME     SE       z      p   lower   upper
##  gorevote -0.0134 0.0043 -3.0949 0.0020 -0.0219 -0.0049
##       rep  0.3170 0.0724  4.3768 0.0000  0.1751  0.4590</code></pre>
<p>So by going through all that trouble, we estimate a minus-1.3 percentage point marginal effect of Gore vote share and a 31 percentage point first difference due to being a Republican.
Let’s compare that to what we get from the linear probability model:</p>
<div class="sourceCode" id="cb382"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb382-1"><a href="logit.html#cb382-1"></a>fit_lpm &lt;-<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>rep <span class="op">+</span><span class="st"> </span>gorevote, <span class="dt">data =</span> iraqVote)</span>
<span id="cb382-2"><a href="logit.html#cb382-2"></a><span class="kw">summary</span>(fit_lpm)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ rep + gorevote, data = iraqVote)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.7654 -0.1533  0.0509  0.2904  0.5707 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.174458   0.236256   4.971 2.87e-06 ***
## repTRUE      0.316933   0.080493   3.937 0.000155 ***
## gorevote    -0.012376   0.004715  -2.625 0.010072 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3603 on 97 degrees of freedom
## Multiple R-squared:  0.2888, Adjusted R-squared:  0.2742 
## F-statistic:  19.7 on 2 and 97 DF,  p-value: 6.617e-08</code></pre>
<p>A minus-1.2 percentage point marginal effect of Gore vote share and a 32 percentage point first difference due to being a Republican.
[Insert gif of the “And that’s why…” guy from <em>Arrested Development</em>.]</p>
</div>
<div id="maximum-likelihood-estimation-1" class="section level3" number="13.5.2">
<h3><span class="header-section-number">13.5.2</span> Maximum Likelihood Estimation</h3>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-davidson1993estimation">
<p>Davidson, Russell, and James G. MacKinnon. 1993. <em>Estimation and Inference in Econometrics</em>. Oxford University Press.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="34">
<li id="fn34"><p>The differences actually go even further than allowing for heteroskedasticity. We can have a homoskedastic linear model where the error variances are the same but their full distributions are different. For example, in the usual linear model we could have <span class="math inline">\(\epsilon_1 \sim U[0, 1]\)</span> and <span class="math inline">\(\epsilon_2 \sim N(0, 1/12)\)</span>, so that <span class="math inline">\(V[\epsilon_1] = V[\epsilon_2] = 1/12\)</span> even though their distributions differ.<a href="logit.html#fnref34" class="footnote-back">↩︎</a></p></li>
<li id="fn35"><p>You <em>can</em> estimate these models with least squares, such as via the nonlinear least functions <code>ns()</code> in R. However, nonlinear least squares is inefficient relative to maximum likelihood estimation, and is usually no simpler to undertake computationally.<a href="logit.html#fnref35" class="footnote-back">↩︎</a></p></li>
<li id="fn36"><p>As it turns out, the OLS formula <em>is</em> the maximum likelihood estimator for the linear model if we assume <span class="math inline">\(\epsilon_n\)</span> is independent across observations and that each <span class="math inline">\(\epsilon_n \sim N(0, \sigma^2)\)</span>.<a href="logit.html#fnref36" class="footnote-back">↩︎</a></p></li>
<li id="fn37"><p>The “MM” subscript denotes the fact that <span class="math inline">\(\hat{\theta}_{\text{MM}}\)</span> is a <em>method of moments</em> estimator of <span class="math inline">\(\theta\)</span>. Like maximum likelihood estimation, the method of moments is a set of principles used to derive estimators.<a href="logit.html#fnref37" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="panel.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/12-logit.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["pdaps.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
