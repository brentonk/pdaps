<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Practical Data Analysis for Political Scientists</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="These are the course notes for Brenton Kenkel’s course PSCI 8357: Statistics for Political Research II. They may or may not also be useful in their own right.">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Practical Data Analysis for Political Scientists" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="These are the course notes for Brenton Kenkel’s course PSCI 8357: Statistics for Political Research II. They may or may not also be useful in their own right." />
  <meta name="github-repo" content="brentonk/pdaps" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Practical Data Analysis for Political Scientists" />
  
  <meta name="twitter:description" content="These are the course notes for Brenton Kenkel’s course PSCI 8357: Statistics for Political Research II. They may or may not also be useful in their own right." />
  

<meta name="author" content="Brenton Kenkel">


<meta name="date" content="2017-02-22">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="ols-matrix.html">
<link rel="next" href="references.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Practical Data Analysis for Political Scientists</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About This Book</a></li>
<li class="chapter" data-level="2" data-path="programming.html"><a href="programming.html"><i class="fa fa-check"></i><b>2</b> Principles of Programming</a><ul>
<li class="chapter" data-level="2.1" data-path="programming.html"><a href="programming.html#write-programs-for-people-not-computers"><i class="fa fa-check"></i><b>2.1</b> Write Programs for People, Not Computers</a></li>
<li class="chapter" data-level="2.2" data-path="programming.html"><a href="programming.html#let-the-computer-do-the-work"><i class="fa fa-check"></i><b>2.2</b> Let the Computer Do the Work</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>3</b> Working with Data</a><ul>
<li class="chapter" data-level="3.1" data-path="data.html"><a href="data.html#loading"><i class="fa fa-check"></i><b>3.1</b> Loading</a></li>
<li class="chapter" data-level="3.2" data-path="data.html"><a href="data.html#tidying"><i class="fa fa-check"></i><b>3.2</b> Tidying</a></li>
<li class="chapter" data-level="3.3" data-path="data.html"><a href="data.html#transforming-and-aggregating"><i class="fa fa-check"></i><b>3.3</b> Transforming and Aggregating</a></li>
<li class="chapter" data-level="3.4" data-path="data.html"><a href="data.html#merging"><i class="fa fa-check"></i><b>3.4</b> Merging</a></li>
<li class="chapter" data-level="3.5" data-path="data.html"><a href="data.html#appendix-creating-the-example-data"><i class="fa fa-check"></i><b>3.5</b> Appendix: Creating the Example Data</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="visualization.html"><a href="visualization.html"><i class="fa fa-check"></i><b>4</b> Data Visualization</a><ul>
<li class="chapter" data-level="4.1" data-path="visualization.html"><a href="visualization.html#basic-plots"><i class="fa fa-check"></i><b>4.1</b> Basic Plots</a></li>
<li class="chapter" data-level="4.2" data-path="visualization.html"><a href="visualization.html#saving-plots"><i class="fa fa-check"></i><b>4.2</b> Saving Plots</a></li>
<li class="chapter" data-level="4.3" data-path="visualization.html"><a href="visualization.html#faceting"><i class="fa fa-check"></i><b>4.3</b> Faceting</a></li>
<li class="chapter" data-level="4.4" data-path="visualization.html"><a href="visualization.html#aesthetics"><i class="fa fa-check"></i><b>4.4</b> Aesthetics</a></li>
<li class="chapter" data-level="4.5" data-path="visualization.html"><a href="visualization.html#appendix-creating-the-example-data-1"><i class="fa fa-check"></i><b>4.5</b> Appendix: Creating the Example Data</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="bivariate.html"><a href="bivariate.html"><i class="fa fa-check"></i><b>5</b> Bivariate Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="bivariate.html"><a href="bivariate.html#probability-refresher"><i class="fa fa-check"></i><b>5.1</b> Probability Refresher</a></li>
<li class="chapter" data-level="5.2" data-path="bivariate.html"><a href="bivariate.html#the-linear-model"><i class="fa fa-check"></i><b>5.2</b> The Linear Model</a></li>
<li class="chapter" data-level="5.3" data-path="bivariate.html"><a href="bivariate.html#least-squares"><i class="fa fa-check"></i><b>5.3</b> Least Squares</a></li>
<li class="chapter" data-level="5.4" data-path="bivariate.html"><a href="bivariate.html#properties"><i class="fa fa-check"></i><b>5.4</b> Properties</a></li>
<li class="chapter" data-level="5.5" data-path="bivariate.html"><a href="bivariate.html#appendix-regression-in-r"><i class="fa fa-check"></i><b>5.5</b> Appendix: Regression in R</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="matrix.html"><a href="matrix.html"><i class="fa fa-check"></i><b>6</b> Matrix Algebra: A Crash Course</a><ul>
<li class="chapter" data-level="6.1" data-path="matrix.html"><a href="matrix.html#vector-operations"><i class="fa fa-check"></i><b>6.1</b> Vector Operations</a></li>
<li class="chapter" data-level="6.2" data-path="matrix.html"><a href="matrix.html#matrix-operations"><i class="fa fa-check"></i><b>6.2</b> Matrix Operations</a></li>
<li class="chapter" data-level="6.3" data-path="matrix.html"><a href="matrix.html#matrix-inversion"><i class="fa fa-check"></i><b>6.3</b> Matrix Inversion</a></li>
<li class="chapter" data-level="6.4" data-path="matrix.html"><a href="matrix.html#solving-linear-systems"><i class="fa fa-check"></i><b>6.4</b> Solving Linear Systems</a></li>
<li class="chapter" data-level="6.5" data-path="matrix.html"><a href="matrix.html#appendix-matrices-in-r"><i class="fa fa-check"></i><b>6.5</b> Appendix: Matrices in R</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ols-matrix.html"><a href="ols-matrix.html"><i class="fa fa-check"></i><b>7</b> Reintroduction to the Linear Model</a><ul>
<li class="chapter" data-level="7.1" data-path="ols-matrix.html"><a href="ols-matrix.html#the-linear-model-in-matrix-form"><i class="fa fa-check"></i><b>7.1</b> The Linear Model in Matrix Form</a></li>
<li class="chapter" data-level="7.2" data-path="ols-matrix.html"><a href="ols-matrix.html#the-ols-estimator"><i class="fa fa-check"></i><b>7.2</b> The OLS Estimator</a></li>
<li class="chapter" data-level="7.3" data-path="ols-matrix.html"><a href="ols-matrix.html#vector-valued-random-variables"><i class="fa fa-check"></i><b>7.3</b> Vector-Valued Random Variables</a></li>
<li class="chapter" data-level="7.4" data-path="ols-matrix.html"><a href="ols-matrix.html#properties-of-ols"><i class="fa fa-check"></i><b>7.4</b> Properties of OLS</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="specification.html"><a href="specification.html"><i class="fa fa-check"></i><b>8</b> Specification Issues</a><ul>
<li class="chapter" data-level="8.1" data-path="specification.html"><a href="specification.html#categorical-variables"><i class="fa fa-check"></i><b>8.1</b> Categorical Variables</a></li>
<li class="chapter" data-level="8.2" data-path="specification.html"><a href="specification.html#interaction-terms"><i class="fa fa-check"></i><b>8.2</b> Interaction Terms</a></li>
<li class="chapter" data-level="8.3" data-path="specification.html"><a href="specification.html#quadratic-and-logarithmic-terms"><i class="fa fa-check"></i><b>8.3</b> Quadratic and Logarithmic Terms</a></li>
<li class="chapter" data-level="8.4" data-path="specification.html"><a href="specification.html#appendix-nonstandard-specifications-in-r"><i class="fa fa-check"></i><b>8.4</b> Appendix: Nonstandard Specifications in R</a><ul>
<li class="chapter" data-level="8.4.1" data-path="specification.html"><a href="specification.html#categorical-variables-1"><i class="fa fa-check"></i><b>8.4.1</b> Categorical Variables</a></li>
<li class="chapter" data-level="8.4.2" data-path="specification.html"><a href="specification.html#interaction-terms-1"><i class="fa fa-check"></i><b>8.4.2</b> Interaction Terms</a></li>
<li class="chapter" data-level="8.4.3" data-path="specification.html"><a href="specification.html#quadratic-and-logarithmic-models"><i class="fa fa-check"></i><b>8.4.3</b> Quadratic and Logarithmic Models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Practical Data Analysis for Political Scientists</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="specification" class="section level1">
<h1><span class="header-section-number">8</span> Specification Issues</h1>
<p>I lied to you about the linear model last week. Like the grade-school teachers who told you everyone thought the world was flat before Columbus proved them wrong, I had good intentions—but it was a lie nonetheless.</p>
<p>I claimed that the linear model assumed that the conditional expectation of the response was a linear function of the covariates. That is false. A data model is a linear model, can be estimated consistently and without bias by OLS, and all that good stuff, as long as it is linear in the <em>parameters</em>.</p>
<p>For example, the following is a linear model. <span class="math display">\[
Y_n = \beta_1 + \beta_2 x_n + \beta_3 x_n^2 + \beta_4 x_n^7 + \epsilon_n.
\]</span> The conditional expectation of <span class="math inline">\(Y_n\)</span> is a nonlinear function of <span class="math inline">\(x_n\)</span> (holding <span class="math inline">\(\beta\)</span> fixed) but a linear function of <span class="math inline">\(\beta\)</span> (holding <span class="math inline">\(x_n\)</span> fixed). Therefore, assuming strict exogeneity holds, OLS is an unbiased, consistent, asymptotically normal estimator of <span class="math inline">\(\beta\)</span>.</p>
<p>The following is not a linear model. <span class="math display">\[
Y_n = 2^{\beta_1} + 2^{\beta_2} x_n + \epsilon_n.
\]</span> Holding <span class="math inline">\(\beta\)</span> fixed, this is a linear function of the covariate <span class="math inline">\(x_n\)</span>. But, holding <span class="math inline">\(x_n\)</span> fixed, this is not a linear function of <span class="math inline">\(\beta\)</span>. OLS is not an appropriate estimator for the parameters of this model.</p>
<p>This week, we will talk about linear models with non-standard covariate specifications—those that aren’t just a linear function of continuous variables.</p>
<div id="categorical-variables" class="section level2">
<h2><span class="header-section-number">8.1</span> Categorical Variables</h2>
<p>Using the linear model, we write the conditional expectation for the <span class="math inline">\(n\)</span>’th response as <span class="math display">\[
E[Y_n \,|\, \mathbf{x}_n] = \mathbf{x}_n \cdot \beta + \epsilon_n,
\]</span> where <span class="math inline">\(\mathbf{x}_n\)</span> is the vector of <span class="math inline">\(K\)</span> covariates (including the intercept) and <span class="math inline">\(\beta\)</span> is the vector of <span class="math inline">\(K\)</span> coefficients we wish to estimate.</p>
<p>This makes sense with numerical variables, but not so much with categorical variables. For example, think of the relationship between party identification and one’s vote in the 2016 presidential election. Suppose our response variable is one’s vote (1 for Trump, 0 for non-Trump), and our party ID variable records whether the respondent is a Republican, Democrat, or independent. The resulting linear model equation, <span class="math display">\[
\text{Trump}_n = \beta_1 + \beta_2 \text{Party ID}_n + \epsilon_n,
\]</span> doesn’t really make sense, because party ID isn’t a number.<a href="#fn21" class="footnoteRef" id="fnref21"><sup>21</sup></a></p>
<p>To incorporate a categorical variable into the linear model, we break each category into its own binary variable. For example, with our party ID variable, we go from <span class="math display">\[
\text{Party ID} = \begin{pmatrix}
\text{R} \\
\text{R} \\
\text{I} \\
\text{I} \\
\text{D} \\
\text{D}
\end{pmatrix}
\]</span> to <span class="math display">\[
\text{Republican} = \begin{pmatrix} 1 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \end{pmatrix},
\text{Independent} = \begin{pmatrix} 0 \\ 0 \\ 1 \\ 1 \\ 0 \\ 0 \end{pmatrix},
\text{Democratic} = \begin{pmatrix} 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 1 \end{pmatrix}.
\]</span> These are called <em>dummy variables</em> or, preferably, <em>indicator variables</em>.</p>
<p>Having turned our categorical variable into a set of indicators, you may be tempted to rewrite the model as <span class="math display">\[
\text{Trump}_n = \beta_1 + \beta_2 \text{Republican}_n + \beta_3 \text{Independent}_n + \beta_4 \text{Democratic}_n + \epsilon_n.
\]</span> But take a look at the matrix of covariates, or <em>design matrix</em>, that would result if we set up the model this way: <span class="math display">\[
\mathbf{X} = \begin{bmatrix}
  1 &amp; 1 &amp; 0 &amp; 0 \\
  1 &amp; 1 &amp; 0 &amp; 0 \\
  1 &amp; 0 &amp; 1 &amp; 0 \\
  1 &amp; 0 &amp; 1 &amp; 0 \\
  1 &amp; 0 &amp; 0 &amp; 1 \\
  1 &amp; 0 &amp; 0 &amp; 1
\end{bmatrix}
\]</span> The columns of the design matrix are linearly dependent: the constant term is equal to the sum of three party ID indicators. (A useful exercise is to calculate <span class="math inline">\(\mathbf{X}^\top \mathbf{X}\)</span> and confirm that its columns are linearly dependent too.) This means we can’t include all three when estimating <span class="math inline">\(\beta\)</span> via OLS—we have to drop one category.</p>
<p>In one sense, which category we drop is immaterial—our regression will make the same predictions either way. However, in order to interpret the results of a regression on categorical variables, it is important that we know what the categories are, and which one has been dropped.</p>
<p>For example, imagine we drop the Republican category, so we have the following linear model: <span class="math display">\[
\text{Trump}_n = \beta_1 + \beta_2 \text{Independent}_n + \beta_3 \text{Democratic}_n + \epsilon_n.
\]</span> For a Republican voter, the Independent and Democratic variables will both equal zero, so we will have <span class="math display">\[
E[\text{Trump}_n \,|\, \text{Party ID}_n = \text{R}] = \beta_1.
\]</span> In other words, the intercept will be the predicted probability that a Republican votes for Trump. For an Independent voter, we will have <span class="math display">\[
E[\text{Trump}_n \,|\, \text{Party ID}_n = \text{I}] = \beta_1 + \beta_2.
\]</span> So the coefficient on the Independent indicator is not the predicted probability that an Independent votes for Trump. Instead, it is the <em>difference</em> in probability of a Trump vote between Independents and the baseline category (in this case, Republicans).</p>
<ul>
<li><p>If Independents are less likely than Republicans to vote for Trump, the coefficient on Independent will be negative.</p></li>
<li><p>If Independents are more likely than Republicans to vote for Trump, the coefficient on Independent will be positive.</p></li>
<li><p>If Independents are equally likely as Republicans to vote for Trump, the coefficient on Independent will be zero.</p></li>
</ul>
<p>Similarly, for a Democratic voter, we have <span class="math display">\[
E[\text{Trump}_n \,|\, \text{Party ID}_n = \text{D}] = \beta_1 + \beta_3.
\]</span> The interpretation of the coefficient on Democratic is the same as for the coefficient on Independent.</p>
<p>Take a look at the following results of a hypothetical regression.</p>
<table>
<thead>
<tr class="header">
<th>Coefficient</th>
<th>Estimate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>(Intercept)</td>
<td>0.9</td>
</tr>
<tr class="even">
<td>Independent</td>
<td>-0.4</td>
</tr>
<tr class="odd">
<td>Democratic</td>
<td>-0.75</td>
</tr>
</tbody>
</table>
<ul>
<li><p>Republicans have a 90% chance of voting for Trump. We see that by looking at the intercept, since Republicans are the omitted category.</p></li>
<li><p>We see from the coefficient on Independent that an Independent’s chance of voting for Trump is 40% lower than a Republican’s. This means that an Independent has a 50% chance of voting for Trump.</p></li>
<li><p>Similarly, we see from the coefficient on Democratic that a Democrat’s chance of voting for Trump is 75% lower than a Republican’s, for a 15% chance overall.</p></li>
</ul>
<p>Had we instead omitted Independent, we would get different coefficients, but the same predictions.</p>
<table>
<thead>
<tr class="header">
<th>Coefficient</th>
<th>Estimate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>(Intercept)</td>
<td>0.5</td>
</tr>
<tr class="even">
<td>Republican</td>
<td>0.4</td>
</tr>
<tr class="odd">
<td>Democratic</td>
<td>-0.35</td>
</tr>
</tbody>
</table>
<p>Same story, different numbers, had we omitted Democratic.</p>
<table>
<thead>
<tr class="header">
<th>Coefficient</th>
<th>Estimate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>(Intercept)</td>
<td>0.15</td>
</tr>
<tr class="even">
<td>Republican</td>
<td>0.75</td>
</tr>
<tr class="odd">
<td>Independent</td>
<td>0.35</td>
</tr>
</tbody>
</table>
<p>Given that the results are substantively the same no matter what, does it matter which category we choose to drop? Yes, for the purpose of communicating your results. The omitted category should serve as a meaningful baseline. For this example, all of our three categories are substantively meaningful, so any choice will do. But imagine replacing our part ID variable with a race variable that has the following categories:</p>
<ul>
<li>White</li>
<li>Black</li>
<li>Hispanic</li>
<li>Asian</li>
<li>Other</li>
</ul>
<p>You may be tempted to make “Other” the excluded category, so that you obtain a coefficient for each of the specific racial groups. But that’s actually the worst choice possible. The coefficient on the White variable would then represent the difference in probability of voting for Trump between a white voter and a voter in the “other” category—which is hard to interpret. Whereas if we instead omitted the Black category, the coefficient on the White variable would represent the difference between white and black voters.</p>
<p>When in doubt, I recommend omitting whichever category is largest in the data.</p>
<p>Now let’s introduce covariates into the mix. Consider a regression of Trump vote on party ID (Republican as omitted category) and age, producing the following results.</p>
<table>
<thead>
<tr class="header">
<th>Coefficient</th>
<th>Estimate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>(Intercept)</td>
<td>0.8</td>
</tr>
<tr class="even">
<td>Independent</td>
<td>-0.4</td>
</tr>
<tr class="odd">
<td>Democratic</td>
<td>-0.75</td>
</tr>
<tr class="even">
<td>Age</td>
<td>0.002</td>
</tr>
</tbody>
</table>
<p>Remember what the coefficient of 0.002 on Age means: if we compared one voter to another who was otherwise identical (in this case, same Party ID) except five years older, we would expect the latter voter to have a 1% greater chance of voting for Trump.</p>
<p>More specifically, we have three different regression lines—one for each group: <span class="math display">\[
\begin{aligned}
  E[\text{Trump}_n \,|\, \text{Party ID}_n = \text{R}, \text{Age}_n] &amp;= 0.8 + 0.002 \text{Age}_n, \\
  E[\text{Trump}_n \,|\, \text{Party ID}_n = \text{I}, \text{Age}_n] &amp;= 0.4 + 0.002 \text{Age}_n, \\
  E[\text{Trump}_n \,|\, \text{Party ID}_n = \text{D}, \text{Age}_n] &amp;= 0.05 + 0.002 \text{Age}_n.
\end{aligned}
\]</span> Notice that the slope is the same in each regression line. Only the intercept varies across groups. When we include a categorical variable in a regression model, it’s like allowing the intercept to differ across categories.</p>
</div>
<div id="interaction-terms" class="section level2">
<h2><span class="header-section-number">8.2</span> Interaction Terms</h2>
<p>When political scientists or economists describe their regression results, they will often talk about the marginal effects of different variables. Formally, the <em>marginal effect</em> of the <span class="math inline">\(k\)</span>’th covariate, <span class="math inline">\(x_{nk}\)</span>, is <span class="math display">\[
{\frac{\partial E[Y_n \,|\, \mathbf{x}_n]}{\partial x_{nk}}},
\]</span> the partial derivative of the conditional expectation with respect to the <span class="math inline">\(k\)</span>’th covariate.</p>
<p>The marginal effect answers the following question: Suppose we have two observations that differ in the <span class="math inline">\(k\)</span>’th covariate by one unit, but are otherwise identical. How much greater, or less, would we expect the response to be for the observation with the one-unit-greater value of <span class="math inline">\(x_{nk}\)</span>?</p>
<p>If we were sure the relationship we were modeling were causal, we could phrase the above question more succinctly. We could ask: Given a one-unit change in the <span class="math inline">\(k\)</span>’th covariate, holding all else fixed, what change in the response should we expect? But we haven’t yet gotten to the point where we can make our claims causal. Hence I will often refer to <em>so-called marginal effects</em>, since I don’t want the “effect” terminology to deceive us into thinking we’re drawing causal inferences. So-called marginal effects are just a nice way to summarize the relationship between individual covariates and the conditional expectation.</p>
<p>The bare-bones linear model has the (sometimes appealing, sometimes not) feature that it assumes constant marginal effects. For each covariate <span class="math inline">\(x_{nk}\)</span>, we have <span class="math display">\[
{\frac{\partial E[Y_n \,|\, \mathbf{x}_n]}{\partial x_{nk}}} = \beta_k,
\]</span> the coefficient on that covariate. This encodes two critical assumptions:</p>
<ol style="list-style-type: decimal">
<li><p>The marginal effect of the <span class="math inline">\(k\)</span>’th covariate does not depend on the value of any other covariates.</p></li>
<li><p>The marginal effect of the <span class="math inline">\(k\)</span>’th covariate does not depend on its own value.</p></li>
</ol>
<p>It is easy to think of scenarios where each of these might be questionable.</p>
<ol style="list-style-type: decimal">
<li><p>Imagine a study of individual voters’ choices in U.S. House races, where we model voting for the incumbent as a function of how often the voter goes to church. The marginal effect of religiosity is probably different if the incumbent is a Republican than if the incumbent is a Democrat.</p></li>
<li><p>Imagine a study of individual voters’ turnout decisions, where we model turnout as a function of the voter’s ideology. Suppose ideology is measured on a 7-point scale, where 1 is most liberal and 7 is most conservative. We know the most ideologically extreme voters are the most likely to turn out. So, all else equal, we’d expect moving from 1 to 2 (very liberal to pretty liberal) to decrease one’s probability of voting, but we’d expect moving from 6 to 7 (pretty conservative to very conservative) to increase one’s probability of voting.</p></li>
</ol>
<p>Let’s start with the first case, where the (so-called) marginal effect of one variable depends on the value of another variable. To allow for this in our models, we include the product of the two covariates in our model.</p>
<p>For example, suppose we are interested in whether the relationship between education and voting for Trump is different between whites and non-whites. We would include three terms in the model (plus an intercept): education, an indicator for white, and their product. <span class="math display">\[
\text{Trump}_n = \beta_1 + \beta_2 \text{Education}_n + \beta_3 \text{White}_n + \beta_4 (\text{Education}_n \times \text{White}_n) + \epsilon_n.
\]</span> The so-called marginal effect of education is now <span class="math display">\[
{\frac{\partial E[\text{Trump}_n \,|\, \mathbf{x}_n]}{\partial \text{Education}_n}}
= \beta_2 + \beta_4 \text{White}_n.
\]</span> This equation tells us three things.</p>
<ul>
<li><p><span class="math inline">\(\beta_2\)</span> is the marginal effect of education for non-white voters.</p></li>
<li><p><span class="math inline">\(\beta_4\)</span> is the difference between the marginal effect of education for white voters and the effect for non-white voters.</p></li>
<li><p><span class="math inline">\(\beta_2 + \beta_4\)</span> is the marginal effect of education for white voters.</p></li>
</ul>
<p>Another way to think of it is that we have two regression lines: <span class="math display">\[
\begin{aligned}
  E[\text{Trump}_n \,|\, \text{White}_n = 0, \text{Education}_n] &amp;= \beta_1 + \beta_2 \text{Education}_n, \\
  E[\text{Trump}_n \,|\, \text{White}_n = 1, \text{Education}_n] &amp;= (\beta_1 + \beta_3) + (\beta_2 + \beta_4) \text{Education}_n.
\end{aligned}
\]</span> We saw before that including a categorical variable is like allowing a different intercept for each category. Including an interaction with a categorical variable is like allowing a different slope for each category.</p>
<p>At this point, you might ask, why not just run two separate regressions? I can think of at least two reasons not to.</p>
<ul>
<li><p>You might want to include other covariates whose effects you don’t think are dependent on race (e.g., age). If you ran separate regressions, you would estimate race-dependent effects for every covariate, at a potential loss of efficiency.</p></li>
<li><p>You might want to formally test the hypothesis that the effect of education is equal for whites and non-whites. This is easiest to do if you have a single model. Next week we will talk about the tools you would need to undertake this sort of test.</p></li>
</ul>
<p>One frequent source of confusion with interaction terms is whether you need to include lower-order terms in the model. For example, if we are only interested in how the effect of education differs with race, why can’t we just include education and its product with race in the specification? The equations above give you the answer. Leaving the white indicator out of the model is like fixing <span class="math inline">\(\beta_3 = 0\)</span>. This means you’re forcing the regression lines for whites and non-whites to have the same intercept, which there’s no good reason to do.</p>
<p>If you’re not yet persuaded on the necessity of including constitutive terms of interactions in your regressions, see <span class="citation">Braumoeller (<a href="#ref-braumoeller2004hypothesis">2004</a>)</span>.</p>
<p>For an example of interaction terms, imagine the following example. Suppose education is measured in years of schooling.</p>
<table>
<thead>
<tr class="header">
<th>Coefficient</th>
<th>Estimate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>(Intercept)</td>
<td>0.3</td>
</tr>
<tr class="even">
<td>Education</td>
<td>0.01</td>
</tr>
<tr class="odd">
<td>White</td>
<td>0.4</td>
</tr>
<tr class="even">
<td>Education * White</td>
<td>-0.03</td>
</tr>
</tbody>
</table>
<p>We would interpret these in the following way.</p>
<ul>
<li><p>A hypothetical non-white voter with zero years of education has a 30% chance of voting for Trump. For each additional year of education, the probability of voting for Trump goes up by 1%.</p></li>
<li><p>A hypothetical white voter with zero years of education has a 70% (0.3 + 0.4) chance of voting for Trump. For each additional year of education, the probability of voting for Trump goes down by 2% (0.01 - 0.03).</p></li>
</ul>
<p>What about an interaction between two continuous variables? For example, imagine an interaction between age and education in our model of voting for Trump.</p>
<table>
<thead>
<tr class="header">
<th>Coefficient</th>
<th>Estimate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>(Intercept)</td>
<td>0.4</td>
</tr>
<tr class="even">
<td>Education</td>
<td>-0.02</td>
</tr>
<tr class="odd">
<td>Age</td>
<td>0.002</td>
</tr>
<tr class="even">
<td>Education * Age</td>
<td>0.0002</td>
</tr>
</tbody>
</table>
<p>One simple way to interpret the effect of each variable is to hold the other one fixed at various values. For example, for a 20-year-old, we have <span class="math display">\[
\begin{aligned}
E[\text{Trump}_n \,|\, \text{Age}_n = 20, \text{Education}_n] &amp;= 0.44 - 0.16 \text{Education}_n,
\end{aligned}
\]</span> whereas for an 80-year-old, we have <span class="math display">\[
\begin{aligned}
E[\text{Trump}_n \,|\, \text{Age}_n = 80, \text{Education}_n] &amp;= 0.56 - 0.04 \text{Education}_n.
\end{aligned}
\]</span> These results would seem to imply that (1) older people have a higher baseline probability of voting for Trump, and (2) the magnitude of the negative relationship between education and voting for Trump is weaker for older voters.</p>
<p>Always remember: when in doubt, take the partial derivative of <span class="math inline">\(Y_n\)</span> (or, more precisely, its conditional expectation) with respect to the variable you’re interested in. For example, here we have <span class="math display">\[
{\frac{\partial E[\text{Trump}_n \,|\, \mathbf{x}_n]}{\partial \text{Education}_n}} = -0.02 + 0.0002 \text{Age}_n.
\]</span></p>
</div>
<div id="quadratic-and-logarithmic-terms" class="section level2">
<h2><span class="header-section-number">8.3</span> Quadratic and Logarithmic Terms</h2>
<p>We use interaction terms when the marginal effect of one variable depends on the value of another variable. But sometimes the marginal effect of a variable depends on its <em>own</em> value. The most stark example is a “U-shaped” relationship, such as we expect between ideology and voter turnout.</p>
<p><img src="pdaps_files/figure-html/u-shaped-1.png" width="672" /></p>
<p>We call this kind of relationship <em>non-monotonic</em>, since it is neither increasing everywhere nor decreasing everywhere. However, even with a monotonic relationship, the marginal effect might depend on the value of the variable. (In other words, while every linear function is monotonic, not every monotonic function is linear.) For example, think of a hockey-stick shaped relationship.</p>
<p><img src="pdaps_files/figure-html/hockey-stick-1.png" width="672" /></p>
<p>If we model the relationship between years of education and voting for Trump as linear, then we impose the assumption that the difference between voters with 16 years of education and 12 years of education (college versus high-school graduates) is the same as between those with 24 and 20 years of education (got the PhD slowly versus got the PhD quickly). Depending on our sample and the goal of our study, that may not be a reasonable assumption (or approximation).</p>
<p>We typically use quadratic models for non-monotonic relationships. In the example above, this would entail a regression model like <span class="math display">\[
\text{Turnout}_n = \beta_1 + \beta_2 \text{Ideology}_n + \beta_3 \text{Ideology}_n^2 + \epsilon_n.
\]</span> Under this model, the marginal effect of Ideology is <span class="math display">\[
{\frac{\partial E[\text{Turnout}_n \,|\, \text{Ideology}_n]}{\partial \text{Ideology}_n}}
= \beta_2 + 2 \beta_3 \text{Ideology}_n.
\]</span></p>
<ul>
<li><p>If <span class="math inline">\(\beta_3\)</span> is positive, that means the effect of Ideology increases with the value of Ideology, representing a U-shaped relationship.</p></li>
<li><p>If <span class="math inline">\(\beta_3\)</span> is negative, that means the effect of Ideology decreases with the value of Ideology, representing an inverse-U-shaped relationship.</p></li>
<li><p>If <span class="math inline">\(beta_3\)</span> is zero, that means the effect of Ideology is constant.</p></li>
</ul>
<p>The other main way to model a nonlinear relationship in a single variable is with a logarithmic model. Remember that, in the standard bivariate linear model, <span class="math display">\[
Y_n = \beta_1 + \beta_2 x_n + \epsilon_n,
\]</span> we say that a 1-unit difference in <span class="math inline">\(x_n\)</span> is associated with a <span class="math inline">\(\beta_2\)</span>-unit difference in <span class="math inline">\(Y_n\)</span>. If we were to instead model the natural logarithm of the response, <span class="math display">\[
\log Y_n = \beta_1 + \beta_2 x_n + \epsilon_n,
\]</span> then we would say that a 1-unit difference in <span class="math inline">\(x_n\)</span> is associated with a <span class="math inline">\(\beta_2\)</span>-<em>percent</em> difference in <span class="math inline">\(Y_n\)</span>. So, for example, if <span class="math inline">\(x_n\)</span> and <span class="math inline">\(Y_n\)</span> are exactly proportional to each other (tripling <span class="math inline">\(x_n\)</span> leads to a tripling of <span class="math inline">\(Y_n\)</span>, for example), we would have <span class="math inline">\(\beta_2 = 1\)</span>. Conversely, if we were to model the response as a function of the natural logarithm of the covariate, <span class="math display">\[
Y_n = \beta_1 + \beta_2 \log x_n + \epsilon_n,
\]</span> then we would say a 1-percent difference in <span class="math inline">\(x_n\)</span> is associated with a <span class="math inline">\(\beta_2\)</span>-unit difference in <span class="math inline">\(Y_n\)</span>. Finally, in a full log-log model, <span class="math display">\[
\log Y_n = \beta_1 + \beta_2 \log x_n + \epsilon_n,
\]</span> we would say that a 1-percent difference in <span class="math inline">\(x_n\)</span> is associated with a <span class="math inline">\(\beta_2\)</span>-percent difference in <span class="math inline">\(Y_n\)</span>.</p>
<p>How do you decide which logarithmic model, if any, to use?</p>
<ul>
<li><p>You may let theory be your guide—develop expectations, on the basis of your substantive knowledge, about whether the relevant changes in conditional expectation will be in terms of levels or proportions.</p></li>
<li><p>Or you may be inductive—make scatterplots of all four possibilities, and choose the specification under which the relationship is closest to linear.</p></li>
</ul>
<p>A final note on logarithmic models. The logarithm of a number <span class="math inline">\(c \leq 0\)</span> does not exist. Therefore, a logarithmic model is only appropriate for a strictly positive response/covariate. For non-negative variables that include zeroes, some people try to “fix” this by doing <span class="math inline">\(\log(Y_n + 1)\)</span>, but in this situation it is generally better to follow the procedure advocated by <span class="citation">Burbidge, Magee, and Robb (<a href="#ref-burbidge1988alternative">1988</a>)</span>.</p>
</div>
<div id="appendix-nonstandard-specifications-in-r" class="section level2">
<h2><span class="header-section-number">8.4</span> Appendix: Nonstandard Specifications in R</h2>
<p>We will use the following packages:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">&quot;tidyverse&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;broom&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;forcats&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;interplot&quot;</span>)</code></pre></div>
<p><strong>forcats</strong> contains convenience functions for <em>factors</em>, which are R’s way of representing categorical variables. <strong>interplot</strong> is for plotting marginal effects from interactive models.</p>
<p>Once again, we will work with the occupational prestige data from the <strong>car</strong> package. We will also convert it to a tibble, so that it will more closely resemble the kind of data frame we would get had we read it in with <code>read_csv()</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">&quot;car&quot;</span>)
<span class="kw">data</span>(Prestige)
Prestige &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(Prestige)

Prestige</code></pre></div>
<pre><code>## # A tibble: 102 × 6
##   education income women prestige census   type
## *     &lt;dbl&gt;  &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;int&gt; &lt;fctr&gt;
## 1     13.11  12351 11.16     68.8   1113   prof
## 2     12.26  25879  4.02     69.1   1130   prof
## 3     12.77   9271 15.70     63.4   1171   prof
## 4     11.42   8865  9.11     56.8   1175   prof
## 5     14.62   8403 11.68     73.5   2111   prof
## # ... with 97 more rows</code></pre>
<div id="categorical-variables-1" class="section level3">
<h3><span class="header-section-number">8.4.1</span> Categorical Variables</h3>
<p>You will notice that the <code>type</code> column of the prestige data is listed as a <code>&lt;fctr&gt;</code>, which stands for factor. A factor is R’s representation of a categorical variable. Let’s take a closer look.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Prestige<span class="op">$</span>type</code></pre></div>
<pre><code>##   [1] prof prof prof prof prof prof prof prof prof prof prof prof prof prof
##  [15] prof prof prof prof prof prof prof prof prof prof prof prof prof bc  
##  [29] prof prof wc   prof wc   &lt;NA&gt; wc   wc   wc   wc   wc   wc   wc   wc  
##  [43] wc   wc   wc   wc   wc   wc   wc   wc   wc   wc   &lt;NA&gt; bc   wc   wc  
##  [57] wc   bc   bc   bc   bc   bc   &lt;NA&gt; bc   bc   bc   &lt;NA&gt; bc   bc   bc  
##  [71] bc   bc   bc   bc   bc   bc   bc   bc   bc   bc   bc   bc   bc   bc  
##  [85] bc   bc   bc   bc   bc   bc   bc   bc   bc   bc   bc   prof bc   bc  
##  [99] bc   bc   bc   bc  
## Levels: bc prof wc</code></pre>
<p>This looks kind of like—but isn’t quite—a character variable. The most noticeable difference is that R tells us its <em>levels</em>: the set of categories available. We can extract these directly with the <code>levels()</code> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">levels</span>(Prestige<span class="op">$</span>type)</code></pre></div>
<pre><code>## [1] &quot;bc&quot;   &quot;prof&quot; &quot;wc&quot;</code></pre>
<p>This brings us to an important difference between <code>read.csv()</code> (with a dot, the built-in R function) and <code>read_csv()</code> (with an underscore, the tidyverse version). The old way, <code>read.csv()</code>, by default treats any column of character strings as a factor. The tidyverse way, <code>read_csv()</code>, never creates factors by default—you must make them explicitly.</p>
<p>To create a factor variable, you can use the <code>factor()</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">example_vector &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">4</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">1</span>)

<span class="kw">factor</span>(example_vector,  <span class="co"># vector to convert to factor</span>
       <span class="dt">levels =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>,    <span class="co"># possible values of the vector</span>
       <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;ONE&quot;</span>,
                  <span class="st">&quot;two&quot;</span>,
                  <span class="st">&quot;Three&quot;</span>,
                  <span class="st">&quot;FOUR!&quot;</span>))  <span class="co"># label corresponding to each value</span></code></pre></div>
<pre><code>## [1] FOUR! ONE   Three Three ONE  
## Levels: ONE two Three FOUR!</code></pre>
<p>Returning to the prestige data, let’s run a regression of occupational prestige on occupational category.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_type &lt;-<span class="st"> </span><span class="kw">lm</span>(prestige <span class="op">~</span><span class="st"> </span>type, <span class="dt">data =</span> Prestige)
fit_type</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = prestige ~ type, data = Prestige)
## 
## Coefficients:
## (Intercept)     typeprof       typewc  
##       35.53        32.32         6.72</code></pre>
<p><code>lm()</code> automatically converts the factor into a set of indicators, and automatically omits one category from the design matrix. In particular, it omits whichever level is listed first (which may not be the first level to appear in the data!). If you want to have a different category omitted, you need to reorder the levels, placing the category you want to omit first. You can do that with <code>fct_relevel()</code> from the <strong>forcats</strong> package. Let’s make white-collar (<code>wc</code>) the omitted category.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Prestige<span class="op">$</span>type &lt;-<span class="st"> </span><span class="kw">fct_relevel</span>(Prestige<span class="op">$</span>type, <span class="st">&quot;wc&quot;</span>)
<span class="kw">levels</span>(Prestige<span class="op">$</span>type)</code></pre></div>
<pre><code>## [1] &quot;wc&quot;   &quot;bc&quot;   &quot;prof&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_type_relevel &lt;-<span class="st"> </span><span class="kw">lm</span>(prestige <span class="op">~</span><span class="st"> </span>type, <span class="dt">data =</span> Prestige)</code></pre></div>
<p>We can confirm by checking out the model fit statistics that which category we omit makes no difference to the overall fit of the model, or the predicted values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">glance</span>(fit_type)</code></pre></div>
<pre><code>##   r.squared adj.r.squared  sigma statistic    p.value df  logLik    AIC
## 1   0.69763       0.69126 9.4986    109.59 2.1168e-25  3 -358.15 724.29
##      BIC deviance df.residual
## 1 734.63   8571.3          95</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">glance</span>(fit_type_relevel)  <span class="co"># Should be the same</span></code></pre></div>
<pre><code>##   r.squared adj.r.squared  sigma statistic    p.value df  logLik    AIC
## 1   0.69763       0.69126 9.4986    109.59 2.1168e-25  3 -358.15 724.29
##      BIC deviance df.residual
## 1 734.63   8571.3          95</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># fitted() extracts the fitted value for each observation</span>
<span class="kw">all.equal</span>(<span class="kw">fitted</span>(fit_type), <span class="kw">fitted</span>(fit_type_relevel))</code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<p>One more thing—it’s rare you should need to do this, but you can use <code>model.matrix()</code> to extract the design matrix for a fitted regression model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">X &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(fit_type)
<span class="kw">dim</span>(X)</code></pre></div>
<pre><code>## [1] 98  3</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(X)</code></pre></div>
<pre><code>##                     (Intercept) typeprof typewc
## gov.administrators            1        1      0
## general.managers              1        1      0
## accountants                   1        1      0
## purchasing.officers           1        1      0
## chemists                      1        1      0
## physicists                    1        1      0</code></pre>
</div>
<div id="interaction-terms-1" class="section level3">
<h3><span class="header-section-number">8.4.2</span> Interaction Terms</h3>
<p>The syntax for an interactive model is pretty intuitive. Let’s look at the joint effect of education and income on occupational prestige.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_interactive &lt;-<span class="st"> </span><span class="kw">lm</span>(prestige <span class="op">~</span><span class="st"> </span>education <span class="op">*</span><span class="st"> </span>income, <span class="dt">data =</span> Prestige)
fit_interactive</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = prestige ~ education * income, data = Prestige)
## 
## Coefficients:
##      (Intercept)         education            income  education:income  
##        -2.21e+01          5.37e+00          3.94e-03         -1.96e-04</code></pre>
<p>Notice that <code>lm()</code> automatically included the lower-order terms for us, so we didn’t have to remember which terms to put in. This is particularly useful when you are interacting with a categorical variable that has many categories, or when you are including higher-order interactions.</p>
<p>If you want to plot the (so-called) marginal effect of education as a function of income, you can use the handy function from the <strong>interplot</strong> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">interplot</span>(fit_interactive, <span class="dt">var1 =</span> <span class="st">&quot;education&quot;</span>, <span class="dt">var2 =</span> <span class="st">&quot;income&quot;</span>)</code></pre></div>
<p><img src="pdaps_files/figure-html/interplot-1.png" width="672" /></p>
<p>We see that the marginal effect of education on prestige is high for low-income occupations, but almost nil for an occupation that earns around $25,000/year. (The bars represent a confidence interval—of course, we haven’t done any inferential statistics yet.)</p>
</div>
<div id="quadratic-and-logarithmic-models" class="section level3">
<h3><span class="header-section-number">8.4.3</span> Quadratic and Logarithmic Models</h3>
<p>The syntax for a quadratic model is a bit weird. You would think you could use a formula like <code>y ~ x + x^2</code>, but that won’t work. Instead, you have to write <code>y ~ x + I(x^2)</code>, as in the following example.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_quad &lt;-<span class="st"> </span><span class="kw">lm</span>(prestige <span class="op">~</span><span class="st"> </span>education <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(education<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span>income, <span class="dt">data =</span> Prestige)
fit_quad</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = prestige ~ education + I(education^2) + income, 
##     data = Prestige)
## 
## Coefficients:
##    (Intercept)       education  I(education^2)          income  
##       10.97951         0.77477         0.15373         0.00128</code></pre>
<p>The easiest way to visualize the results of a quadratic model is to create a synthetic dataset, where you vary the relevant variable across its range while holding all the other variables fixed at the same value. Then plug the synthetic dataset into the model to get predicted values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">synthetic_data &lt;-<span class="st"> </span><span class="kw">data_frame</span>(
    <span class="dt">education =</span> <span class="kw">seq</span>(<span class="kw">min</span>(Prestige<span class="op">$</span>education),
                    <span class="kw">max</span>(Prestige<span class="op">$</span>education),
                    <span class="dt">length.out =</span> <span class="dv">100</span>),
    <span class="dt">income =</span> <span class="kw">mean</span>(Prestige<span class="op">$</span>income)
)

synthetic_data &lt;-<span class="st"> </span><span class="kw">augment</span>(fit_quad, <span class="dt">newdata =</span> synthetic_data)
<span class="kw">head</span>(synthetic_data)</code></pre></div>
<pre><code>##   education income .fitted .se.fit
## 1    6.3800 6797.9  30.855  2.2842
## 2    6.4769 6797.9  31.122  2.1959
## 3    6.5737 6797.9  31.391  2.1104
## 4    6.6706 6797.9  31.663  2.0280
## 5    6.7675 6797.9  31.939  1.9486
## 6    6.8643 6797.9  32.217  1.8723</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(synthetic_data, <span class="kw">aes</span>(<span class="dt">x =</span> education, <span class="dt">y =</span> .fitted)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_line</span>()</code></pre></div>
<p><img src="pdaps_files/figure-html/plot-quadratic-1.png" width="672" /></p>
<p>In this case, within the range spanned by the sample data, the estimated quadratic relationship is only barely nonlinear.</p>
<p>Finally, to run a logarithmic model, just put <code>log()</code> around the variables you want to log.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_log &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(prestige) <span class="op">~</span><span class="st"> </span><span class="kw">log</span>(income) <span class="op">+</span><span class="st"> </span>education, <span class="dt">data =</span> Prestige)
fit_log</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = log(prestige) ~ log(income) + education, data = Prestige)
## 
## Coefficients:
## (Intercept)  log(income)    education  
##      0.3373       0.2991       0.0789</code></pre>
<p>To visualize the resulting relationship, you can use the same technique as for quadratic models.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-braumoeller2004hypothesis">
<p>Braumoeller, Bear F. 2004. “Hypothesis Testing and Multiplicative Interaction Terms.” <em>International Organization</em> 58 (04). Cambridge University Press: 807–20.</p>
</div>
<div id="ref-burbidge1988alternative">
<p>Burbidge, John B, Lonnie Magee, and A Leslie Robb. 1988. “Alternative Transformations to Handle Extreme Values of the Dependent Variable.” <em>Journal of the American Statistical Association</em> 83 (401). Taylor &amp; Francis: 123–27.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="21">
<li id="fn21"><p>Oldish-school political scientists, by which I mean just about anyone who got their PhD between 1990 and 2010, would now be clutching their pearls at the mere thought of using a linear model for vote choice. But so-called “binary dependent variable” models like logistic regression are at best overrated, as you read in <em>Mostly Harmless Econometrics</em>.<a href="specification.html#fnref21">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ols-matrix.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/07-specification.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
