<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Practical Data Analysis for Political Scientists</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="These are the course notes for Brenton Kenkel’s course PSCI 8357: Statistics for Political Research II. They may or may not also be useful in their own right.">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Practical Data Analysis for Political Scientists" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="These are the course notes for Brenton Kenkel’s course PSCI 8357: Statistics for Political Research II. They may or may not also be useful in their own right." />
  <meta name="github-repo" content="brentonk/pdaps" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Practical Data Analysis for Political Scientists" />
  
  <meta name="twitter:description" content="These are the course notes for Brenton Kenkel’s course PSCI 8357: Statistics for Political Research II. They may or may not also be useful in their own right." />
  

<meta name="author" content="Brenton Kenkel">


<meta name="date" content="2017-02-20">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="ols-matrix.html">
<link rel="next" href="references.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Practical Data Analysis for Political Scientists</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About This Book</a></li>
<li class="chapter" data-level="2" data-path="programming.html"><a href="programming.html"><i class="fa fa-check"></i><b>2</b> Principles of Programming</a><ul>
<li class="chapter" data-level="2.1" data-path="programming.html"><a href="programming.html#write-programs-for-people-not-computers"><i class="fa fa-check"></i><b>2.1</b> Write Programs for People, Not Computers</a></li>
<li class="chapter" data-level="2.2" data-path="programming.html"><a href="programming.html#let-the-computer-do-the-work"><i class="fa fa-check"></i><b>2.2</b> Let the Computer Do the Work</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>3</b> Working with Data</a><ul>
<li class="chapter" data-level="3.1" data-path="data.html"><a href="data.html#loading"><i class="fa fa-check"></i><b>3.1</b> Loading</a></li>
<li class="chapter" data-level="3.2" data-path="data.html"><a href="data.html#tidying"><i class="fa fa-check"></i><b>3.2</b> Tidying</a></li>
<li class="chapter" data-level="3.3" data-path="data.html"><a href="data.html#transforming-and-aggregating"><i class="fa fa-check"></i><b>3.3</b> Transforming and Aggregating</a></li>
<li class="chapter" data-level="3.4" data-path="data.html"><a href="data.html#merging"><i class="fa fa-check"></i><b>3.4</b> Merging</a></li>
<li class="chapter" data-level="3.5" data-path="data.html"><a href="data.html#appendix-creating-the-example-data"><i class="fa fa-check"></i><b>3.5</b> Appendix: Creating the Example Data</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="visualization.html"><a href="visualization.html"><i class="fa fa-check"></i><b>4</b> Data Visualization</a><ul>
<li class="chapter" data-level="4.1" data-path="visualization.html"><a href="visualization.html#basic-plots"><i class="fa fa-check"></i><b>4.1</b> Basic Plots</a></li>
<li class="chapter" data-level="4.2" data-path="visualization.html"><a href="visualization.html#saving-plots"><i class="fa fa-check"></i><b>4.2</b> Saving Plots</a></li>
<li class="chapter" data-level="4.3" data-path="visualization.html"><a href="visualization.html#faceting"><i class="fa fa-check"></i><b>4.3</b> Faceting</a></li>
<li class="chapter" data-level="4.4" data-path="visualization.html"><a href="visualization.html#aesthetics"><i class="fa fa-check"></i><b>4.4</b> Aesthetics</a></li>
<li class="chapter" data-level="4.5" data-path="visualization.html"><a href="visualization.html#appendix-creating-the-example-data-1"><i class="fa fa-check"></i><b>4.5</b> Appendix: Creating the Example Data</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="bivariate.html"><a href="bivariate.html"><i class="fa fa-check"></i><b>5</b> Bivariate Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="bivariate.html"><a href="bivariate.html#probability-refresher"><i class="fa fa-check"></i><b>5.1</b> Probability Refresher</a></li>
<li class="chapter" data-level="5.2" data-path="bivariate.html"><a href="bivariate.html#the-linear-model"><i class="fa fa-check"></i><b>5.2</b> The Linear Model</a></li>
<li class="chapter" data-level="5.3" data-path="bivariate.html"><a href="bivariate.html#least-squares"><i class="fa fa-check"></i><b>5.3</b> Least Squares</a></li>
<li class="chapter" data-level="5.4" data-path="bivariate.html"><a href="bivariate.html#properties"><i class="fa fa-check"></i><b>5.4</b> Properties</a></li>
<li class="chapter" data-level="5.5" data-path="bivariate.html"><a href="bivariate.html#appendix-regression-in-r"><i class="fa fa-check"></i><b>5.5</b> Appendix: Regression in R</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="matrix.html"><a href="matrix.html"><i class="fa fa-check"></i><b>6</b> Matrix Algebra: A Crash Course</a><ul>
<li class="chapter" data-level="6.1" data-path="matrix.html"><a href="matrix.html#vector-operations"><i class="fa fa-check"></i><b>6.1</b> Vector Operations</a></li>
<li class="chapter" data-level="6.2" data-path="matrix.html"><a href="matrix.html#matrix-operations"><i class="fa fa-check"></i><b>6.2</b> Matrix Operations</a></li>
<li class="chapter" data-level="6.3" data-path="matrix.html"><a href="matrix.html#matrix-inversion"><i class="fa fa-check"></i><b>6.3</b> Matrix Inversion</a></li>
<li class="chapter" data-level="6.4" data-path="matrix.html"><a href="matrix.html#solving-linear-systems"><i class="fa fa-check"></i><b>6.4</b> Solving Linear Systems</a></li>
<li class="chapter" data-level="6.5" data-path="matrix.html"><a href="matrix.html#appendix-matrices-in-r"><i class="fa fa-check"></i><b>6.5</b> Appendix: Matrices in R</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ols-matrix.html"><a href="ols-matrix.html"><i class="fa fa-check"></i><b>7</b> Reintroduction to the Linear Model</a><ul>
<li class="chapter" data-level="7.1" data-path="ols-matrix.html"><a href="ols-matrix.html#the-linear-model-in-matrix-form"><i class="fa fa-check"></i><b>7.1</b> The Linear Model in Matrix Form</a></li>
<li class="chapter" data-level="7.2" data-path="ols-matrix.html"><a href="ols-matrix.html#the-ols-estimator"><i class="fa fa-check"></i><b>7.2</b> The OLS Estimator</a></li>
<li class="chapter" data-level="7.3" data-path="ols-matrix.html"><a href="ols-matrix.html#vector-valued-random-variables"><i class="fa fa-check"></i><b>7.3</b> Vector-Valued Random Variables</a></li>
<li class="chapter" data-level="7.4" data-path="ols-matrix.html"><a href="ols-matrix.html#properties-of-ols"><i class="fa fa-check"></i><b>7.4</b> Properties of OLS</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="specification.html"><a href="specification.html"><i class="fa fa-check"></i><b>8</b> Specification Issues</a><ul>
<li class="chapter" data-level="8.1" data-path="specification.html"><a href="specification.html#categorical-variables"><i class="fa fa-check"></i><b>8.1</b> Categorical Variables</a></li>
<li class="chapter" data-level="8.2" data-path="specification.html"><a href="specification.html#interaction-terms"><i class="fa fa-check"></i><b>8.2</b> Interaction Terms</a></li>
<li class="chapter" data-level="8.3" data-path="specification.html"><a href="specification.html#higher-order-terms"><i class="fa fa-check"></i><b>8.3</b> Higher-Order Terms</a></li>
<li class="chapter" data-level="8.4" data-path="specification.html"><a href="specification.html#appendix-categorical-and-higher-order-specifications-in-r"><i class="fa fa-check"></i><b>8.4</b> Appendix: Categorical and Higher-Order Specifications in R</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Practical Data Analysis for Political Scientists</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="specification" class="section level1">
<h1><span class="header-section-number">8</span> Specification Issues</h1>
<p>I lied to you about the linear model last week. Like the grade-school teachers who told you everyone thought the world was flat before Columbus proved them wrong, I had good intentions—but it was a lie nonetheless.</p>
<p>I claimed that the linear model assumed that the conditional expectation of the response was a linear function of the covariates. That is false. A data model is a linear model, can be estimated consistently and without bias by OLS, and all that good stuff, as long as it is linear in the <em>parameters</em>.</p>
<p>For example, the following is a linear model. <span class="math display">\[
Y_n = \beta_1 + \beta_2 x_n + \beta_3 x_n^2 + \beta_4 x_n^7 + \epsilon_n.
\]</span> The conditional expectation of <span class="math inline">\(Y_n\)</span> is a nonlinear function of <span class="math inline">\(x_n\)</span> (holding <span class="math inline">\(\beta\)</span> fixed) but a linear function of <span class="math inline">\(\beta\)</span> (holding <span class="math inline">\(x_n\)</span> fixed). Therefore, assuming strict exogeneity holds, OLS is an unbiased, consistent, asymptotically normal estimator of <span class="math inline">\(\beta\)</span>.</p>
<p>The following is not a linear model. <span class="math display">\[
Y_n = 2^{\beta_1} + 2^{\beta_2} x_n + \epsilon_n.
\]</span> Holding <span class="math inline">\(\beta\)</span> fixed, this is a linear function of the covariate <span class="math inline">\(x_n\)</span>. But, holding <span class="math inline">\(x_n\)</span> fixed, this is not a linear function of <span class="math inline">\(\beta\)</span>. OLS is not an appropriate estimator for the parameters of this model.</p>
<p>This week, we will talk about linear models with non-standard covariate specifications—those that aren’t just a linear function of continuous variables.</p>
<div id="categorical-variables" class="section level2">
<h2><span class="header-section-number">8.1</span> Categorical Variables</h2>
<p>Using the linear model, we write the conditional expectation for the <span class="math inline">\(n\)</span>’th response as <span class="math display">\[
E[Y_n \,|\, \mathbf{x}_n] = \mathbf{x}_n \cdot \beta + \epsilon_n,
\]</span> where <span class="math inline">\(\mathbf{x}_n\)</span> is the vector of <span class="math inline">\(K\)</span> covariates (including the intercept) and <span class="math inline">\(\beta\)</span> is the vector of <span class="math inline">\(K\)</span> coefficients we wish to estimate.</p>
<p>This makes sense with numerical variables, but not so much with categorical variables. For example, think of the relationship between party identification and one’s vote in the 2016 presidential election. Suppose our response variable is one’s vote (1 for Trump, 0 for non-Trump), and our party ID variable records whether the respondent is a Republican, Democrat, or independent. The resulting linear model equation, <span class="math display">\[
\text{Trump}_n = \beta_1 + \beta_2 \text{Party ID}_n + \epsilon_n,
\]</span> doesn’t really make sense, because party ID isn’t a number.<a href="#fn21" class="footnoteRef" id="fnref21"><sup>21</sup></a></p>
<p>To incorporate a categorical variable into the linear model, we break each category into its own binary variable. For example, with our party ID variable, we go from <span class="math display">\[
\text{Party ID} = \begin{pmatrix}
\text{R} \\
\text{R} \\
\text{I} \\
\text{I} \\
\text{D} \\
\text{D}
\end{pmatrix}
\]</span> to <span class="math display">\[
\text{Republican} = \begin{pmatrix} 1 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \end{pmatrix},
\text{Independent} = \begin{pmatrix} 0 \\ 0 \\ 1 \\ 1 \\ 0 \\ 0 \end{pmatrix},
\text{Democratic} = \begin{pmatrix} 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 1 \end{pmatrix}.
\]</span> These are called <em>dummy variables</em> or, preferably, <em>indicator variables</em>.</p>
<p>Having turned our categorical variable into a set of indicators, you may be tempted to rewrite the model as <span class="math display">\[
\text{Trump}_n = \beta_1 + \beta_2 \text{Republican}_n + \beta_3 \text{Independent}_n + \beta_4 \text{Democratic}_n + \epsilon_n.
\]</span> But take a look at the matrix of covariates, or <em>design matrix</em>, that would result if we set up the model this way: <span class="math display">\[
\mathbf{X} = \begin{bmatrix}
  1 &amp; 1 &amp; 0 &amp; 0 \\
  1 &amp; 1 &amp; 0 &amp; 0 \\
  1 &amp; 0 &amp; 1 &amp; 0 \\
  1 &amp; 0 &amp; 1 &amp; 0 \\
  1 &amp; 0 &amp; 0 &amp; 1 \\
  1 &amp; 0 &amp; 0 &amp; 1
\end{bmatrix}
\]</span> The columns of the design matrix are linearly dependent: the constant term is equal to the sum of three party ID indicators. (A useful exercise is to calculate <span class="math inline">\(\mathbf{X}^\top \mathbf{X}\)</span> and confirm that its columns are linearly dependent too.) This means we can’t include all three when estimating <span class="math inline">\(\beta\)</span> via OLS—we have to drop one category.</p>
<p>In one sense, which category we drop is immaterial—our regression will make the same predictions either way. However, in order to interpret the results of a regression on categorical variables, it is important that we know what the categories are, and which one has been dropped.</p>
<p>For example, imagine we drop the Republican category, so we have the following linear model: <span class="math display">\[
\text{Trump}_n = \beta_1 + \beta_2 \text{Independent}_n + \beta_3 \text{Democratic}_n + \epsilon_n.
\]</span> For a Republican voter, the Independent and Democratic variables will both equal zero, so we will have <span class="math display">\[
E[\text{Trump}_n \,|\, \text{Party ID}_n = \text{R}] = \beta_1.
\]</span> In other words, the intercept will be the predicted probability that a Republican votes for Trump. For an Independent voter, we will have <span class="math display">\[
E[\text{Trump}_n \,|\, \text{Party ID}_n = \text{I}] = \beta_1 + \beta_2.
\]</span> So the coefficient on the Independent indicator is not the predicted probability that an Independent votes for Trump. Instead, it is the <em>difference</em> in probability of a Trump vote between Independents and the baseline category (in this case, Republicans).</p>
<ul>
<li><p>If Independents are less likely than Republicans to vote for Trump, the coefficient on Independent will be negative.</p></li>
<li><p>If Independents are more likely than Republicans to vote for Trump, the coefficient on Independent will be positive.</p></li>
<li><p>If Independents are equally likely as Republicans to vote for Trump, the coefficient on Independent will be zero.</p></li>
</ul>
<p>Similarly, for a Democratic voter, we have <span class="math display">\[
E[\text{Trump}_n \,|\, \text{Party ID}_n = \text{D}] = \beta_1 + \beta_3.
\]</span> The interpretation of the coefficient on Democratic is the same as for the coefficient on Independent.</p>
<p>Take a look at the following results of a hypothetical regression.</p>
<table>
<thead>
<tr class="header">
<th>Coefficient</th>
<th>Estimate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>(Intercept)</td>
<td>0.9</td>
</tr>
<tr class="even">
<td>Independent</td>
<td>-0.4</td>
</tr>
<tr class="odd">
<td>Democratic</td>
<td>-0.75</td>
</tr>
</tbody>
</table>
<ul>
<li><p>Republicans have a 90% chance of voting for Trump. We see that by looking at the intercept, since Republicans are the omitted category.</p></li>
<li><p>We see from the coefficient on Independent that an Independent’s chance of voting for Trump is 40% lower than a Republican’s. This means that an Independent has a 50% chance of voting for Trump.</p></li>
<li><p>Similarly, we see from the coefficient on Democratic that a Democrat’s chance of voting for Trump is 75% lower than a Republican’s, for a 15% chance overall.</p></li>
</ul>
<p>Had we instead omitted Independent, we would get different coefficients, but the same predictions.</p>
<table>
<thead>
<tr class="header">
<th>Coefficient</th>
<th>Estimate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>(Intercept)</td>
<td>0.5</td>
</tr>
<tr class="even">
<td>Republican</td>
<td>0.4</td>
</tr>
<tr class="odd">
<td>Democratic</td>
<td>-0.35</td>
</tr>
</tbody>
</table>
<p>Same story, different numbers, had we omitted Democratic.</p>
<table>
<thead>
<tr class="header">
<th>Coefficient</th>
<th>Estimate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>(Intercept)</td>
<td>0.15</td>
</tr>
<tr class="even">
<td>Republican</td>
<td>0.75</td>
</tr>
<tr class="odd">
<td>Independent</td>
<td>0.35</td>
</tr>
</tbody>
</table>
<p>Given that the results are substantively the same no matter what, does it matter which category we choose to drop? Yes, for the purpose of communicating your results. The omitted category should serve as a meaningful baseline. For this example, all of our three categories are substantively meaningful, so any choice will do. But imagine replacing our part ID variable with a race variable that has the following categories:</p>
<ul>
<li>White</li>
<li>Black</li>
<li>Hispanic</li>
<li>Asian</li>
<li>Other</li>
</ul>
<p>You may be tempted to make “Other” the excluded category, so that you obtain a coefficient for each of the specific racial groups. But that’s actually the worst choice possible. The coefficient on the White variable would then represent the difference in probability of voting for Trump between a white voter and a voter in the “other” category—which is hard to interpret. Whereas if we instead omitted the Black category, the coefficient on the White variable would represent the difference between white and black voters.</p>
<p>When in doubt, I recommend omitting whichever category is largest in the data.</p>
<p>Now let’s introduce covariates into the mix. Consider a regression of Trump vote on party ID (Republican as omitted category) and age, producing the following results.</p>
<table>
<thead>
<tr class="header">
<th>Coefficient</th>
<th>Estimate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>(Intercept)</td>
<td>0.8</td>
</tr>
<tr class="even">
<td>Independent</td>
<td>-0.4</td>
</tr>
<tr class="odd">
<td>Democratic</td>
<td>-0.75</td>
</tr>
<tr class="even">
<td>Age</td>
<td>0.002</td>
</tr>
</tbody>
</table>
<p>Remember what the coefficient of 0.002 on Age means: if we compared one voter to another who was otherwise identical (in this case, same Party ID) except five years older, we would expect the latter voter to have a 1% greater chance of voting for Trump.</p>
<p>More specifically, we have three different regression lines—one for each group: <span class="math display">\[
\begin{aligned}
  E[\text{Trump}_n \,|\, \text{Party ID}_n = \text{R}, \text{Age}_n] &amp;= 0.8 + 0.002 \text{Age}_n, \\
  E[\text{Trump}_n \,|\, \text{Party ID}_n = \text{I}, \text{Age}_n] &amp;= 0.4 + 0.002 \text{Age}_n, \\
  E[\text{Trump}_n \,|\, \text{Party ID}_n = \text{D}, \text{Age}_n] &amp;= 0.05 + 0.002 \text{Age}_n.
\end{aligned}
\]</span> Notice that the slope is the same in each regression line. Only the intercept varies across groups. When we include a categorical variable in a regression model, it’s like allowing the intercept to differ across categories.</p>
</div>
<div id="interaction-terms" class="section level2">
<h2><span class="header-section-number">8.2</span> Interaction Terms</h2>
<p>When political scientists or economists describe their regression results, they will often talk about the marginal effects of different variables. Formally, the <em>marginal effect</em> of the <span class="math inline">\(k\)</span>’th covariate, <span class="math inline">\(x_{nk}\)</span>, is <span class="math display">\[
{\frac{\partial E[Y_n \,|\, \mathbf{x}_n]}{\partial x_{nk}}},
\]</span> the partial derivative of the conditional expectation with respect to the <span class="math inline">\(k\)</span>’th covariate.</p>
<p>The marginal effect answers the following question: Suppose we have two observations that differ in the <span class="math inline">\(k\)</span>’th covariate by one unit, but are otherwise identical. How much greater, or less, would we expect the response to be for the observation with the one-unit-greater value of <span class="math inline">\(x_{nk}\)</span>?</p>
<p>If we were sure the relationship we were modeling were causal, we could phrase the above question more succinctly. We could ask: Given a one-unit change in the <span class="math inline">\(k\)</span>’th covariate, holding all else fixed, what change in the response should we expect? But we haven’t yet gotten to the point where we can make our claims causal. Hence I will often refer to <em>so-called marginal effects</em>, since I don’t want the “effect” terminology to deceive us into thinking we’re drawing causal inferences. So-called marginal effects are just a nice way to summarize the relationship between individual covariates and the conditional expectation.</p>
<p>The bare-bones linear model has the (sometimes appealing, sometimes not) feature that it assumes constant marginal effects. For each covariate <span class="math inline">\(x_{nk}\)</span>, we have <span class="math display">\[
{\frac{\partial E[Y_n \,|\, \mathbf{x}_n]}{\partial x_{nk}}} = \beta_k,
\]</span> the coefficient on that covariate. This encodes two critical assumptions:</p>
<ol style="list-style-type: decimal">
<li><p>The marginal effect of the <span class="math inline">\(k\)</span>’th covariate does not depend on the value of any other covariates.</p></li>
<li><p>The marginal effect of the <span class="math inline">\(k\)</span>’th covariate does not depend on its own value.</p></li>
</ol>
<p>It is easy to think of scenarios where each of these might be questionable.</p>
<ol style="list-style-type: decimal">
<li><p>Imagine a study of individual voters’ choices in U.S. House races, where we model voting for the incumbent as a function of how often the voter goes to church. The marginal effect of religiosity is probably different if the incumbent is a Republican than if the incumbent is a Democrat.</p></li>
<li><p>Imagine a study of individual voters’ turnout decisions, where we model turnout as a function of the voter’s ideology. Suppose ideology is measured on a 7-point scale, where 1 is most liberal and 7 is most conservative. We know the most ideologically extreme voters are the most likely to turn out. So, all else equal, we’d expect moving from 1 to 2 (very liberal to pretty liberal) to decrease one’s probability of voting, but we’d expect moving from 6 to 7 (pretty conservative to very conservative) to increase one’s probability of voting.</p></li>
</ol>
<p>Let’s start with the first case, where the (so-called) marginal effect of one variable depends on the value of another variable. To allow for this in our models, we include the product of the two covariates in our model.</p>
<p>For example, suppose we are interested in whether the relationship between education and voting for Trump is different between whites and non-whites. We would include three terms in the model (plus an intercept): education, an indicator for white, and their product. <span class="math display">\[
\text{Trump}_n = \beta_1 + \beta_2 \text{Education}_n + \beta_3 \text{White}_n + \beta_4 (\text{Education}_n \times \text{White}_n) + \epsilon_n.
\]</span> The so-called marginal effect of education is now <span class="math display">\[
{\frac{\partial E[\text{Trump}_n \,|\, \mathbf{x}_n]}{\partial \text{Education}_n}}
= \beta_2 + \beta_4 \text{White}_n.
\]</span> This equation tells us three things.</p>
<ul>
<li><p><span class="math inline">\(\beta_2\)</span> is the marginal effect of education for non-white voters.</p></li>
<li><p><span class="math inline">\(\beta_4\)</span> is the difference between the marginal effect of education for white voters and the effect for non-white voters.</p></li>
<li><p><span class="math inline">\(\beta_2 + \beta_4\)</span> is the marginal effect of education for white voters.</p></li>
</ul>
<p>Another way to think of it is that we have two regression lines: <span class="math display">\[
\begin{aligned}
  E[\text{Trump}_n \,|\, \text{White}_n = 0, \text{Education}_n] &amp;= \beta_1 + \beta_2 \text{Education}_n, \\
  E[\text{Trump}_n \,|\, \text{White}_n = 1, \text{Education}_n] &amp;= (\beta_1 + \beta_3) + (\beta_2 + \beta_4) \text{Education}_n.
\end{aligned}
\]</span> We saw before that including a categorical variable is like allowing a different intercept for each category. Including an interaction with a categorical variable is like allowing a different slope for each category.</p>
<p>At this point, you might ask, why not just run two separate regressions? I can think of at least two reasons not to.</p>
<ul>
<li><p>You might want to include other covariates whose effects you don’t think are dependent on race (e.g., age). If you ran separate regressions, you would estimate race-dependent effects for every covariate, at a potential loss of efficiency.</p></li>
<li><p>You might want to formally test the hypothesis that the effect of education is equal for whites and non-whites. This is easiest to do if you have a single model. Next week we will talk about the tools you would need to undertake this sort of test.</p></li>
</ul>
<p>One frequent source of confusion with interaction terms is whether you need to include lower-order terms in the model. For example, if we are only interested in how the effect of education differs with race, why can’t we just include education and its product with race in the specification? The equations above give you the answer. Leaving the white indicator out of the model is like fixing <span class="math inline">\(\beta_3 = 0\)</span>. This means you’re forcing the regression lines for whites and non-whites to have the same intercept, which there’s no good reason to do.</p>
<p>If you’re not yet persuaded on the necessity of including constitutive terms of interactions in your regressions, see <span class="citation">Braumoeller (<a href="#ref-braumoeller2004hypothesis">2004</a>)</span>.</p>
<p>For an example of interaction terms, imagine the following example. Suppose education is measured in years of schooling.</p>
<table>
<thead>
<tr class="header">
<th>Coefficient</th>
<th>Estimate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>(Intercept)</td>
<td>0.3</td>
</tr>
<tr class="even">
<td>Education</td>
<td>0.01</td>
</tr>
<tr class="odd">
<td>White</td>
<td>0.4</td>
</tr>
<tr class="even">
<td>Education * White</td>
<td>-0.03</td>
</tr>
</tbody>
</table>
<p>We would interpret these in the following way.</p>
<ul>
<li><p>A hypothetical non-white voter with zero years of education has a 30% chance of voting for Trump. For each additional year of education, the probability of voting for Trump goes up by 1%.</p></li>
<li><p>A hypothetical white voter with zero years of education has a 70% (0.3 + 0.4) chance of voting for Trump. For each additional year of education, the probability of voting for Trump goes down by 2% (0.01 - 0.03).</p></li>
</ul>
<p>What about an interaction between two continuous variables? For example, imagine an interaction between age and education in our model of voting for Trump.</p>
<table>
<thead>
<tr class="header">
<th>Coefficient</th>
<th>Estimate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>(Intercept)</td>
<td>0.4</td>
</tr>
<tr class="even">
<td>Education</td>
<td>-0.02</td>
</tr>
<tr class="odd">
<td>Age</td>
<td>0.002</td>
</tr>
<tr class="even">
<td>Education * Age</td>
<td>0.0002</td>
</tr>
</tbody>
</table>
<p>One simple way to interpret the effect of each variable is to hold the other one fixed at various values. For example, for a 20-year-old, we have <span class="math display">\[
\begin{aligned}
E[\text{Trump}_n \,|\, \text{Age}_n = 20, \text{Education}_n] &amp;= 0.44 - 0.16 \text{Education}_n,
\end{aligned}
\]</span> whereas for an 80-year-old, we have <span class="math display">\[
\begin{aligned}
E[\text{Trump}_n \,|\, \text{Age}_n = 80, \text{Education}_n] &amp;= 0.56 - 0.04 \text{Education}_n.
\end{aligned}
\]</span> These results would seem to imply that (1) older people have a higher baseline probability of voting for Trump, and (2) the magnitude of the negative relationship between education and voting for Trump is weaker for older voters.</p>
<p>Always remember: when in doubt, take the partial derivative of <span class="math inline">\(Y_n\)</span> (or, more precisely, its conditional expectation) with respect to the variable you’re interested in. For example, here we have <span class="math display">\[
{\frac{\partial E[\text{Trump}_n \,|\, \mathbf{x}_n]}{\partial \text{Education}_n}} = -0.02 + 0.0002 \text{Age}_n.
\]</span></p>
</div>
<div id="higher-order-terms" class="section level2">
<h2><span class="header-section-number">8.3</span> Higher-Order Terms</h2>
<p>Coming soon!</p>
</div>
<div id="appendix-categorical-and-higher-order-specifications-in-r" class="section level2">
<h2><span class="header-section-number">8.4</span> Appendix: Categorical and Higher-Order Specifications in R</h2>
<p>Coming soon!</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-braumoeller2004hypothesis">
<p>Braumoeller, Bear F. 2004. “Hypothesis Testing and Multiplicative Interaction Terms.” <em>International Organization</em> 58 (04). Cambridge University Press: 807–20.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="21">
<li id="fn21"><p>Oldish-school political scientists, by which I mean just about anyone who got their PhD between 1990 and 2010, would now be clutching their pearls at the mere thought of using a linear model for vote choice. But so-called “binary dependent variable” models like logistic regression are at best overrated, as you read in <em>Mostly Harmless Econometrics</em>.<a href="specification.html#fnref21">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ols-matrix.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/07-specification.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
