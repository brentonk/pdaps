<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Practical Data Analysis for Political Scientists</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="These are the course notes for Brenton Kenkel’s course PSCI 8357: Statistics for Political Research II. They may or may not also be useful in their own right.">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Practical Data Analysis for Political Scientists" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="These are the course notes for Brenton Kenkel’s course PSCI 8357: Statistics for Political Research II. They may or may not also be useful in their own right." />
  <meta name="github-repo" content="brentonk/pdaps" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Practical Data Analysis for Political Scientists" />
  
  <meta name="twitter:description" content="These are the course notes for Brenton Kenkel’s course PSCI 8357: Statistics for Political Research II. They may or may not also be useful in their own right." />
  

<meta name="author" content="Brenton Kenkel">


<meta name="date" content="2017-03-12">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="matrix.html">
<link rel="next" href="specification.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Practical Data Analysis for Political Scientists</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About This Book</a></li>
<li class="chapter" data-level="2" data-path="programming.html"><a href="programming.html"><i class="fa fa-check"></i><b>2</b> Principles of Programming</a><ul>
<li class="chapter" data-level="2.1" data-path="programming.html"><a href="programming.html#write-programs-for-people-not-computers"><i class="fa fa-check"></i><b>2.1</b> Write Programs for People, Not Computers</a></li>
<li class="chapter" data-level="2.2" data-path="programming.html"><a href="programming.html#let-the-computer-do-the-work"><i class="fa fa-check"></i><b>2.2</b> Let the Computer Do the Work</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>3</b> Working with Data</a><ul>
<li class="chapter" data-level="3.1" data-path="data.html"><a href="data.html#loading"><i class="fa fa-check"></i><b>3.1</b> Loading</a></li>
<li class="chapter" data-level="3.2" data-path="data.html"><a href="data.html#tidying"><i class="fa fa-check"></i><b>3.2</b> Tidying</a></li>
<li class="chapter" data-level="3.3" data-path="data.html"><a href="data.html#transforming-and-aggregating"><i class="fa fa-check"></i><b>3.3</b> Transforming and Aggregating</a></li>
<li class="chapter" data-level="3.4" data-path="data.html"><a href="data.html#merging"><i class="fa fa-check"></i><b>3.4</b> Merging</a></li>
<li class="chapter" data-level="3.5" data-path="data.html"><a href="data.html#appendix-creating-the-example-data"><i class="fa fa-check"></i><b>3.5</b> Appendix: Creating the Example Data</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="visualization.html"><a href="visualization.html"><i class="fa fa-check"></i><b>4</b> Data Visualization</a><ul>
<li class="chapter" data-level="4.1" data-path="visualization.html"><a href="visualization.html#basic-plots"><i class="fa fa-check"></i><b>4.1</b> Basic Plots</a></li>
<li class="chapter" data-level="4.2" data-path="visualization.html"><a href="visualization.html#saving-plots"><i class="fa fa-check"></i><b>4.2</b> Saving Plots</a></li>
<li class="chapter" data-level="4.3" data-path="visualization.html"><a href="visualization.html#faceting"><i class="fa fa-check"></i><b>4.3</b> Faceting</a></li>
<li class="chapter" data-level="4.4" data-path="visualization.html"><a href="visualization.html#aesthetics"><i class="fa fa-check"></i><b>4.4</b> Aesthetics</a></li>
<li class="chapter" data-level="4.5" data-path="visualization.html"><a href="visualization.html#appendix-creating-the-example-data-1"><i class="fa fa-check"></i><b>4.5</b> Appendix: Creating the Example Data</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="bivariate.html"><a href="bivariate.html"><i class="fa fa-check"></i><b>5</b> Bivariate Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="bivariate.html"><a href="bivariate.html#probability-refresher"><i class="fa fa-check"></i><b>5.1</b> Probability Refresher</a></li>
<li class="chapter" data-level="5.2" data-path="bivariate.html"><a href="bivariate.html#the-linear-model"><i class="fa fa-check"></i><b>5.2</b> The Linear Model</a></li>
<li class="chapter" data-level="5.3" data-path="bivariate.html"><a href="bivariate.html#least-squares"><i class="fa fa-check"></i><b>5.3</b> Least Squares</a></li>
<li class="chapter" data-level="5.4" data-path="bivariate.html"><a href="bivariate.html#properties"><i class="fa fa-check"></i><b>5.4</b> Properties</a></li>
<li class="chapter" data-level="5.5" data-path="bivariate.html"><a href="bivariate.html#appendix-regression-in-r"><i class="fa fa-check"></i><b>5.5</b> Appendix: Regression in R</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="matrix.html"><a href="matrix.html"><i class="fa fa-check"></i><b>6</b> Matrix Algebra: A Crash Course</a><ul>
<li class="chapter" data-level="6.1" data-path="matrix.html"><a href="matrix.html#vector-operations"><i class="fa fa-check"></i><b>6.1</b> Vector Operations</a></li>
<li class="chapter" data-level="6.2" data-path="matrix.html"><a href="matrix.html#matrix-operations"><i class="fa fa-check"></i><b>6.2</b> Matrix Operations</a></li>
<li class="chapter" data-level="6.3" data-path="matrix.html"><a href="matrix.html#matrix-inversion"><i class="fa fa-check"></i><b>6.3</b> Matrix Inversion</a></li>
<li class="chapter" data-level="6.4" data-path="matrix.html"><a href="matrix.html#solving-linear-systems"><i class="fa fa-check"></i><b>6.4</b> Solving Linear Systems</a></li>
<li class="chapter" data-level="6.5" data-path="matrix.html"><a href="matrix.html#appendix-matrices-in-r"><i class="fa fa-check"></i><b>6.5</b> Appendix: Matrices in R</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ols-matrix.html"><a href="ols-matrix.html"><i class="fa fa-check"></i><b>7</b> Reintroduction to the Linear Model</a><ul>
<li class="chapter" data-level="7.1" data-path="ols-matrix.html"><a href="ols-matrix.html#the-linear-model-in-matrix-form"><i class="fa fa-check"></i><b>7.1</b> The Linear Model in Matrix Form</a></li>
<li class="chapter" data-level="7.2" data-path="ols-matrix.html"><a href="ols-matrix.html#the-ols-estimator"><i class="fa fa-check"></i><b>7.2</b> The OLS Estimator</a></li>
<li class="chapter" data-level="7.3" data-path="ols-matrix.html"><a href="ols-matrix.html#vector-valued-random-variables"><i class="fa fa-check"></i><b>7.3</b> Vector-Valued Random Variables</a></li>
<li class="chapter" data-level="7.4" data-path="ols-matrix.html"><a href="ols-matrix.html#properties-of-ols"><i class="fa fa-check"></i><b>7.4</b> Properties of OLS</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="specification.html"><a href="specification.html"><i class="fa fa-check"></i><b>8</b> Specification Issues</a><ul>
<li class="chapter" data-level="8.1" data-path="specification.html"><a href="specification.html#categorical-variables"><i class="fa fa-check"></i><b>8.1</b> Categorical Variables</a></li>
<li class="chapter" data-level="8.2" data-path="specification.html"><a href="specification.html#interaction-terms"><i class="fa fa-check"></i><b>8.2</b> Interaction Terms</a></li>
<li class="chapter" data-level="8.3" data-path="specification.html"><a href="specification.html#quadratic-and-logarithmic-terms"><i class="fa fa-check"></i><b>8.3</b> Quadratic and Logarithmic Terms</a></li>
<li class="chapter" data-level="8.4" data-path="specification.html"><a href="specification.html#appendix-nonstandard-specifications-in-r"><i class="fa fa-check"></i><b>8.4</b> Appendix: Nonstandard Specifications in R</a><ul>
<li class="chapter" data-level="8.4.1" data-path="specification.html"><a href="specification.html#categorical-variables-1"><i class="fa fa-check"></i><b>8.4.1</b> Categorical Variables</a></li>
<li class="chapter" data-level="8.4.2" data-path="specification.html"><a href="specification.html#interaction-terms-1"><i class="fa fa-check"></i><b>8.4.2</b> Interaction Terms</a></li>
<li class="chapter" data-level="8.4.3" data-path="specification.html"><a href="specification.html#quadratic-and-logarithmic-models"><i class="fa fa-check"></i><b>8.4.3</b> Quadratic and Logarithmic Models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>9</b> Drawing Inferences</a><ul>
<li class="chapter" data-level="9.1" data-path="inference.html"><a href="inference.html#the-basics-of-hypothesis-testing"><i class="fa fa-check"></i><b>9.1</b> The Basics of Hypothesis Testing</a></li>
<li class="chapter" data-level="9.2" data-path="inference.html"><a href="inference.html#variance-of-ols"><i class="fa fa-check"></i><b>9.2</b> Variance of OLS</a></li>
<li class="chapter" data-level="9.3" data-path="inference.html"><a href="inference.html#single-variable-hypotheses"><i class="fa fa-check"></i><b>9.3</b> Single Variable Hypotheses</a></li>
<li class="chapter" data-level="9.4" data-path="inference.html"><a href="inference.html#multiple-variable-hypotheses"><i class="fa fa-check"></i><b>9.4</b> Multiple Variable Hypotheses</a></li>
<li class="chapter" data-level="9.5" data-path="inference.html"><a href="inference.html#appendix-full-derivation-of-ols-variance"><i class="fa fa-check"></i><b>9.5</b> Appendix: Full Derivation of OLS Variance</a></li>
<li class="chapter" data-level="9.6" data-path="inference.html"><a href="inference.html#appendix-regression-inference-in-r"><i class="fa fa-check"></i><b>9.6</b> Appendix: Regression Inference in R</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Practical Data Analysis for Political Scientists</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ols-matrix" class="section level1">
<h1><span class="header-section-number">7</span> Reintroduction to the Linear Model</h1>
<p>Having learned some matrix algebra, let us now return to the world of statistics. We are going to take what we learned about regression and ordinary least squares in the bivariate case, then generalize it to a setting with potentially many variables. To make that task feasible, we will rely on the tools of matrix algebra that we learned last week.</p>
<div id="the-linear-model-in-matrix-form" class="section level2">
<h2><span class="header-section-number">7.1</span> The Linear Model in Matrix Form</h2>
<p>We have a sequence of observations indexed by <span class="math inline">\(n \in \{1, \ldots, N\}\)</span>. Each observation consists of a response, <span class="math inline">\(Y_n\)</span>, a real number; and a vector of <span class="math inline">\(K\)</span> covariates, <span class="math display">\[
\mathbf{x}_n = \begin{pmatrix}
  x_{n1} \\
  x_{n2} \\
  \vdots \\
  x_{nK}
\end{pmatrix}.
\]</span> Just like in bivariate regression, our goal is to estimate the conditional expectation of the response given the covariates, <span class="math inline">\(E[Y_n \,|\, \mathbf{x}_n]\)</span>. To make that task feasible, we will assume the relationship is linear, <span class="math display">\[
E[Y_n \,|\, \mathbf{x}_n] = \beta \cdot \mathbf{x}_n,
\]</span> where <span class="math inline">\(\beta\)</span> is the <span class="math inline">\(K \times 1\)</span> vector of coefficients, <span class="math display">\[
\beta = \begin{pmatrix}
  \beta_1 \\
  \beta_2 \\
  \vdots \\
  \beta_K
\end{pmatrix}.
\]</span> Our data model is <span class="math display">\[
Y_n = \beta \cdot \mathbf{x}_n + \epsilon_n,
\]</span> where <span class="math inline">\(\epsilon_n\)</span> is “white noise” error that is uncorrelated with the covariates. (More on this in a second.)</p>
<p>This data model looks a little bit different than our bivariate linear model, which you’ll recall was <span class="math display">\[
Y_n = \alpha + \beta x_n + \epsilon_n.
\]</span> What happened to <span class="math inline">\(\alpha\)</span>, the intercept? When working with the multivariate linear model, it will make our lives easiest to treat the intercept like any other coefficient. Specifically, we will assume <span class="math inline">\(x_{n1} = 1\)</span> for all <span class="math inline">\(n\)</span>, and we will treat <span class="math inline">\(\beta_1\)</span> as the intercept. With <span class="math inline">\(K = 2\)</span>, our multivariate model becomes <span class="math display">\[
\begin{aligned}
Y_n &amp;= \beta \cdot \mathbf{x}_n + \epsilon_n \\
&amp;= \begin{pmatrix} \beta_1 \\ \beta_2 \end{pmatrix} \cdot \begin{pmatrix} 1 \\ x_{n2} \end{pmatrix} + \epsilon_n \\
&amp;= \beta_1 + \beta_2 x_{n2} + \epsilon_n,
\end{aligned}
\]</span> which is the same as our bivariate regression model, replacing the intercept <span class="math inline">\(\alpha\)</span> with <span class="math inline">\(\beta_1\)</span>, the slope <span class="math inline">\(\beta\)</span> with <span class="math inline">\(\beta_2\)</span>, and the covariate <span class="math inline">\(x_n\)</span> with <span class="math inline">\(x_{n2}\)</span>.</p>
<p>If we were to stack up all of our data, we would have <span class="math inline">\(N\)</span> equations, <span class="math display">\[
\begin{aligned}
Y_1 &amp;= \beta \cdot \mathbf{x}_1 + \epsilon_1, \\
Y_2 &amp;= \beta \cdot \mathbf{x}_2 + \epsilon_2, \\
&amp;\vdots \\
Y_N &amp;= \beta \cdot \mathbf{x}_N + \epsilon_N.
\end{aligned}
\]</span> Like any system of linear equations, we can write this one more easily in matrix form. Let <span class="math inline">\(\mathbf{Y}\)</span> be the <span class="math inline">\(N \times 1\)</span> vector that collects the response, <span class="math display">\[
\mathbf{Y} = \begin{pmatrix} Y_1 \\ Y_2 \\ \vdots \\ Y_N \end{pmatrix}.
\]</span> Let <span class="math inline">\(\mathbf{X}\)</span> be the <span class="math inline">\(N \times K\)</span> matrix that collects the covariates, <span class="math display">\[
\mathbf{X} =
\begin{bmatrix}
  x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1K} \\
  x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2K} \\
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
  x_{N1} &amp; x_{N2} &amp; \cdots &amp; x_{NK}
\end{bmatrix}
=
\begin{bmatrix}
  1 &amp; x_{12} &amp; \cdots &amp; x_{1K} \\
  1 &amp; x_{22} &amp; \cdots &amp; x_{2K} \\
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
  1 &amp; x_{N2} &amp; \cdots &amp; x_{NK}
\end{bmatrix}.
\]</span> The <span class="math inline">\(n\)</span>’th row of <span class="math inline">\(\mathbf{X}\)</span>, which we will write <span class="math inline">\(\mathbf{x}_n\)</span> (lowercase), contains the covariates for the <span class="math inline">\(n\)</span>’th observation. The <span class="math inline">\(k\)</span>’th column of <span class="math inline">\(\mathbf{X}\)</span>, which we will write <span class="math inline">\(\mathbf{X}_k\)</span> (uppercase), contains the value of the <span class="math inline">\(k\)</span>’th covariate for every observation. Finally, we will collect the error terms in an <span class="math inline">\(N \times 1\)</span> vector, <span class="math display">\[
\epsilon = \begin{pmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_N \end{pmatrix}.
\]</span> We can now write a model of the full data, <span class="math display">\[
\mathbf{Y} = \mathbf{X} \beta + \epsilon.
\]</span></p>
<p>It is worth pausing to clarify what is known and unknown here.</p>
<ul>
<li><p>The covariate matrix <span class="math inline">\(\mathbf{X}\)</span> and the response vector <span class="math inline">\(\mathbf{Y}\)</span> are known. They are our data.</p></li>
<li><p>The regression parameters <span class="math inline">\(\beta\)</span> are unknown. They are what we are trying to learn from the data.</p></li>
<li><p>The error term <span class="math inline">\(\epsilon\)</span> is also unknown. We can think of each observation of <span class="math inline">\(Y_n\)</span> as being a combination of “signal”, <span class="math inline">\(\mathbf{x}_n \cdot \beta\)</span>, and “noise”, <span class="math inline">\(\epsilon_n\)</span>. The fundamental problem is that we don’t know exactly what the signal is and what the noise is.</p></li>
</ul>
</div>
<div id="the-ols-estimator" class="section level2">
<h2><span class="header-section-number">7.2</span> The OLS Estimator</h2>
<p>Consider the linear model with three covariates, <span class="math display">\[
Y_n = \beta_1 x_{n1} + \beta_2 x_{n2} + \beta_3 x_{n3} + \epsilon_n.
\]</span> Let’s do like we did with bivariate regression, and imagine estimating the parameters of the model by least squares. Let <span class="math inline">\((b_1, b_2, b_3)\)</span> denote an estimate of the parameters.<a href="#fn20" class="footnoteRef" id="fnref20"><sup>20</sup></a> We will set up the sum of squared errors as a function of the parameters, <span class="math display">\[
{\mathop{\rm SSE}\nolimits}(b_1, b_2, b_3)
= \sum_n (Y_n - b_1 x_{n1} - b_2 x_{n2} - b_3 x_{n3})^2.
\]</span></p>
<p>Just as we did to derive the bivariate OLS estimator, let’s begin by taking the partial derivative of the SSE with respect to the first regression coefficient, then equalizing it to zero. <span class="math display">\[
\frac{\partial {\mathop{\rm SSE}\nolimits}}{\partial b_1}
= -2 \sum_n x_{n1} (Y_n - b_1 x_{n1} - b_2 x_{n2} - b_3 x_{n3})
= 0.
\]</span> Dividing each side by <span class="math inline">\(-2\)</span> and rearranging terms gives us <span class="math display">\[
\sum_n x_{n1} (b_1 x_{n1} + b_2 x_{n2} + b_3 x_{n3}) = \sum_n x_{n1} Y_n.
\]</span> If we break up the left-hand sum into three individual sums, we get <span class="math display">\[
\left( \sum_n x_{n1}^2 \right) b_1 + \left( \sum_n x_{n1} x_{n2} \right) b_2 + \left( \sum_n x_{n1} x_{n3} \right) b_3 = \sum_n x_{n1} Y_n,
\]</span> which is a linear condition on <span class="math inline">\((b_1, b_2, b_3)\)</span>. If we go through the same steps with <span class="math inline">\(\partial {\mathop{\rm SSE}\nolimits}/ \partial b_2\)</span> and <span class="math inline">\(\partial {\mathop{\rm SSE}\nolimits}/ \partial b_3\)</span>, we obtain the linear system <span class="math display">\[
\begin{aligned}
\left( \sum_n x_{n1}^2 \right) b_1 + \left( \sum_n x_{n1} x_{n2} \right) b_2 + \left( \sum_n x_{n1} x_{n3} \right) b_3 &amp;= \sum_n x_{n1} Y_n, \\
\left( \sum_n x_{n2} x_{n1} \right) b_1 + \left( \sum_n x_{n2}^2 \right) b_2 + \left( \sum_n x_{n2} x_{n3} \right) b_3 &amp;= \sum_n x_{n2} Y_n, \\
\left( \sum_n x_{n3} x_{n1} \right) b_1 + \left( \sum_n x_{n3} x_{n2} \right) b_2 + \left( \sum_n x_{n3}^2 \right) b_3 &amp;= \sum_n x_{n3} Y_n.
\end{aligned}
\]</span> This is a linear system of three equations in three unknowns, namely <span class="math inline">\((b_1, b_2, b_3)\)</span>. We can write it as <span class="math inline">\(\mathbf{A} \mathbf{b} = \mathbf{c}\)</span>, where <span class="math inline">\(\mathbf{b}\)</span> is the <span class="math inline">\(3 \times 1\)</span> column vector we are trying to solve for. You’ll remember from last week that we use matrix algebra to solve linear systems like this one.</p>
<p>Let’s take a closer look at the coefficient matrix we have here, <span class="math display">\[
\mathbf{A} =
\begin{bmatrix}
\sum_n x_{n1}^2 &amp; \sum_n x_{n1} x_{n2} &amp; \sum_n x_{n1} x_{n3} \\
\sum_n x_{n2} x_{n1} &amp; \sum_n x_{n2}^2 &amp; \sum_n x_{n2} x_{n3} \\
\sum_n x_{n3} x_{n1} &amp; \sum_n x_{n3} x_{n2} &amp; \sum_n x_{n3}^2
\end{bmatrix}
\]</span> Notice that each <span class="math inline">\(ij\)</span>’th element is <span class="math display">\[
a_{ij} = \sum_n x_{ni} x_{nj} = \mathbf{X}_i \cdot \mathbf{X}_j,
\]</span> the dot product of the <span class="math inline">\(i\)</span>’th and <span class="math inline">\(j\)</span>’th columns of our <span class="math inline">\(\mathbf{X}\)</span> matrix. Of course, the <span class="math inline">\(i\)</span>’th column of <span class="math inline">\(\mathbf{X}\)</span> is the <span class="math inline">\(i\)</span>’th row of <span class="math inline">\(\mathbf{X}^\top\)</span>. If the <span class="math inline">\(ij\)</span>’th entry of <span class="math inline">\(\mathbf{A}\)</span> is the dot product of the <span class="math inline">\(i\)</span>’th row of <span class="math inline">\(\mathbf{X}^\top\)</span> and the <span class="math inline">\(j\)</span>’th column of <span class="math inline">\(\mathbf{X}\)</span>, that means <span class="math display">\[
\mathbf{A} = \mathbf{X}^\top \mathbf{X}.
\]</span></p>
<p>Similarly, let’s take a look at our right-hand side, <span class="math display">\[
\mathbf{c} = \begin{bmatrix}
\sum_n x_{n1} Y_n \\
\sum_n x_{n2} Y_n \\
\sum_n x_{n3} Y_n
\end{bmatrix}.
\]</span> Each <span class="math inline">\(i\)</span>’th entry of <span class="math inline">\(\mathbf{c}\)</span> is <span class="math display">\[
c_i = \sum_n x_{ni} Y_n = \mathbf{X}_i \cdot \mathbf{Y}.
\]</span> the dot product of the <span class="math inline">\(i\)</span>’th column of <span class="math inline">\(\mathbf{X}\)</span> (i.e., the <span class="math inline">\(i\)</span>’th column of <span class="math inline">\(\mathbf{X}^\top\)</span>) and the vector <span class="math inline">\(\mathbf{Y}\)</span>. Therefore, we have <span class="math display">\[
\mathbf{c} = \mathbf{X}^\top \mathbf{Y}.
\]</span></p>
<p>Our linear system of equations, <span class="math inline">\(\mathbf{A} \mathbf{b} = \mathbf{c}\)</span>, is equivalent to <span class="math display">\[
(\mathbf{X}^\top \mathbf{X}) \mathbf{b} = \mathbf{X}^\top \mathbf{Y}.
\]</span> Consequently, the solution to the system is <span class="math display">\[
\mathbf{b} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{Y}.
\]</span></p>
<p>Although we got here via the <span class="math inline">\(3 \times 3\)</span> case, this formula works for any number of covariates. The <em>OLS estimator</em> of the linear model coefficients from covariate matrix <span class="math inline">\(\mathbf{X}\)</span> and response vector <span class="math inline">\(\mathbf{Y}\)</span> is <span class="math display">\[
\hat{\beta}_{{\text{OLS}}}(\mathbf{X}, \mathbf{Y})
= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{Y}.
\]</span></p>
<p>When you see this formula, your hackles should be raised. <em>Wait a minute</em>, you ought to be saying. <em>How do we know the inverse of <span class="math inline">\(\mathbf{X}^\top \mathbf{X}\)</span> exists?</em> That’s an excellent question! Luckily, there’s a simple condition: <span class="math inline">\(\mathbf{X}^\top \mathbf{X}\)</span> is invertible if and only if the columns of <span class="math inline">\(\mathbf{X}\)</span> are linearly independent.</p>
<p>The linear independence condition isn’t just a technical thing that we need to satisfy. It goes to the heart of what we’re doing in linear regression. If the columns of <span class="math inline">\(\mathbf{X}\)</span> aren’t linearly independent, then the question you’re asking of OLS—to learn something about the coefficients from the data—is ill-defined.</p>
<p>Imagine you have a linear dependency between two variables, so one is just a scalar multiple of the other. For example, a regression of a person’s weight on their height in inches and height in centimeters. Or a regression of whether it rains on temperature Fahrenheit and temperature Celsius. It is absurd to think that the relationship between temperature and rain might be different depending on how you measure it. But that’s exactly what you’re asking for when you run this regression—separate estimates for the effect of degrees Fahrenheit and the effect of degrees Celsius.</p>
</div>
<div id="vector-valued-random-variables" class="section level2">
<h2><span class="header-section-number">7.3</span> Vector-Valued Random Variables</h2>
<p>Before we can talk about the properties of OLS in the multivariate case, we need to refresh ourselves on how basic statistical operations (expected value and variance) translate when we’re dealing with vectors of random variables.</p>
<p>Let <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> be random variables with means <span class="math inline">\(\mu_A = E[A]\)</span> and <span class="math inline">\(\mu_B = E[B]\)</span> respectively. Let <span class="math inline">\(C\)</span> be the column vector whose first value is <span class="math inline">\(A\)</span> and whose second value is <span class="math inline">\(B\)</span>: <span class="math display">\[
C = \begin{pmatrix} A \\ B \end{pmatrix}.
\]</span> As a function of random variables, <span class="math inline">\(C\)</span> is itself a random variable. Unlike those we’ve encountered before, though, it is a <em>vector-valued</em> random variable.</p>
<p>Assume <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> take values in the finite sets <span class="math inline">\(\mathcal{A}\)</span> and <span class="math inline">\(\mathcal{B}\)</span> respectively. The expected value of <span class="math inline">\(C\)</span> is <span class="math display">\[
\begin{aligned}
E[C]
&amp;= \sum_{a \in \mathcal{A}} \sum_{b \in \mathcal{B}} \begin{pmatrix} a \\ b \end{pmatrix} \Pr(A = a, B = b) \\
&amp;= \begin{pmatrix} \mu_A \\ \mu_B \end{pmatrix}.
\end{aligned}
\]</span> I encourage you to prove this on your own—the proof just relies on simple facts about vector addition and joint probability that we’ve already covered in this class. It is easiest to prove in the finite case, but it remains true that <span class="math inline">\(E[C] = (\mu_A, \mu_B)\)</span> in the more general case.</p>
<p>You might expect the variance of <span class="math inline">\(C\)</span> to be a vector too. You would be wrong—it’s a <span class="math inline">\(2 \times 2\)</span> matrix. <span class="math display">\[
\begin{aligned}
V[C]
&amp;= E\left[(C - E[C]) (C - E[C])^\top \right] \\
&amp;= E \left[
  \begin{pmatrix} A - \mu_A \\ B - \mu_B \end{pmatrix}
  \begin{pmatrix} A - \mu_A &amp; B - \mu_B \end{pmatrix}
\right] \\
&amp;= E \left[
  \begin{bmatrix}
  (A - \mu_A)^2 &amp; (A - \mu_A) (B - \mu_B) \\
  (A - \mu_A) (B - \mu_B) &amp; (B - \mu_B)^2
  \end{bmatrix}
\right] \\
&amp;= \begin{bmatrix}
  E[(A - \mu_A)^2] &amp; E[(A - \mu_A) (B - \mu_B)] \\
  E[(A - \mu_A) (B - \mu_B)] &amp; E[(B - \mu_B)^2]
\end{bmatrix} \\
&amp;= \begin{bmatrix}
  V[A] &amp; {\mathop{\rm Cov}\nolimits}[A, B] \\
  {\mathop{\rm Cov}\nolimits}[A, B] &amp; V[B]
\end{bmatrix}.
\end{aligned}
\]</span> This is what we call the <em>variance</em> matrix, or <em>variance-covariance matrix</em>, of a vector-valued random variable. The <span class="math inline">\(i\)</span>’th element along the main diagonal gives us the variance of the <span class="math inline">\(i\)</span>’th element of the vector. The <span class="math inline">\(ij\)</span>’th off-diagonal element gives us the covariance of the <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>’th elements. Consequently, since <span class="math inline">\({\mathop{\rm Cov}\nolimits}[A, B] = {\mathop{\rm Cov}\nolimits}[B, A]\)</span>, the variance matrix is always symmetric.</p>
</div>
<div id="properties-of-ols" class="section level2">
<h2><span class="header-section-number">7.4</span> Properties of OLS</h2>
<p>Just like in the bivariate case, the “good” properties of OLS depend on whether the process that generated our data satisfies particular assumptions. The key assumption, which we call <em>strict exogeneity</em>, is <span class="math display">\[
E[\epsilon \,|\, \mathbf{X}] = \mathbf{0}.
\]</span> In other words, the error term must be uncorrelated with the covariates. Remember that the error for the <span class="math inline">\(n\)</span>’th observation, <span class="math inline">\(\epsilon_n\)</span>, collects everything that affects <span class="math inline">\(Y_n\)</span> but is not included in <span class="math inline">\(\mathbf{x}_n\)</span>. So what we’re saying when we impose this condition is either that there’s nothing else out there that affects <span class="math inline">\(\mathbf{Y}\)</span> besides <span class="math inline">\(\mathbf{X}\)</span> (unlikely!), or that anything else that affects <span class="math inline">\(\mathbf{Y}\)</span> is uncorrelated with <span class="math inline">\(\mathbf{X}\)</span> (also unlikely, but slightly less so!).</p>
<p>In the ’90s and ’00s, as more data became available and computing power increased, political scientists labored under the delusion that the way to make strict exogeneity hold was to throw every covariate you could imagine into each regression. This approach was statistically illiterate <span class="citation">(Clarke <a href="#ref-clarke2005phantom">2005</a>)</span> and scholars have since begun to favor <em>design-based</em> approaches. The basic idea is to collect data with relatively little unobservable heterogeneity, whether through experiments or through careful observational work, rather than to try to eliminate it through endless controls. We’ll talk more about design when we get to causal inference, and it will be a major source of discussion in Stat III.</p>
<p>For now, let us proceed imagining that strict exogeneity holds. Then, just as in the bivariate case, OLS is unbiased. In fact, it’s even easier to prove now. First, notice that under strict exogeneity, we have <span class="math display">\[
\begin{aligned}
E[\mathbf{Y} \,|\, \mathbf{X}]
&amp;= E[\mathbf{X} \beta + \epsilon \,|\, \mathbf{X}] \\
&amp;= \mathbf{X} \beta + E[\epsilon \,|\, \mathbf{X}] \\
&amp;= \mathbf{X} \beta.
\end{aligned}
\]</span> It follows that <span class="math display">\[
\begin{aligned}
E[\hat{\beta}_{{\text{OLS}}}(\mathbf{X}, \mathbf{Y}) \,|\, \mathbf{X}]
&amp;= E[(\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{Y} \,|\, \mathbf{X}] \\
&amp;= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top E[\mathbf{Y} \,|\, \mathbf{X}] \\
&amp;= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top (\mathbf{X} \beta) \\
&amp;= (\mathbf{X}^\top \mathbf{X})^{-1} (\mathbf{X}^\top \mathbf{X}) \beta \\
&amp;= \beta,
\end{aligned}
\]</span> which is the definition of unbiasedness.</p>
<p>Unbiasedness is a small-sample property. No matter the sample size, if strict exogeneity holds, the OLS estimator is unbiased. OLS also has some asymptotic (or large-sample) properties under strict exogeneity that we won’t prove, but are worth mentioning:</p>
<ul>
<li><p>OLS is <em>consistent</em>. Informally, what this means is that as <span class="math inline">\(N\)</span> grows larger, the distribution of the OLS estimator becomes tighter around the population parameter <span class="math inline">\(\beta\)</span>. In other words, with a sufficiently large sample, it becomes highly unlikely that you will draw a sample <span class="math inline">\((\mathbf{X}, \mathbf{Y})\)</span> such that <span class="math inline">\(\hat{\beta}_{{\text{OLS}}}(\mathbf{X}, \mathbf{Y})\)</span> is far from the true value.</p>
<p>Of course, you can’t know that the OLS estimate from any particular sample is close to the truth. But you’re much more likely to get an estimate close to the truth if <span class="math inline">\(N = 100{,}000\)</span> than if <span class="math inline">\(N = 10\)</span>.</p></li>
<li><p>OLS is <em>asymptotically normal</em>. Informally, what this is means is that if <span class="math inline">\(N\)</span> is large enough, the sampling distribution of <span class="math inline">\(\hat{\beta}_{{\text{OLS}}}\)</span> (i.e., its distribution across different possible samples) is roughly normal. This makes the computation of inferential statistics fairly simple in large samples. More on this in two weeks.</p></li>
</ul>
<p>Unbiasedness and consistency are nice, but frankly they’re kind of dime-a-dozen. Lots of estimators are unbiased and consistent. Why is OLS so ubiquitous? The reason is that it is <em>efficient</em>, at least under a particular condition on the error term. Unlike unbiasedness and consistency, efficiency is defined with reference to other estimators. Given some class or collection of estimators, one is efficient if it has the lowest standard errors—i.e., it is the least sensitive to sampling variation, and thereby the most likely to come close to the true parameter value.</p>
<p>The condition we need to hold is that we have <em>spherical errors</em>: <span class="math display">\[
V[\epsilon \,|\, \mathbf{X}] = \sigma^2 \mathbf{I}_N
= \begin{bmatrix}
  \sigma^2 &amp; 0 &amp; \cdots &amp; 0 \\
  0 &amp; \sigma^2 &amp; \cdots &amp; 0 \\
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
  0 &amp; 0 &amp; \cdots &amp; \sigma^2
\end{bmatrix}.
\]</span> Spherical errors summarizes two important conditions:</p>
<ul>
<li><p>The variance of each <span class="math inline">\(\epsilon_n\)</span>—i.e., the expected “spread” of the points around the regression line—is the same for every observation. This is also known as <em>homoskedasticity</em>.</p></li>
<li><p>For <span class="math inline">\(n \neq m\)</span>, there is no correlation between <span class="math inline">\(\epsilon_n\)</span> and <span class="math inline">\(\epsilon_m\)</span>. In other words, the fact that <span class="math inline">\(Y_n\)</span> lies above the regression line doesn’t tell us anything about whether <span class="math inline">\(Y_m\)</span> lies above or below the regression line. This is also known as <em>no autocorrelation</em>.</p></li>
</ul>
<p>Spherical errors holds if each <span class="math inline">\(\epsilon_n\)</span> is independent and identically distributed, though it is possible for non-i.i.d. errors to satisfy the condition. The illustration below compares spherical and non-spherical errors.</p>
<p><img src="pdaps_files/figure-html/spherical-errors-1.png" width="672" /></p>
<p>Notice that in the right-hand graph, the distribution of errors around the regression line is uneven—the spread is much greater at greater values of the covariate.</p>
<p>According to the Gauss-Markov theorem, if the errors are spherical, then OLS is the <em>best linear unbiased estimator (BLUE)</em> of the linear model parameters <span class="math inline">\(\beta\)</span>. By “best,” we mean that it is efficient—any other linear unbiased estimator has larger standard errors. In other words, under the spherical error condition, any estimator <span class="math inline">\(\hat{\beta}\)</span> with a smaller standard errors than OLS must either be:</p>
<ul>
<li><p>Biased: <span class="math inline">\(E[\hat{\beta}] \neq \beta\)</span>.</p></li>
<li><p>Nonlinear: <span class="math inline">\(\hat{\beta}\)</span> cannot be written as a linear function of <span class="math inline">\(Y\)</span>.</p></li>
</ul>
<p>Much later in the course, we will encounter ridge regression, a linear estimator that has lower standard errors than OLS. The Gauss-Markov theorem tells us that we’re making a tradeoff when we use ridge regression—that we’re taking on some bias in exchange for the reduction in variance.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-clarke2005phantom">
<p>Clarke, Kevin A. 2005. “The Phantom Menace: Omitted Variable Bias in Econometric Research.” <em>Conflict Management and Peace Science</em> 22 (4): 341–52.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="20">
<li id="fn20"><p>I’m using <span class="math inline">\(b_k\)</span> instead of <span class="math inline">\(\hat{\beta}_k\)</span> simply because it’s exhausting to type all those <code>\hat{}</code>s.<a href="ols-matrix.html#fnref20">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="matrix.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="specification.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/06-ols-matrix.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
