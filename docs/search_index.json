[
["index.html", "Practical Data Analysis for Political Scientists 1 About This Book", " Practical Data Analysis for Political Scientists Brenton Kenkel 2017-04-02 1 About This Book This book contains the course notes for Brenton Kenkel’s course Statistics for Political Research II (PSCI 8357 at Vanderbilt University). It covers the basics of statistical modeling and programming with linear models, along with applications in R. This book is written in R Markdown and published via Bookdown on GitHub Pages. You can find the R Markdown source files at https://github.com/brentonk/pdaps. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
["programming.html", "2 Principles of Programming 2.1 Write Programs for People, Not Computers 2.2 Let the Computer Do the Work", " 2 Principles of Programming It may seem strange to begin a statistics class with two weeks on programming. It is strange. Here is why I have made this strange choice. First, as a working social scientist, most of the time you spend on data analysis won’t be on the analysis part. It’ll be on obtaining and cleaning the data, to get it in a form that makes sense to analyze. Good programming skills will let you spend less time cleaning data and more time publishing papers. Second, even if you don’t want to develop good programming habits, journals are going to force you to. Every reputable political science journal requires that you provide replication scripts, and some of the best (e.g., American Journal of Political Science) have begun auditing the replication materials as a condition of publication. Better to learn The Right Way now when you have lots of time than to be forced to when you’re writing a dissertation or on the market or teaching your own courses. Third, while I feel embarrassed to invoke the cliché that is Big Data, that doesn’t mean it’s not a real thing. Political scientists have access to more data and more computing power than ever before. You can’t collect, manage, clean, and analyze large quantities of data without understanding the basic principles of programming. As Bowers (2011) puts it, “Data analysis is computer programming.” By getting a PhD in political science,1 by necessity you’re going to become a computer programmer. The choice before you is whether to be a good one or a bad one. G. Wilson et al. (2014) list eight “best practices for scientific computing.” The first two encapsulate most of what you need to know: Write programs for people, not computers. Let the computer do the work. 2.1 Write Programs for People, Not Computers The first two words here—write programs—are crucial. When you are doing analysis for a research project, you should be writing and running scripts, not typing commands into the R (or Stata) console. The console is ephemeral, but scripts are forever, at least if you save them. Like the manuscripts you will write to describe your findings, your analysis scripts are a form of scientific communication. You wouldn’t write a paper that is disorganized, riddled with grammatical errors, or incomprehensible to anyone besides yourself. Don’t write your analysis scripts that way either. Each script should be self-contained, ideally accomplishing one major task. Using an omnibus script that runs every bit of analysis is like writing a paper without paragraph breaks. A typical breakdown of scripts for a project of mine looks like: 0-download.r: downloads the data 1-clean.r: cleans the data 2-run.r: runs the main analysis 3-figs.r: generates figures The exact structure varies depending on the nature of the project. Notice that the scripts are numbered in the order they should be run. Within each script, write the code to make it as easy as possible for your reader to follow what you’re doing. You should indent your code according to style conventions such as http://adv-r.had.co.nz/Style.html. Even better, use the Code -&gt; Reindent Lines menu option in R Studio to automatically indent according to a sane style. # Bad my_results &lt;- c(mean(variable), quantile(variable, probs = 0.25), max(variable)) # Better my_results &lt;- c(mean(variable), quantile(variable, probs = 0.25), max(variable)) Another way to make your code readable—one that, unfortunately, cannot be accomplished quite so algorithmically—is to add explanatory comments. The point of comments is not to document how the language works. The following comment is an extreme example of a useless comment. # Take the square root of the errors and assign them to # the output variable output &lt;- sqrt(errors) A better use for the comment would be to explain why you’re taking the square root of the errors, at least if your purpose in doing so would be unclear to a hypothetical reader of the code. My basic heuristic for code readability is If I got hit by a bus tomorrow, could one of my coauthors figure out what the hell I was doing and finish the paper? 2.2 Let the Computer Do the Work Computers are really good at structured, repetitive tasks. If you ever find yourself entering the same thing into the computer over and over again, you are Doing It Wrong. Your job as the human directing the computer is to figure out the structure that underlies the repeated task and to program the computer to do the repetition. For example, imagine you have just run a large experiment and you want to estimate effects by subgroups.2 Your respondents differ across four variables—party ID (R or D), gender (male or female), race (white or nonwhite), and education (college degree or not)—giving you 16 subgroups. You could copy and paste your code to estimate the treatment effect 16 times. But this is a bad idea for a few reasons. Copy-paste doesn’t scale. Copy-paste is managable (albeit misguided) for 16 iterations, but probably not for 50 and definitely not for more than 100. Making changes becomes painful. Suppose you decide to change how you calculate the estimate. Now you have to go back and individually edit 16 chunks of code. Copy-paste is error-prone, and insidiously so. If you do the calculation wrong all 16 times, you’ll probably notice. But what if you screwed up for just one or two cases? Are you really going to go through and check that you did everything right in each individual case? We’re going to look at the most basic ways to get the computer to repeat structured tasks—functions and control flow statements. To illustrate these, we will use a result you discussed in Stat I: the central limit theorem. The central limit theorem concerns the sampling distribution of the sample mean, \\[\\begin{equation*} \\bar{X} = \\frac{1}{N} \\sum_{n = 1}^{N} X_n, \\end{equation*}\\] where each \\(X_n\\) is independent and identically distributed with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Loosely speaking, the CLT says that as \\(N\\) grows large, the sampling distribution of \\(\\bar{X}\\) becomes approximately normal with mean \\(\\mu\\) and variance \\(\\sigma^2 / N\\). Here’s what we would need to do to see the CLT in practice. We’d want to take a bunch of samples, each of size \\(N\\), and calculate the sample mean of each. Then we’d have a sample of sample means, and we could check to verify that they are approximately normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^2 / N\\). This is a structured, repetitive task—exactly the kind of thing that should be programmed. We’ll try it out with a random variable from a Poisson distribution with \\(\\lambda = 3\\), which has mean \\(\\mu = 3\\) and variance \\(\\sigma^2 = 3\\). First things first. We can use the rpois function to draw a random sample of \\(N\\) numbers from the Poisson distribution. samp &lt;- rpois(10, lambda = 3) samp ## [1] 2 3 8 3 5 4 3 4 2 2 To calculate the sample mean, we simply use the mean function. mean(samp) ## [1] 3.6 We are interested in the distribution of the sample mean across many samples like this one. To begin, we will write a function that automates our core task—drawing a sample of \\(N\\) observations from Poisson(3) and calculating the sample mean. A function consists of a set of arguments (the inputs) and a body of code specifying which calculations to perform on the inputs to produce the output. pois_mean &lt;- function(n_obs) { samp &lt;- rpois(n_obs, lambda = 3) ans &lt;- mean(samp) return(ans) } This code creates a function called pois_mean. It has a single argument, called n_obs. It generates a random sample of n_obs draws from Poisson(3) and calculates its sample mean. It then returns the sample mean as the output. Let’s try calling this function a few times, each with a sample size of \\(N = 30\\). Your output will differ slightly from what’s printed here, since the function is generating random numbers. pois_mean(n_obs = 30) ## [1] 3.0333 pois_mean(n_obs = 30) ## [1] 2.4667 pois_mean(n_obs = 30) ## [1] 2.9667 Remember that what we’re interested in is the sampling distribution of the sample mean—the distribution of the sample mean across every possible sample of \\(N\\) observations. We can approximate this distribution by running pois_mean many times (e.g., 1000 or more). This would be infeasible via copy-paste. Instead, we will use a for loop. # Set up a vector to store the output n_replicates &lt;- 1000 sampling_dist &lt;- rep(NA, n_replicates) for (i in 1:n_replicates) { sampling_dist[i] &lt;- pois_mean(n_obs = 30) } Here’s how the for loop works. We specified i as the name of the index variable, with values 1:n_replicates. The for loop takes each value in the sequence, assigns it to the variable i, runs the given expression (in this case, assigning the output of pois_mean to the i’th element of sampling_dist), and then moves on to the next value in sequence, until it reaches the end. Let’s take a look at the results and compare them to our expectations. mean(sampling_dist) # Expect 3 ## [1] 2.9952 var(sampling_dist) # Expect 1/10 ## [1] 0.096944 hist(sampling_dist) # Expect roughly normal For loops are fun, but don’t overuse them. Many simple operations are vectorized and don’t require a loop. For example, suppose you want to take the square of a sequence of numbers. You could use a for loop … input &lt;- c(1, 3, 7, 29) output &lt;- rep(NA, length(input)) for (i in 1:length(input)) { output[i] &lt;- input[i]^2 } output ## [1] 1 9 49 841 … but it’s faster (in terms of computational speed) and easier to just take advantage of vectorization: input^2 ## [1] 1 9 49 841 Another useful piece of control flow is if/else statements. These check a logical condition—an expression whose value is TRUE or FALSE—and run different code depending on the value of the expression. (You may want to catch up on the comparison operators: ==, &gt;, &gt;=, &lt;, &lt;=, etc.) Let’s edit the pois_mean function to allow us to calculate the median instead of the mean. We’ll add a second argument to the function, and implement the option using an if/else statement. pois_mean &lt;- function(n_obs, use_median = FALSE) { samp &lt;- rpois(n_obs, lambda = 3) if (use_median) { ans &lt;- median(samp) } else { ans &lt;- mean(samp) } return(ans) } A couple of things to notice about the structure of the function. We use a comma to separate multiple function arguments. Also, we’ve specified FALSE as the default for the use_median argument. If we call the function without explicitly specifying a value for use_median, the function sets it to FALSE. pois_mean(n_obs = 9) ## [1] 3.7778 pois_mean(n_obs = 9, use_median = TRUE) ## [1] 2 pois_mean(n_obs = 9, use_median = FALSE) ## [1] 2.6667 There is a vectorized version of if/else statements called, naturally, the ifelse function. This function takes three arguments, each a vector of the same length: (1) a logical condition, (2) an output value if the condition is TRUE, (3) an output value if the condition is FALSE. x &lt;- 1:10 big_x &lt;- x * 100 small_x &lt;- x * -100 ifelse(x &gt; 5, big_x, small_x) ## [1] -100 -200 -300 -400 -500 600 700 800 900 1000 Functions, for loops, and if/else statements are just a few of the useful tools for programming in R.3 But even these simple tools are enough to allow you to do much more at scale than you could with a copy-paste philosophy. References "],
["data.html", "3 Working with Data 3.1 Loading 3.2 Tidying 3.3 Transforming and Aggregating 3.4 Merging 3.5 Appendix: Creating the Example Data", " 3 Working with Data Some material in this chapter is adapted from notes Matt DiLorenzo wrote for the Spring 2016 session of PSCI 8357. Let me repeat something I said last week. In your careers as social scientists, starting with your dissertation research—if not earlier—you will probably spend more time collecting, merging, and cleaning data than you will on statistical analysis. So it’s worth taking some time to learn how to do this well. Best practices for data management can be summarized in a single sentence: Record and document everything you do to the data. The first corollary of this principle is that raw data is sacrosanct. You should never edit raw data “in place”. Once you download the raw data file, that file should never change.4 In almost any non-trivial analysis, the “final” data—the format you plug into your analysis—will differ significantly from the raw data. It may consist of information merged from multiple sources. The variables may have been transformed, aggregated, or otherwise altered. The unit of observation may even differ from the original source. You must document every one of these changes, so that another researcher working from the exact same raw data will end up with the exact same final data. The most sensible way to achieve this level of reproducibility is to do all of your data merging and cleaning in a script. In other words, no going into Excel and mucking around manually. Like any other piece of your analysis, your pipeline from raw data to final data should follow the principles of programming that we discussed last week. Luckily for you,5 the tidyverse suite of R packages (including dplyr, tidyr, and others) makes it easy to script your “data pipeline”. We’ll begin by loading the package. library(&quot;tidyverse&quot;) 3.1 Loading The first step in working with data is to acquire some data. Depending on the nature of your research, you will be getting some or all of your data from sources available online. When you download data from online repositories, you should keep track of where you got it from. The best way to do so is—you guessed it—to script your data acquisition. The R function download.file() is the easiest way to download files from URLs from within R. Just specify where you’re getting the file from and where you want it to go. For the examples today, we’ll use an “untidied” version of the World Development Indicators data from the World Bank that I’ve posted to my website. download.file(url = &quot;http://bkenkel.com/data/untidy-data.csv&quot;, destfile = &quot;my-untidy-data.csv&quot;) Once you’ve got the file stored locally, use the utilities from the readr package (part of tidyverse) to read it into R as a data frame.6 We have a CSV file, so we will use read_csv. See help(package = &quot;readr&quot;) for other possibilities. untidy_data &lt;- read_csv(file = &quot;my-untidy-data.csv&quot;) ## Parsed with column specification: ## cols( ## country = col_character(), ## gdp.2005 = col_double(), ## gdp.2006 = col_double(), ## gdp.2007 = col_double(), ## gdp.2008 = col_double(), ## pop.2005 = col_double(), ## pop.2006 = col_double(), ## pop.2007 = col_double(), ## pop.2008 = col_double(), ## unemp.2005 = col_double(), ## unemp.2006 = col_double(), ## unemp.2007 = col_double(), ## unemp.2008 = col_double() ## ) Remember that each column of a data frame might be a different type, or more formally class, of object. read_csv and its ilk try to guess the type of data each column contains: character, integer, decimal number (“double” in programming-speak), or something else. The readout above tells you what guesses it made. If it gets something wrong—say, reading a column as numbers that ought to be characters—you can use the col_types argument to set it straight. FYI, you could also run read_csv() directly on a URL, as in: read_csv(&quot;http://bkenkel.com/data/untidy-data.csv&quot;) However, in analyses intended for publication, it’s usually preferable to download and save the raw data. What’s stored at a URL might change or disappear, and you’ll need to have a hard copy of the raw data for replication purposes. Now let’s take a look at the data we’ve just loaded in. head(untidy_data) ## # A tibble: 6 × 13 ## country gdp.2005 gdp.2006 gdp.2007 gdp.2008 pop.2005 pop.2006 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AD 3.8423 4.0184 4.0216 3.6759 0.081223 0.083373 ## 2 AE 253.9655 278.9489 287.8318 297.0189 4.481976 5.171255 ## 3 AF 9.7630 10.3052 11.7212 12.1445 24.399948 25.183615 ## 4 AG 1.1190 1.2687 1.3892 1.3902 0.082565 0.083467 ## 5 AL 9.2684 9.7718 10.3483 11.1275 3.011487 2.992547 ## 6 AM 7.6678 8.6797 9.8731 10.5544 3.014917 3.002161 ## # ... with 6 more variables: pop.2007 &lt;dbl&gt;, pop.2008 &lt;dbl&gt;, ## # unemp.2005 &lt;dbl&gt;, unemp.2006 &lt;dbl&gt;, unemp.2007 &lt;dbl&gt;, unemp.2008 &lt;dbl&gt; We have a country variable giving country abbreviations. The other variables are numerical values: the country’s GDP in 2005, 2006, 2007, and 2008; then the same for population and unemployment. Let’s get this into a format we could use for analysis. 3.2 Tidying Wickham (2014) outlines three qualities that make data “tidy”: Each variable forms a column. Each observation forms a row. Each type of observational unit forms a table. For one thing, this means that whether a dataset is tidy or not depends—at least in part (some data collections are messy from any angle)—on the purpose it’s being analyzed for. Each row of untidy_data is a country. In observational studies in comparative politics and international relations, more commonly the unit of observation is the country-year.7 How can we take untidy_data and easily make it into country-year data? We’ll use the tidyr package (again, part of tidyverse) to clean up this data. The biggest problem right now is that each column, besides the country identifier, really encodes two pieces of information: the year of observation and the variable being observed. To deal with this, we’ll have to first transform the data from one untidy format to another. We’re going to use the gather() function to make each row a country-year-variable. What gather() does is make a row for each entry from a set of columns. It’s probably easiest to understand it by seeing it in practice: long_data &lt;- gather(untidy_data, key = variable, value = number, gdp.2005:unemp.2008) head(long_data) ## # A tibble: 6 × 3 ## country variable number ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 AD gdp.2005 3.8423 ## 2 AE gdp.2005 253.9655 ## 3 AF gdp.2005 9.7630 ## 4 AG gdp.2005 1.1190 ## 5 AL gdp.2005 9.2684 ## 6 AM gdp.2005 7.6678 With the first argument, we told gather() to use the untidy_data data frame. With the last argument, we told it the set of columns to “gather” together into a single column. The key column specifies the name of the variable to store the “key” (original column name) in, and the value column specifies the name of the variable to store the associated value. For example, the second row of long_data encodes what we previously saw as the gdp.2005 column of untidy_data. Now we have a new problem, which is that variable encodes two pieces of information: the variable and the year of its observation. tidyr provides the separate() function to solve that, splitting a single variable into two. long_data &lt;- separate(long_data, col = variable, into = c(&quot;var&quot;, &quot;year&quot;)) head(long_data) ## # A tibble: 6 × 4 ## country var year number ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 AD gdp 2005 3.8423 ## 2 AE gdp 2005 253.9655 ## 3 AF gdp 2005 9.7630 ## 4 AG gdp 2005 1.1190 ## 5 AL gdp 2005 9.2684 ## 6 AM gdp 2005 7.6678 So now we have country-year-variable data, with the year and variable conveniently stored in different columns. To turn this into country-year data, we can use the spread() function, which is like the inverse of gather(). spread() takes a key column and a value column, and turns each different key into a column of its own. clean_data &lt;- spread(long_data, key = var, value = number) head(clean_data) ## # A tibble: 6 × 5 ## country year gdp pop unemp ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AD 2005 3.8423 0.081223 NA ## 2 AD 2006 4.0184 0.083373 NA ## 3 AD 2007 4.0216 0.084878 NA ## 4 AD 2008 3.6759 0.085616 NA ## 5 AE 2005 253.9655 4.481976 3.1 ## 6 AE 2006 278.9489 5.171255 3.3 When using spread() on data that you didn’t previously gather(), be sure to set the fill argument to tell it how to fill in empty cells. A simple example: test_data ## # A tibble: 3 × 3 ## id k v ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 brenton a 10 ## 2 brenton b 20 ## 3 patrick b 5 spread(test_data, key = k, value = v) ## # A tibble: 2 × 3 ## id a b ## * &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 brenton 10 20 ## 2 patrick NA 5 spread(test_data, key = k, value = v, fill = 100) ## # A tibble: 2 × 3 ## id a b ## * &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 brenton 10 20 ## 2 patrick 100 5 One more important note on tidyverse semantics. It includes a fabulous feature called the pipe, %&gt;%, which makes it easy to string together a truly mind-boggling number of commands. In pipe syntax, x %&gt;% f() is equivalent to f(x). That seems like a wasteful and confusing way to write f(x), and it is. But if you want to string together a bunch of commands, it’s much easier to comprehend x %&gt;% f() %&gt;% g() %&gt;% h() %&gt;% i() than i(h(g(f(x)))). You can pass function arguments using the pipe too. For example, f(x, bear = &quot;moose&quot;) is equivalent to x %&gt;% f(bear = &quot;moose&quot;). The key thing about the tidyverse functions is that each of them takes a data frame as its first argument, and returns a data frame as its output. This makes them highly amenable to piping. For example, we can combine all three steps of our tidying above with a single command, thanks to the pipe:8 untidy_data %&gt;% gather(key = variable, value = number, gdp.2005:unemp.2008) %&gt;% separate(col = variable, into = c(&quot;var&quot;, &quot;year&quot;)) %&gt;% spread(key = var, value = number) ## # A tibble: 860 × 5 ## country year gdp pop unemp ## * &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AD 2005 3.8423 0.081223 NA ## 2 AD 2006 4.0184 0.083373 NA ## 3 AD 2007 4.0216 0.084878 NA ## 4 AD 2008 3.6759 0.085616 NA ## 5 AE 2005 253.9655 4.481976 3.1 ## # ... with 855 more rows Without the pipe, if we wanted to run all those commands together, we would have to write: spread(separate(gather(untidy_data, key = variable, value = number, gdp.2005:unemp.2008), col = variable, into = c(&quot;var&quot;, &quot;year&quot;)), key = var, value = number) Sad! 3.3 Transforming and Aggregating Tidying the data usually isn’t the end of the process. If you want to perform further calculations on the raw, that’s where the tools in dplyr (part of, you guessed it, the tidyverse) come in. Perhaps the simplest dplyr function (or “verb”, as the R hipsters would say) is rename(), which lets you rename columns. clean_data %&gt;% rename(gross_domestic_product = gdp) ## # A tibble: 860 × 5 ## country year gross_domestic_product pop unemp ## * &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AD 2005 3.8423 0.081223 NA ## 2 AD 2006 4.0184 0.083373 NA ## 3 AD 2007 4.0216 0.084878 NA ## 4 AD 2008 3.6759 0.085616 NA ## 5 AE 2005 253.9655 4.481976 3.1 ## # ... with 855 more rows The dplyr functions, like the vast majority of R functions, do not modify their inputs. In other words, running rename() on clean_data will return a renamed copy of clean_data, but won’t overwrite the original. clean_data ## # A tibble: 860 × 5 ## country year gdp pop unemp ## * &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AD 2005 3.8423 0.081223 NA ## 2 AD 2006 4.0184 0.083373 NA ## 3 AD 2007 4.0216 0.084878 NA ## 4 AD 2008 3.6759 0.085616 NA ## 5 AE 2005 253.9655 4.481976 3.1 ## # ... with 855 more rows If you wanted to make the change stick, you would have to run: clean_data &lt;- clean_data %&gt;% rename(gross_domestic_product = gdp) select() lets you keep a couple of columns and drop all the others. Or vice versa if you use minus signs. clean_data %&gt;% select(country, gdp) ## # A tibble: 860 × 2 ## country gdp ## * &lt;chr&gt; &lt;dbl&gt; ## 1 AD 3.8423 ## 2 AD 4.0184 ## 3 AD 4.0216 ## 4 AD 3.6759 ## 5 AE 253.9655 ## # ... with 855 more rows clean_data %&gt;% select(-pop) ## # A tibble: 860 × 4 ## country year gdp unemp ## * &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AD 2005 3.8423 NA ## 2 AD 2006 4.0184 NA ## 3 AD 2007 4.0216 NA ## 4 AD 2008 3.6759 NA ## 5 AE 2005 253.9655 3.1 ## # ... with 855 more rows mutate() lets you create new variables that are transformations of old ones. clean_data %&gt;% mutate(gdppc = gdp / pop, log_gdppc = log(gdppc)) ## # A tibble: 860 × 7 ## country year gdp pop unemp gdppc log_gdppc ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AD 2005 3.8423 0.081223 NA 47.305 3.8566 ## 2 AD 2006 4.0184 0.083373 NA 48.198 3.8753 ## 3 AD 2007 4.0216 0.084878 NA 47.381 3.8582 ## 4 AD 2008 3.6759 0.085616 NA 42.935 3.7597 ## 5 AE 2005 253.9655 4.481976 3.1 56.664 4.0371 ## # ... with 855 more rows filter() cuts down the data according to the logical condition(s) you specify. clean_data %&gt;% filter(year == 2006) ## # A tibble: 215 × 5 ## country year gdp pop unemp ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AD 2006 4.0184 0.083373 NA ## 2 AE 2006 278.9489 5.171255 3.3 ## 3 AF 2006 10.3052 25.183615 8.8 ## 4 AG 2006 1.2687 0.083467 NA ## 5 AL 2006 9.7718 2.992547 12.4 ## # ... with 210 more rows summarise() calculates summaries of the data. For example, let’s find the maximum unemployment rate. clean_data %&gt;% summarise(max_unemp = max(unemp, na.rm = TRUE)) ## # A tibble: 1 × 1 ## max_unemp ## &lt;dbl&gt; ## 1 37.6 This seems sort of useless, until you combine it with the group_by() function. If you group the data before summarise-ing it, you’ll calculate a separate summary for each group. For example, let’s calculate the maximum unemployment rate for each year in the data. clean_data %&gt;% group_by(year) %&gt;% summarise(max_unemp = max(unemp, na.rm = TRUE)) ## # A tibble: 4 × 2 ## year max_unemp ## &lt;chr&gt; &lt;dbl&gt; ## 1 2005 37.3 ## 2 2006 36.0 ## 3 2007 34.9 ## 4 2008 37.6 summarise() produces a “smaller” data frame than the input—one row per group. If you want to do something similar, but preserving the structure of the original data, use mutate in combination with group_by. clean_data %&gt;% group_by(year) %&gt;% mutate(max_unemp = max(unemp, na.rm = TRUE), unemp_over_max = unemp / max_unemp) %&gt;% select(country, year, contains(&quot;unemp&quot;)) ## Source: local data frame [860 x 5] ## Groups: year [4] ## ## country year unemp max_unemp unemp_over_max ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AD 2005 NA 37.3 NA ## 2 AD 2006 NA 36.0 NA ## 3 AD 2007 NA 34.9 NA ## 4 AD 2008 NA 37.6 NA ## 5 AE 2005 3.1 37.3 0.08311 ## # ... with 855 more rows This gives us back the original data, but with a max_unemp variable recording the highest unemployment level that year. We can then calculate each individual country’s unemployment as a percentage of the maximum. Whether grouped mutate or summarise is better depends, of course, on the purpose and structure of your analysis. Notice how I selected all of the unemployment-related columns with contains(&quot;unemp&quot;). See ?select_helpers for a full list of helpful functions like this for select-ing variables. 3.4 Merging Only rarely will you be lucky enough to draw all your data from a single source. More often, you’ll be merging together data from multiple sources. The key to merging data from separate tables is to have consistent identifiers across tables. For example, if you run an experiment, you might have demographic data on each subject in one table, and each subject’s response to each treatment in another table. Naturally, you’ll want to have a subject identifier that “links” the records across tables, as in the following hypothetical example. subject_data ## # A tibble: 3 × 4 ## id gender loves_bernie does_yoga ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1001 male yes no ## 2 1002 female no yes ## 3 1003 male no no subject_response_data ## # A tibble: 6 × 3 ## id treatment response ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1001 read_book sad ## 2 1001 watch_tv sad ## 3 1002 read_book happy ## 4 1002 watch_tv sad ## 5 1003 read_book sad ## 6 1003 watch_tv happy Let’s practice merging data with our cleaned-up country-year data. We’ll take two datasets from my website: a country-level dataset with latitudes and longitudes, and a country-year–level dataset with inflation over time. latlong_data &lt;- read_csv(&quot;http://bkenkel.com/data/latlong.csv&quot;) latlong_data ## # A tibble: 245 × 3 ## country latitude longitude ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AD 42.546 1.6016 ## 2 AE 23.424 53.8478 ## 3 AF 33.939 67.7100 ## 4 AG 17.061 -61.7964 ## 5 AI 18.221 -63.0686 ## # ... with 240 more rows inflation_data &lt;- read_csv(&quot;http://bkenkel.com/data/inflation.csv&quot;) inflation_data ## # A tibble: 1,070 × 3 ## country year inflation ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 AD 2004 NA ## 2 AD 2005 NA ## 3 AD 2006 NA ## 4 AD 2007 NA ## 5 AD 2008 NA ## # ... with 1,065 more rows For your convenience, both of these datasets use the same two-letter country naming scheme as the original data. Unfortunately, out in the real world, data from different sources often use incommensurate naming schemes. Converting from one naming scheme to another is part of the data cleaning process, and it requires careful attention. dplyr contains various _join() functions for merging. Each of these take as arguments the two data frames to merge, plus the names of the identifier variables to merge them on. The one I use most often is left_join(), which keeps every row from the first (“left”) data frame and merges in the columns from the second (“right”) data frame. For example, let’s merge the latitude and longitude data for each country into clean_data. left_join(clean_data, latlong_data, by = &quot;country&quot;) ## # A tibble: 860 × 7 ## country year gdp pop unemp latitude longitude ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AD 2005 3.8423 0.081223 NA 42.546 1.6016 ## 2 AD 2006 4.0184 0.083373 NA 42.546 1.6016 ## 3 AD 2007 4.0216 0.084878 NA 42.546 1.6016 ## 4 AD 2008 3.6759 0.085616 NA 42.546 1.6016 ## 5 AE 2005 253.9655 4.481976 3.1 23.424 53.8478 ## # ... with 855 more rows Since latlong_data is country-level, the value is the same for each year. So the merged data contains redundant information. This is one reason to store data observed at different levels in different tables—with redundant observations, it is easier to make errors yet harder to catch them and fix them. We can also merge data when the identifier is stored across multiple columns, as in the case of our country-year data. But first, a technical note.9 You might notice that the year column of clean_data is labeled &lt;chr&gt;, as in character data. Yet the year column of inflation_data is labeled &lt;int&gt;, as in integer data. We can check that by running class() on each respective column. class(clean_data$year) ## [1] &quot;character&quot; class(inflation_data$year) ## [1] &quot;integer&quot; From R’s perspective, the character string &quot;1999&quot; is a very different thing than the integer number 1999. Therefore, if we try to merge clean_data and inflation_data on the year variable, it will throw an error. left_join(clean_data, inflation_data, by = c(&quot;country&quot;, &quot;year&quot;)) ## Error in left_join_impl(x, y, by$x, by$y, suffix$x, suffix$y): Can&#39;t join on &#39;year&#39; x &#39;year&#39; because of incompatible types (integer / character) To fix this, let’s use mutate() to convert the year column of clean_data to an integer. We probably should have done this in the first place—after all, having the year encoded as a character string would have thrown off plotting functions, statistical functions, or anything else where it would be more natural to treat the year like a number. clean_data &lt;- mutate(clean_data, year = as.integer(year)) clean_data ## # A tibble: 860 × 5 ## country year gdp pop unemp ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AD 2005 3.8423 0.081223 NA ## 2 AD 2006 4.0184 0.083373 NA ## 3 AD 2007 4.0216 0.084878 NA ## 4 AD 2008 3.6759 0.085616 NA ## 5 AE 2005 253.9655 4.481976 3.1 ## # ... with 855 more rows Looks the same as before, except with an important difference: year is now labeled &lt;int&gt;. Now we can merge the two datasets together without issue. Notice how we use a vector in the by argument to specify multiple columns to merge on. left_join(clean_data, inflation_data, by = c(&quot;country&quot;, &quot;year&quot;)) ## # A tibble: 860 × 6 ## country year gdp pop unemp inflation ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AD 2005 3.8423 0.081223 NA NA ## 2 AD 2006 4.0184 0.083373 NA NA ## 3 AD 2007 4.0216 0.084878 NA NA ## 4 AD 2008 3.6759 0.085616 NA NA ## 5 AE 2005 253.9655 4.481976 3.1 NA ## # ... with 855 more rows You might remember that inflation_data contained some country-years not included in the original data (namely, observations from 2004). If we want the merged data to use the observations from inflation_data rather than clean_data, we can use the right_join() function. right_join(clean_data, inflation_data, by = c(&quot;country&quot;, &quot;year&quot;)) ## # A tibble: 1,070 × 6 ## country year gdp pop unemp inflation ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AD 2004 NA NA NA NA ## 2 AD 2005 3.8423 0.081223 NA NA ## 3 AD 2006 4.0184 0.083373 NA NA ## 4 AD 2007 4.0216 0.084878 NA NA ## 5 AD 2008 3.6759 0.085616 NA NA ## # ... with 1,065 more rows One last common issue in merging is that the identifier variables have different names in the two datasets. If it’s inconvenient or infeasible to correct this by renaming the columns in one or the other, you can specify the by argument as in the following example. inflation_data &lt;- rename(inflation_data, the_country = country, the_year = year) inflation_data ## # A tibble: 1,070 × 3 ## the_country the_year inflation ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 AD 2004 NA ## 2 AD 2005 NA ## 3 AD 2006 NA ## 4 AD 2007 NA ## 5 AD 2008 NA ## # ... with 1,065 more rows left_join(clean_data, inflation_data, by = c(&quot;country&quot; = &quot;the_country&quot;, &quot;year&quot; = &quot;the_year&quot;)) ## # A tibble: 860 × 6 ## country year gdp pop unemp inflation ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AD 2005 3.8423 0.081223 NA NA ## 2 AD 2006 4.0184 0.083373 NA NA ## 3 AD 2007 4.0216 0.084878 NA NA ## 4 AD 2008 3.6759 0.085616 NA NA ## 5 AE 2005 253.9655 4.481976 3.1 NA ## # ... with 855 more rows 3.5 Appendix: Creating the Example Data I used the same tools this chapter introduces to create the untidy data. I may as well include the code to do it, in case it helps further illustrate how to use the tidyverse tools (and, as a bonus, the WDI package for downloading World Development Indicators data). First I load the necessary packages. library(&quot;tidyverse&quot;) library(&quot;WDI&quot;) library(&quot;countrycode&quot;) library(&quot;stringr&quot;) Next, I download the relevant WDI data. I used the WDIsearch() function to locate the appropriate indicator names. dat_raw &lt;- WDI(country = &quot;all&quot;, indicator = c(&quot;NY.GDP.MKTP.KD&quot;, # GDP in 2000 USD &quot;SP.POP.TOTL&quot;, # Total population &quot;SL.UEM.TOTL.ZS&quot;), # Unemployment rate start = 2005, end = 2008) head(dat_raw) ## iso2c country year NY.GDP.MKTP.KD SP.POP.TOTL SL.UEM.TOTL.ZS ## 1 1A Arab World 2005 1.6428e+12 313430911 12.1402 ## 2 1A Arab World 2006 1.7629e+12 320906736 11.3296 ## 3 1A Arab World 2007 1.8625e+12 328766559 10.8961 ## 4 1A Arab World 2008 1.9799e+12 336886468 10.5060 ## 5 1W World 2005 5.7703e+13 6513959904 6.1593 ## 6 1W World 2006 6.0229e+13 6594722462 5.9000 I want to get rid of the aggregates, like the “Arab World” and “World” we see here. As a rough tack at that, I’m going to exclude those so-called countries whose ISO codes don’t appear in the countrycode package data.10 dat_countries &lt;- dat_raw %&gt;% filter(iso2c %in% countrycode_data$iso2c) Let’s check on which countries are left. (I cut it down to max six characters per country name for printing purposes.) dat_countries$country %&gt;% unique() %&gt;% str_sub(start = 1, end = 6) ## [1] &quot;Andorr&quot; &quot;United&quot; &quot;Afghan&quot; &quot;Antigu&quot; &quot;Albani&quot; &quot;Armeni&quot; &quot;Angola&quot; ## [8] &quot;Argent&quot; &quot;Americ&quot; &quot;Austri&quot; &quot;Austra&quot; &quot;Aruba&quot; &quot;Azerba&quot; &quot;Bosnia&quot; ## [15] &quot;Barbad&quot; &quot;Bangla&quot; &quot;Belgiu&quot; &quot;Burkin&quot; &quot;Bulgar&quot; &quot;Bahrai&quot; &quot;Burund&quot; ## [22] &quot;Benin&quot; &quot;Bermud&quot; &quot;Brunei&quot; &quot;Bolivi&quot; &quot;Brazil&quot; &quot;Bahama&quot; &quot;Bhutan&quot; ## [29] &quot;Botswa&quot; &quot;Belaru&quot; &quot;Belize&quot; &quot;Canada&quot; &quot;Congo,&quot; &quot;Centra&quot; &quot;Congo,&quot; ## [36] &quot;Switze&quot; &quot;Cote d&quot; &quot;Chile&quot; &quot;Camero&quot; &quot;China&quot; &quot;Colomb&quot; &quot;Costa &quot; ## [43] &quot;Cuba&quot; &quot;Cabo V&quot; &quot;Curaca&quot; &quot;Cyprus&quot; &quot;Czech &quot; &quot;German&quot; &quot;Djibou&quot; ## [50] &quot;Denmar&quot; &quot;Domini&quot; &quot;Domini&quot; &quot;Algeri&quot; &quot;Ecuado&quot; &quot;Estoni&quot; &quot;Egypt,&quot; ## [57] &quot;Eritre&quot; &quot;Spain&quot; &quot;Ethiop&quot; &quot;Finlan&quot; &quot;Fiji&quot; &quot;Micron&quot; &quot;Faroe &quot; ## [64] &quot;France&quot; &quot;Gabon&quot; &quot;United&quot; &quot;Grenad&quot; &quot;Georgi&quot; &quot;Ghana&quot; &quot;Gibral&quot; ## [71] &quot;Greenl&quot; &quot;Gambia&quot; &quot;Guinea&quot; &quot;Equato&quot; &quot;Greece&quot; &quot;Guatem&quot; &quot;Guam&quot; ## [78] &quot;Guinea&quot; &quot;Guyana&quot; &quot;Hong K&quot; &quot;Hondur&quot; &quot;Croati&quot; &quot;Haiti&quot; &quot;Hungar&quot; ## [85] &quot;Indone&quot; &quot;Irelan&quot; &quot;Israel&quot; &quot;Isle o&quot; &quot;India&quot; &quot;Iraq&quot; &quot;Iran, &quot; ## [92] &quot;Icelan&quot; &quot;Italy&quot; &quot;Jamaic&quot; &quot;Jordan&quot; &quot;Japan&quot; &quot;Kenya&quot; &quot;Kyrgyz&quot; ## [99] &quot;Cambod&quot; &quot;Kiriba&quot; &quot;Comoro&quot; &quot;St. Ki&quot; &quot;Korea,&quot; &quot;Korea,&quot; &quot;Kuwait&quot; ## [106] &quot;Cayman&quot; &quot;Kazakh&quot; &quot;Lao PD&quot; &quot;Lebano&quot; &quot;St. Lu&quot; &quot;Liecht&quot; &quot;Sri La&quot; ## [113] &quot;Liberi&quot; &quot;Lesoth&quot; &quot;Lithua&quot; &quot;Luxemb&quot; &quot;Latvia&quot; &quot;Libya&quot; &quot;Morocc&quot; ## [120] &quot;Monaco&quot; &quot;Moldov&quot; &quot;Monten&quot; &quot;St. Ma&quot; &quot;Madaga&quot; &quot;Marsha&quot; &quot;Macedo&quot; ## [127] &quot;Mali&quot; &quot;Myanma&quot; &quot;Mongol&quot; &quot;Macao &quot; &quot;Northe&quot; &quot;Maurit&quot; &quot;Malta&quot; ## [134] &quot;Maurit&quot; &quot;Maldiv&quot; &quot;Malawi&quot; &quot;Mexico&quot; &quot;Malays&quot; &quot;Mozamb&quot; &quot;Namibi&quot; ## [141] &quot;New Ca&quot; &quot;Niger&quot; &quot;Nigeri&quot; &quot;Nicara&quot; &quot;Nether&quot; &quot;Norway&quot; &quot;Nepal&quot; ## [148] &quot;Nauru&quot; &quot;New Ze&quot; &quot;Oman&quot; &quot;Panama&quot; &quot;Peru&quot; &quot;French&quot; &quot;Papua &quot; ## [155] &quot;Philip&quot; &quot;Pakist&quot; &quot;Poland&quot; &quot;Puerto&quot; &quot;West B&quot; &quot;Portug&quot; &quot;Palau&quot; ## [162] &quot;Paragu&quot; &quot;Qatar&quot; &quot;Romani&quot; &quot;Serbia&quot; &quot;Russia&quot; &quot;Rwanda&quot; &quot;Saudi &quot; ## [169] &quot;Solomo&quot; &quot;Seyche&quot; &quot;Sudan&quot; &quot;Sweden&quot; &quot;Singap&quot; &quot;Sloven&quot; &quot;Slovak&quot; ## [176] &quot;Sierra&quot; &quot;San Ma&quot; &quot;Senega&quot; &quot;Somali&quot; &quot;Surina&quot; &quot;South &quot; &quot;Sao To&quot; ## [183] &quot;El Sal&quot; &quot;Sint M&quot; &quot;Syrian&quot; &quot;Swazil&quot; &quot;Turks &quot; &quot;Chad&quot; &quot;Togo&quot; ## [190] &quot;Thaila&quot; &quot;Tajiki&quot; &quot;Timor-&quot; &quot;Turkme&quot; &quot;Tunisi&quot; &quot;Tonga&quot; &quot;Turkey&quot; ## [197] &quot;Trinid&quot; &quot;Tuvalu&quot; &quot;Tanzan&quot; &quot;Ukrain&quot; &quot;Uganda&quot; &quot;United&quot; &quot;Urugua&quot; ## [204] &quot;Uzbeki&quot; &quot;St. Vi&quot; &quot;Venezu&quot; &quot;Britis&quot; &quot;Virgin&quot; &quot;Vietna&quot; &quot;Vanuat&quot; ## [211] &quot;Samoa&quot; &quot;Yemen,&quot; &quot;South &quot; &quot;Zambia&quot; &quot;Zimbab&quot; With that out of the way, there’s still some cleaning up to do. The magnitudes of GDP and population are too large, and the variable names are impenetrable. Also, the country variable, while helpful, is redundant now that we’re satisfied with the list of countries remaining. dat_countries &lt;- dat_countries %&gt;% select(-country) %&gt;% rename(gdp = NY.GDP.MKTP.KD, pop = SP.POP.TOTL, unemp = SL.UEM.TOTL.ZS, country = iso2c) %&gt;% mutate(gdp = gdp / 1e9, pop = pop / 1e6) head(dat_countries) ## country year gdp pop unemp ## 1 AD 2005 3.8423 0.081223 NA ## 2 AD 2006 4.0184 0.083373 NA ## 3 AD 2007 4.0216 0.084878 NA ## 4 AD 2008 3.6759 0.085616 NA ## 5 AE 2005 253.9655 4.481976 3.1 ## 6 AE 2006 278.9489 5.171255 3.3 Now I convert the data to “long” format. dat_countries_long &lt;- dat_countries %&gt;% gather(key = variable, value = value, gdp:unemp) head(dat_countries_long) ## country year variable value ## 1 AD 2005 gdp 3.8423 ## 2 AD 2006 gdp 4.0184 ## 3 AD 2007 gdp 4.0216 ## 4 AD 2008 gdp 3.6759 ## 5 AE 2005 gdp 253.9655 ## 6 AE 2006 gdp 278.9489 I then smush variable and year into a single column, and drop the individual components. dat_countries_long &lt;- dat_countries_long %&gt;% mutate(var_year = paste(variable, year, sep = &quot;.&quot;)) %&gt;% select(-variable, -year) head(dat_countries_long) ## country value var_year ## 1 AD 3.8423 gdp.2005 ## 2 AD 4.0184 gdp.2006 ## 3 AD 4.0216 gdp.2007 ## 4 AD 3.6759 gdp.2008 ## 5 AE 253.9655 gdp.2005 ## 6 AE 278.9489 gdp.2006 Finally, I “widen” the data, so that each var_year is a column of its own. dat_countries_wide &lt;- dat_countries_long %&gt;% spread(key = var_year, value = value) head(dat_countries_wide) ## country gdp.2005 gdp.2006 gdp.2007 gdp.2008 pop.2005 pop.2006 ## 1 AD 3.8423 4.0184 4.0216 3.6759 0.081223 0.083373 ## 2 AE 253.9655 278.9489 287.8318 297.0189 4.481976 5.171255 ## 3 AF 9.7630 10.3052 11.7212 12.1445 24.399948 25.183615 ## 4 AG 1.1190 1.2687 1.3892 1.3902 0.082565 0.083467 ## 5 AL 9.2684 9.7718 10.3483 11.1275 3.011487 2.992547 ## 6 AM 7.6678 8.6797 9.8731 10.5544 3.014917 3.002161 ## pop.2007 pop.2008 unemp.2005 unemp.2006 unemp.2007 unemp.2008 ## 1 0.084878 0.085616 NA NA NA NA ## 2 6.010100 6.900142 3.1 3.3 3.4 4.0 ## 3 25.877544 26.528741 8.5 8.8 8.4 8.9 ## 4 0.084397 0.085350 NA NA NA NA ## 5 2.970017 2.947314 12.5 12.4 13.5 13.0 ## 6 2.988117 2.975029 27.8 28.6 28.4 16.4 Now we have some ugly data. I save the output to upload to my website. write_csv(dat_countries_wide, path = &quot;untidy-data.csv&quot;) And here’s how I made the second country-year dataset used in the merging section. The country dataset with latitudes and longitudes is from https://developers.google.com/public-data/docs/canonical/countries_csv. dat_2 &lt;- WDI(country = &quot;all&quot;, indicator = &quot;FP.CPI.TOTL.ZG&quot;, start = 2004, end = 2008) %&gt;% as_data_frame() %&gt;% select(country = iso2c, year, inflation = FP.CPI.TOTL.ZG) %&gt;% mutate(year = as.integer(year)) %&gt;% filter(country %in% clean_data$country) %&gt;% arrange(country, year) write_csv(dat_2, path = &quot;inflation.csv&quot;) References "],
["visualization.html", "4 Data Visualization 4.1 Basic Plots 4.2 Saving Plots 4.3 Faceting 4.4 Aesthetics 4.5 Appendix: Creating the Example Data", " 4 Data Visualization Visualization is most important at the very beginning and the very end of the data analysis process. In the beginning, when you’ve just gotten your data together, visualization is perhaps the easiest tool to explore each variable and learn about the relationships among them. And when your analysis is almost complete, you will (usually) use visualizations to communicate your findings to your audience. We only have time to scratch the surface of data visualization. This chapter will cover the plotting techniques I find most useful for exploratory and descriptive data analysis. We will talk about graphical techniques for presenting the results of regression analyses later in the class—once we’ve, you know, learned something about regression. 4.1 Basic Plots We will use the ggplot2 package, which is part of—I’m as tired of it as you are—the tidyverse. library(&quot;tidyverse&quot;) For the examples today, we’ll be using a dataset with statistics about the fifty U.S. states in 1977,11 which is posted on my website. state_data &lt;- read_csv(&quot;http://bkenkel.com/data/state-data.csv&quot;) state_data ## # A tibble: 50 × 12 ## State Abbrev Region Population Income Illiteracy LifeExp Murder ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alabama AL South 3615 3624 2.1 69.05 15.1 ## 2 Alaska AK West 365 6315 1.5 69.31 11.3 ## 3 Arizona AZ West 2212 4530 1.8 70.55 7.8 ## 4 Arkansas AR South 2110 3378 1.9 70.66 10.1 ## 5 California CA West 21198 5114 1.1 71.71 10.3 ## # ... with 45 more rows, and 4 more variables: HSGrad &lt;dbl&gt;, Frost &lt;dbl&gt;, ## # Area &lt;dbl&gt;, IncomeGroup &lt;chr&gt; When I obtain data, I start by looking at the univariate distribution of each variable via a histogram. The following code creates a histogram in ggplot. ggplot(state_data, aes(x = Population)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Let’s walk through the syntax there. In the first line, we call ggplot(), specifying the data frame to draw from, then in the aes() command (which stands for “aesthetic”) we specify the variable to plot. If this were a bivariate analysis, here we would have also specified a y variable to put on the y-axis. If we had just stopped there, we would have a sad, empty plot. The + symbol indicates that we’ll be adding something to the plot. geom_histogram() is the command to overlay a histogram. We’ll only be looking at a few of the ggplot commands today. I recommend taking a look at the online package documentation at http://docs.ggplot2.org to see all of the many features available. When you’re just making graphs for yourself to explore the data, you don’t need to worry about things like axis labels as long as you can comprehend what’s going on. But when you prepare graphs for others to read (including those of us grading your problem sets!) you need to include an informative title and axis labels. To that end, use the xlab(), ylab(), and ggtitle() commands. ggplot(state_data, aes(x = Population)) + geom_histogram() + xlab(&quot;Population (thousands)&quot;) + ylab(&quot;Number of states&quot;) + ggtitle(&quot;Some states are big, but most are small&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. The density plot is a close relative of the histogram. It provides a smooth estimate of the probability density function of the data. Accordingly, the area under the density plot integrates to one. Depending on your purposes, this can make the y-axis of a density plot easier or (usually) harder to interpret than the count given by a histogram. ggplot(state_data, aes(x = Population)) + geom_density() The box plot is a common way to look at the distribution of a continuous variable across different levels of a categorical variable. ggplot(state_data, aes(x = Region, y = Population)) + geom_boxplot() A box plot consists of the following components: Center line: median of the data Bottom of box: 25th percentile Top of box: 75th percentile Lower “whisker”: range of observations no more than 1.5 IQR (height of box) below the 25th percentile Upper “whisker”: range of observations no more than 1.5 IQR above the 75th percentile Plotted points: any data lying outside the whiskers If you want to skip the summary and plot the full distribution of a variable across categories, you can use a violin plot. ggplot(state_data, aes(x = Region, y = Population)) + geom_violin() Technically, violin plots convey more information than box plots since they show the full distribution. However, readers aren’t as likely to be familiar with a violin plot. It’s harder to spot immediately where the median is (though you could add that to the plot if you wanted). Plus, violin plots look goofy with outliers—see the “West” column above—whereas box plots handle them easily. For visualizing relationships between continuous variables, nothing beats the scatterplot. ggplot(state_data, aes(x = Illiteracy, y = LifeExp)) + geom_point() When you’re plotting states or countries, a hip thing to do is plot abbreviated names instead of points. To do that, you can use geom_text() instead of geom_point(), supplying an additional aesthetic argument telling ggplot where to draw the labels from. ggplot(state_data, aes(x = Illiteracy, y = LifeExp)) + geom_text(aes(label = Abbrev)) Maybe it’s overwhelming to look at all that raw data and you just want a summary. For example, maybe you want an estimate of expected LifeExp for each value of Illiteracy. This is called the conditional expectation and will be the subject of much of the rest of the course. For now, just now that you can calculate a smoothed conditional expectation via geom_smooth(). ggplot(state_data, aes(x = Illiteracy, y = LifeExp)) + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; And if you’re the kind of overachiever who likes to have the raw data and the summary, you can do it. Just add them both to the ggplot() call. ggplot(state_data, aes(x = Illiteracy, y = LifeExp)) + geom_smooth() + geom_point() ## `geom_smooth()` using method = &#39;loess&#39; 4.2 Saving Plots When you’re writing in R Markdown, the plots go straight into your document without much fuss. Odds are, your dissertation will contain plots but won’t be written in R Markdown, which means you’ll need to learn how to save them. It’s pretty simple: Assign your ggplot() call to a variable. Pass that variable to the ggsave() function. pop_hist &lt;- ggplot(state_data, aes(x = Population)) + geom_histogram() ggsave(filename = &quot;pop-hist.pdf&quot;, plot = pop_hist, width = 6, height = 3) If you want plot types other than PDF, just set a different extension. See ?ggsave for the possibilities. 4.3 Faceting Suppose you want to split the data into subgroups, as defined by some variable in the data (e.g., the region states are in), and make the same plot for each subgroup. ggplot’s faceting functions, facet_wrap() and facet_grid(), make this easy. To split up plots according to a single grouping variable, use facet_wrap(). This uses R’s formula syntax, defined by the tilde ~, which you’ll become well acquainted with once we start running regressions. ggplot(state_data, aes(x = Population)) + geom_histogram() + facet_wrap(~ Region) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. If you don’t like the default arrangement, use the ncol argument. ggplot(state_data, aes(x = Population)) + geom_histogram() + facet_wrap(~ Region, ncol = 1) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. For two grouping variables, use facet_grid(), putting variables on both sides of the formula. ggplot(state_data, aes(x = Population)) + geom_histogram() + facet_grid(Region ~ IncomeGroup) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 4.4 Aesthetics Faceting is one way to incorporate information about additional variables into what would otherwise be a plot of just one or two variables. Aesthetics—which alter the appearance of particular plot features depending on the value of a variable—provide another way to do that. For example, when visualizing the relationship between statewide illiteracy and life expectancy, you might want larger states to get more visual weight. You can set the size aesthetic of the point geometry to vary according to the state’s population. ggplot(state_data, aes(x = Illiteracy, y = LifeExp)) + geom_point(aes(size = Population)) The ggplot2 documentation lists the available aesthetics for each function. Another popular one is colour, which is great for on-screen display but not so much for the printed page. (And terrible for the colorblind!) ggplot(state_data, aes(x = Illiteracy, y = LifeExp)) + geom_point(aes(colour = Region)) For line graphs or density plots, you can set the linetype to vary by category. ggplot(state_data, aes(x = Population)) + geom_density(aes(linetype = IncomeGroup)) (I always find these incomprehensible with more than two lines, but maybe that’s just me.) You can use multiple aesthetics together, and you can even combine aesthetics with faceting, as in the following example. ggplot(state_data, aes(x = Illiteracy, y = LifeExp)) + geom_smooth() + geom_text(aes(label = Abbrev, colour = Region, size = Population)) + facet_wrap(~ IncomeGroup) ## `geom_smooth()` using method = &#39;loess&#39; But the fact that you can do something doesn’t mean you should. That plot is so cluttered that it’s hard to extract the relevant information from it. Data visualizations should communicate a clear message to viewers without overwhelming them. To do this well takes practice, patience, and maybe even a bit of taste. 4.5 Appendix: Creating the Example Data The example data comes from data on U.S. states in 1977 that are included with base R. See ?state. library(&quot;tidyverse&quot;) state_data &lt;- state.x77 %&gt;% as_tibble() %&gt;% add_column(State = rownames(state.x77), Abbrev = state.abb, Region = state.region, .before = 1) %&gt;% rename(LifeExp = `Life Exp`, HSGrad = `HS Grad`) %&gt;% mutate(IncomeGroup = cut(Income, breaks = quantile(Income, probs = seq(0, 1, by = 1/3)), labels = c(&quot;Low&quot;, &quot;Medium&quot;, &quot;High&quot;), include.lowest = TRUE)) write_csv(state_data, path = &quot;state-data.csv&quot;) Why 1977? Because it was easily available. See the appendix to this chapter.↩ "],
["bivariate.html", "5 Bivariate Regression 5.1 Probability Refresher 5.2 The Linear Model 5.3 Least Squares 5.4 Properties 5.5 Appendix: Regression in R", " 5 Bivariate Regression The goal of empirical social science is usually to learn about the relationships between variables in the social world. Our goals might be descriptive: were college graduates more likely to vote for Clinton in 2016? Or causal: does receiving more education make a person more liberal on average? Or predictive: what kinds of voters should Democrats target in 2020 to have the best chance of victory? The linear model is one of the simplest ways to model relationships between variables. Ordinary least squares regression is one of the easiest and (often) best ways to estimate the parameters of the linear model. Consequently, a linear model estimated by OLS is the starting point for many analyses. We will start with the simplest case: regression on a single covariate. 5.1 Probability Refresher Let \\(Y\\) be a random variable that takes values in the finite set \\(\\mathcal{Y}\\) according to the probability mass function \\(f_Y : \\mathcal{Y} \\to [0, 1]\\). The expected value (aka expectation) of \\(Y\\) is the weighted average of each value in \\(\\mathcal{Y}\\), where the weights are the corresponding probabilities: \\[\\begin{equation} E[Y] = \\sum_{y \\in \\mathcal{Y}} y \\: f_Y(y); \\end{equation}\\] For a continuous random variable \\(Y\\) on \\(\\mathbb{R}\\) with probability density function \\(f_Y\\), the expected value is the analogous integral: \\[\\begin{equation} E[Y] = \\int y \\: f_Y(y) \\, dy. \\end{equation}\\] Now suppose \\((X, Y)\\) is a pair of discrete random variables drawn according to the joint mass function \\(f_{XY}\\) on \\(\\mathcal{X} \\times \\mathcal{Y}\\), with respective marginal mass functions \\(f_X\\) and \\(f_Y\\).12 Recall the formula for conditional probability, \\[\\begin{equation} \\Pr(Y = y \\,|\\, X = x) = \\frac{\\Pr(X = x, Y = y)}{\\Pr(X = x)} = \\frac{f_{XY}(x, y)}{f_X(x)}. \\end{equation}\\] For each \\(x \\in \\mathcal{X}\\), we have the conditional mass function \\[\\begin{equation} f_{Y|X}(y \\,|\\, x) = \\frac{f_{XY}(x, y)}{f_X(x)} \\end{equation}\\] and corresponding conditional expectation \\[\\begin{equation} E[Y | X = x] = \\sum_{y \\in \\mathcal{Y}} y \\: f_{Y|X}(y \\,|\\, x). \\end{equation}\\] For continuous random variables, the conditional expectation is \\[\\begin{equation} E[Y | X = x] = \\int y \\: f_{Y|X} (y \\,|\\, x) \\, dy, \\end{equation}\\] where \\(f_{Y|X}\\) is the conditional density function. The variance of a random variable \\(Y\\) is \\[\\begin{equation} V[Y] = E[(Y - E[Y])^2]. \\end{equation}\\] Given a sample \\(Y_1, \\ldots, Y_N\\) of observations of \\(Y\\), we usually estimate \\(V[Y]\\) with the sample variance \\[\\begin{equation} S_Y^2 = \\frac{1}{N-1} \\sum_n (Y_n - \\bar{Y})^2, \\end{equation}\\] where \\(\\bar{Y}\\) is the sample mean and \\(\\sum_n\\) denotes summation from \\(n = 1\\) to \\(N\\). Similarly (in fact a generalization of the above), the covariance between random variables \\(X\\) and \\(Y\\) is \\[ {\\mathop{\\rm Cov}\\nolimits}[X, Y] = E[(X - E[X]) (Y - E[Y])], \\] which we estimate with the sample covariance \\[\\begin{equation} S_{XY} = \\frac{1}{N-1} \\sum_n (X_n - \\bar{X}) (Y_n - \\bar{Y}). \\end{equation}\\] A fun fact about the sample covariance is that \\[\\begin{align} S_{XY} &amp;= \\frac{1}{N-1} \\sum_n (X_n - \\bar{X}) (Y_n - \\bar{Y}) \\\\ &amp;= \\frac{1}{N-1} \\left[ \\sum_n X_n (Y_n - \\bar{Y}) + \\sum_n \\bar{X} (Y_n - \\bar{Y}) \\right] \\\\ &amp;= \\frac{1}{N-1} \\left[ \\sum_n X_n (Y_n - \\bar{Y}) + \\bar{X} \\sum_n (Y_n - \\bar{Y}) \\right] \\\\ &amp;= \\frac{1}{N-1} \\sum_n X_n (Y_n - \\bar{Y}). \\end{align}\\] If we had split up the second term instead of the first, we would see that \\[ S_{XY} = \\frac{1}{N-1} \\sum_n Y_n (X_n - \\bar{X}) \\] as well. Since the (sample) variance is a special case of the (sample) covariance, by the same token we have \\[\\begin{equation} S_Y^2 = \\frac{1}{N-1} \\sum_n Y_n (Y_n - \\bar{Y}). \\end{equation}\\] 5.2 The Linear Model Suppose we observe a sequence of \\(N\\) draws from \\(f_{XY}\\), denoted \\((X_1, Y_1), (X_2, Y_2), \\ldots, (X_N, Y_N)\\), or \\(\\{(X_n, Y_n)\\}_{n=1}^N\\) for short. What can we learn about the relationship between \\(X\\) and \\(Y\\) from this sample of data? If we were really ambitious, we could try to estimate the shape of the full joint distribution, \\(f_{XY}\\). The joint distribution encodes everything there is to know about the relationship between the two variables, so it would be pretty useful to know. But except in the most trivial cases, it would be infeasible to estimate \\(f_{XY}\\) precisely. If \\(X\\) or \\(Y\\) can take on more than a few values, estimating the joint distribution would require an amount of data that we’re unlikely to have.13 The first way we simplify our estimation task is to set our sights lower. Let \\(Y\\) be the response or the dependent variable—i.e., the thing we want to explain. We call \\(X\\) the covariate or the independent variable. Instead of estimating the full joint distribution, we’re just going to try to learn the conditional expectation, \\(E[Y \\,|\\, X]\\). In other words, for each potential value of the covariate, what is the expected value of the response? This will allow us to answer questions like whether greater values of \\(X\\) are associated with greater values of \\(Y\\). Two important things about the estimation of conditional expectations before we go any further. Statements about conditional expectations are not causal. If \\(Y\\) is rain and \\(X\\) is umbrella sales, we know \\(E[Y | X]\\) increases with \\(X\\), but that doesn’t mean umbrella sales make it rain. We will spend some time in the latter part of the course on how to move from conditional expectations to causality. Then, in Stat III, you will learn about causal inference in excruciating detail. The conditional expectation doesn’t give you everything you’d want to know about the relationship between variables. As a hypothetical example, suppose I told you that taking a particular drug made people happier on average. In other words, \\(E[\\text{Happiness} \\,|\\, \\text{Drug}] &gt; E[\\text{Happiness} \\,|\\, \\text{No Drug}]\\). Sounds great! Then imagine the dose-response graph looked like this: The fact that expected happiness rises by half a point doesn’t quite tell the whole story. In spite of these caveats, conditional expectation is a really useful tool for summarizing the relationship between variables. If \\(X\\) takes on sufficiently few values (and we have enough data), we don’t need to model the conditional expectation function. We can just directly estimate \\(E[Y | X = x]\\) for each \\(x \\in \\mathcal{X}\\). The graph above, where there are just two values of \\(X\\), is one example. But if \\(X\\) is continuous, or even if it is discrete with many values, estimating \\(E[Y | X]\\) for each distinct value is infeasible. In this case, we need to model the relationship. The very simplest choice—and thus the default for social scientists—is to model the conditional expectation of \\(Y\\) as a linear function of \\(X\\): \\[\\begin{equation} E[Y \\,|\\, X] = \\alpha + \\beta X. \\end{equation}\\] In this formulation, \\(\\alpha\\) and \\(\\beta\\) are the parameters to be estimated from sample data. We call \\(\\alpha\\) and \\(\\beta\\) “coefficients,” with \\(\\alpha\\) the “intercept” and \\(\\beta\\) the “slope.” Regardless of how many different values \\(X\\) might take on, we only need to estimate two parameters of the linear model. Exercise your judgment before using a linear model. Ask yourself, is a linear conditional expectation function at least minimally plausible? Not perfect—just a reasonable approximation. If \\(X\\) is years of education and \\(Y\\) is annual income, the answer is probably yes (depending on the population!). But if \\(X\\) is hour of the day (0–24) and \\(Y\\) is the amount of traffic on I-65, probably not. To obtain the linear conditional expectation, we usually assume the following model of the response variable: \\[\\begin{equation} Y_n = \\alpha + \\beta X_n + \\epsilon_n, \\end{equation}\\] where \\(\\epsilon_n\\) is “white noise” error with the property \\[\\begin{equation} E[\\epsilon_n \\,|\\, X_1, \\ldots X_N] = 0. \\end{equation}\\] You can think of \\(\\epsilon_n\\) as the summation of everything besides the covariate \\(X_n\\) that affects the response \\(Y_n\\). The assumption that \\(E[\\epsilon_n \\,|\\, X_1, \\ldots, X_N] = 0\\) implies that these external factors are uncorrelated with the covariate. This is not a trivial technical condition that you can ignore—it is a substantive statement about the variables in your model. It requires justification, and it is difficult to justify. For now we will proceed assuming that our data satisfy the above conditions. Later in the course, we will talk about how to proceed when \\(E[\\epsilon_n \\,|\\, X_1, \\ldots, X_N] \\neq 0\\), and you will learn much more about such strategies in Stat III. 5.3 Least Squares To estimate the parameters of the linear model, we will rely on a mathematically convenient method called least squares. We will see that this method not only is convenient, but also has nice statistical properties. Given a parameter estimate \\((\\hat{\\alpha}, \\hat{\\beta})\\), define the residual of the \\(n\\)’th observation as the difference between the true and predicted values: \\[\\begin{equation} e_n(\\hat{\\alpha}, \\hat{\\beta}) = Y_n - \\hat{\\alpha} - \\hat{\\beta} X_n. \\end{equation}\\] The residual is directional. The residual is positive when the regression line falls below the observation, and vice versa when it is negative. We would like the regression line to lie close to the data—i.e., for the residuals to be small in magnitude. “Close” can mean many things, so we need to be a bit more specific to derive an estimator. The usual one, ordinary least squares, is chosen to minimize the sum of squared errors, \\[ {\\mathop{\\rm SSE}\\nolimits}(\\hat{\\alpha}, \\hat{\\beta}) = \\sum_n e_n(\\hat{\\alpha}, \\hat{\\beta})^2. \\] (Throughout the rest of this chapter, I write \\(\\sum_n\\) as shorthand for \\(\\sum_{n=1}^N\\).) When we focus on squared error, we penalize a positive residual the same as a negative residual of the same size. Moreover, we penalize one big residual proportionally more than a few small ones. It is important to keep the linear model and ordinary least squares distinct in your mind. The linear model is a model of the data. Ordinary least squares is one estimator—one among many—of the parameters of the linear model. Assuming a linear model does not commit you to estimate it with OLS if you think another estimator is more appropriate. And using OLS does not necessarily commit you to the linear model, as we will discuss when we get to multiple regression. To derive the OLS estimator, we will derive the conditions for minimization of the sum of squared errors. The SSE is a quadratic and therefore continuously differentiable function of the estimands, \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\). You will remember from calculus that, at any extreme point of a continuous function, all its partial derivatives equal zero. To derive necessary conditions for minimization,14 we can take the derivatives of the SSE and set them to equal zero. The derivative with respect to the intercept is \\[ {\\frac{\\partial {\\mathop{\\rm SSE}\\nolimits}(\\hat{\\alpha}, \\hat{\\beta})}{\\partial \\hat{\\alpha}}} = -2 \\sum_n (Y_n - \\hat{\\alpha} - \\hat{\\beta} X_n). \\] Setting this to equal zero gives \\[ \\hat{\\alpha} = \\frac{1}{N} \\sum_n (Y_n - \\hat{\\beta} X_n) = \\bar{Y} - \\hat{\\beta} \\bar{X}. \\] This gives us one important property of OLS: the regression line estimated by OLS always passes through \\((\\bar{X}, \\bar{Y})\\). The derivative with respect to the slope is \\[ {\\frac{\\partial {\\mathop{\\rm SSE}\\nolimits}(\\hat{\\alpha}, \\hat{\\beta})}{\\partial \\hat{\\beta}}} = -2 \\sum_n X_n (Y_n - \\hat{\\alpha} - \\hat{\\beta} X_n). \\] Setting this equal to zero and substituting in the expression for \\(\\hat{\\alpha}\\) we derived above gives \\[ \\sum_n X_n (Y_n - \\bar{Y}) = \\hat{\\beta} \\sum_n X_n (X_n - \\bar{X}). \\] As long as the sample variance of \\(X\\) is non-zero (i.e., \\(X\\) is not a constant), we can divide to solve for \\(\\hat{\\beta}\\): \\[ \\hat{\\beta} = \\frac{\\sum_n X_n (Y_n - \\bar{Y})}{\\sum_n X_n (X_n - \\bar{X})} = \\frac{S_{XY}}{S_X^2}. \\] Combining these two results, we have the OLS estimators of the intercept and slope of the bivariate linear model. We write them as functions of \\((X_1, \\ldots, X_N, Y_1, \\ldots, Y_N)\\), or \\((X, Y)\\) for short,15 to emphasize that an estimator is a statistic, which in turn is a function of sample data. We place the “OLS” subscript on them to emphasize that there are many estimators of these parameters, of which OLS is just one (good!) choice. \\[ \\begin{aligned} \\hat{\\alpha}_{\\text{OLS}}(X, Y) &amp;= \\bar{Y} - \\frac{S_{XY}}{S_X^2} \\bar{X}, \\\\ \\hat{\\beta}_{\\text{OLS}}(X, Y) &amp;= \\frac{S_{XY}}{S_X^2}. \\\\ \\end{aligned} \\] Regression is a convenient way to summarize the relationship between variables, but it is a complement to—not a substitute for—graphical analysis. The statistician Francis Anscombe found that OLS yields nearly identical regression lines for all four of the datasets in the following graph: Unless your data all lie along a line, the regression line estimated by OLS will not predict the data perfectly. Let the residual sum of squares be the squared error left over by OLS, \\[ {\\mathop{\\rm RSS}\\nolimits}= {\\mathop{\\rm SSE}\\nolimits}(\\hat{\\alpha}_{\\text{OLS}}, \\hat{\\beta}_{\\text{OLS}}), \\] and let the total sum of squares be the squared error that would result from a horizontal regression line through the mean of \\(Y\\), \\[ {\\mathop{\\rm TSS}\\nolimits}= {\\mathop{\\rm SSE}\\nolimits}(\\bar{Y}, 0). \\] The \\(R^2\\) statistic is the proportion of “variance explained” by \\(X\\), calculated as \\[ R^2 = 1 - \\frac{{\\mathop{\\rm RSS}\\nolimits}}{{\\mathop{\\rm TSS}\\nolimits}}. \\] If the regression line is flat, in which case \\(\\hat{\\beta}_{\\text{OLS}} = 0\\) and \\({\\mathop{\\rm RSS}\\nolimits}= {\\mathop{\\rm TSS}\\nolimits}\\), we have \\(R^2= 0\\). Conversely, if the regression line fits perfectly, in which case \\({\\mathop{\\rm RSS}\\nolimits}= 0\\), we have \\(R^2 = 1\\). A statistic that is often more useful than \\(R^2\\) is the residual variance. The residual variance is (almost) the sample variance of the regression residuals, calculated as \\[ \\hat{\\sigma}^2 = \\frac{1}{N - 2} \\sum_n e_n(\\hat{\\alpha}_{\\text{OLS}}, \\hat{\\beta}_{\\text{OLS}})^2 = \\frac{{\\mathop{\\rm RSS}\\nolimits}}{N - 2} \\] Since bivariate regression uses two degrees of freedom (one for the intercept, one for the slope), we divide by \\(N - 2\\) instead of the usual \\(N - 1\\). The most useful quantity is \\(\\hat{\\sigma}\\), the square root of the residual variance. \\(\\hat{\\sigma}\\) is measured in the same units as \\(Y\\), and it is a measure of the spread of points around the regression line. If the residuals are roughly normally distributed, then we would expect roughly 95% of the data to lie within \\(\\pm 2 \\hat{\\sigma}\\) of the regression line. 5.4 Properties We didn’t use any fancy statistical theory to derive the OLS estimator. We just found the intercept and slope that minimize the sum of squared residuals. As it turns out, though, OLS indeed has some very nice statistical properties as an estimator of the linear model. The first desirable property of OLS is that it is unbiased. Recall that an estimator \\(\\hat{\\theta}\\) of the parameter \\(\\theta\\) is unbiased if \\(E[\\hat{\\theta}] = \\theta\\). This doesn’t mean the estimator always gives us the right answer, just that on average it is not systematically biased upward or downward. In other words, if we could take many many samples and apply the estimator to each of them, the average would equal the true parameter. We will begin by showing that the OLS estimator of the slope is unbiased; i.e., that \\(E[\\hat{\\beta}_{\\text{OLS}}(X, Y)] = \\beta\\). At first, we’ll take the conditional expectation of the slope estimator, treating the covariates \\((X_1, \\ldots, X_N)\\) as fixed. \\[ \\begin{aligned} E[\\hat{\\beta}_{\\text{OLS}}(X, Y) \\,|\\, X] &amp;= E \\left[ \\left. \\frac{S_{XY}}{S_X^2} \\,\\right|\\, X \\right] \\\\ &amp;= E \\left[ \\left. \\frac{\\sum_n Y_n (X_n - \\bar{X})}{\\sum_n X_n (X_n - \\bar{X})} \\,\\right|\\, X \\right] \\\\ &amp;= \\frac{\\sum_n E[Y_n \\,|\\, X] (X_n - \\bar{X})}{\\sum_n X_n (X_n - \\bar{X})} \\\\ &amp;= \\frac{\\sum_n (\\alpha + \\beta X_n) (X_n - \\bar{X})}{\\sum_n X_n (X_n - \\bar{X})} \\\\ &amp;= \\frac{\\alpha \\sum_n (X_n - \\bar{X}) + \\beta \\sum_n X_n (X_n - \\bar{X})}{\\sum_n X_n (X_n - \\bar{X})} \\\\ &amp;= \\frac{\\beta \\sum_n X_n (X_n - \\bar{X})}{\\sum_n X_n (X_n - \\bar{X})} \\\\ &amp;= \\beta. \\end{aligned} \\] It then follows from the law of iterated expectation16 that \\[ E[\\hat{\\beta}_{\\text{OLS}}(X, Y)] = \\beta. \\] Then, for the intercept, we have \\[ \\begin{aligned} E[\\hat{\\alpha}_{\\text{OLS}}(X, Y) \\,|\\, X] &amp;= E [\\bar{Y} - \\hat{\\beta}_{\\text{OLS}}(X, Y) \\bar{X} \\,|\\, X] \\\\ &amp;= E [\\bar{Y} \\,|\\, X] - E[\\hat{\\beta}_{\\text{OLS}}(X, Y) \\,|\\, X] \\bar{X} \\\\ &amp;= E \\left[ \\left. \\frac{1}{N} \\sum_n Y_n \\,\\right|\\, X \\right] - \\beta \\bar{X} \\\\ &amp;= E \\left[ \\left. \\frac{1}{N} \\sum_n (\\alpha + \\beta X_n + \\epsilon_n) \\,\\right|\\, X \\right] - \\beta \\bar{X} \\\\ &amp;= \\frac{1}{N} \\sum_n E[\\alpha + \\beta X_n + \\epsilon_n \\,|\\, X] - \\beta \\bar{X} \\\\ &amp;= \\frac{1}{N} \\sum_n \\alpha + \\frac{\\beta}{N} \\sum_n X_n + \\frac{1}{N} \\sum_n E[\\epsilon_n \\,|\\, X] - \\beta \\bar{X} \\\\ &amp;= \\alpha + \\beta \\bar{X} - \\beta \\bar{X} \\\\ &amp;= \\alpha. \\end{aligned} \\] As with the slope, this conditional expectation gives us the unconditional expectation we want: \\[ E[\\hat{\\alpha}_{\\text{OLS}}(X, Y)] = \\alpha. \\] To sum up: as long as the crucial condition \\(E[\\epsilon_n \\,|\\, X_1, \\ldots, X_N] = 0\\) holds, then OLS is an unbiased estimator of the parameters of the linear model. Another important property of OLS is that it is consistent. Informally, this means that in sufficiently large samples, the OLS estimates \\((\\hat{\\alpha}_{\\text{OLS}}, \\hat{\\beta}_{\\text{OLS}})\\) are very likely to be close to the true parameter values \\((\\alpha, \\beta)\\). Another way to think of consistency is that, as \\(N \\to \\infty\\), the bias and variance of the OLS estimator both go to zero.17 Of course the bias “goes to” zero, since OLS is unbiased. The real trick to proving consistency is to show that the variance goes to zero. If you wanted to do that for the slope estimate, you’d derive an expression for \\[ V[\\hat{\\beta}_{\\text{OLS}}] = E[(\\hat{\\beta}_{\\text{OLS}} - E[\\hat{\\beta}_{\\text{OLS}}])^2] = E[(\\hat{\\beta}_{\\text{OLS}} - \\beta)^2] \\] and show that \\[ \\lim_{N \\to \\infty} V[\\hat{\\beta}_{\\text{OLS}}] = 0. \\] This takes more algebra than we have time for, so I leave it as an exercise for the reader. 5.5 Appendix: Regression in R We will be using the tidyverse package as always, the car package for the Prestige data, and the broom package for its convenient post-analysis functions. library(&quot;tidyverse&quot;) library(&quot;car&quot;) library(&quot;broom&quot;) Let’s take a look at Prestige, which records basic information (including perceived prestige) for a variety of occupations. head(Prestige) ## education income women prestige census type ## gov.administrators 13.11 12351 11.16 68.8 1113 prof ## general.managers 12.26 25879 4.02 69.1 1130 prof ## accountants 12.77 9271 15.70 63.4 1171 prof ## purchasing.officers 11.42 8865 9.11 56.8 1175 prof ## chemists 14.62 8403 11.68 73.5 2111 prof ## physicists 15.64 11030 5.13 77.6 2113 prof Suppose we want to run a regression of prestige on education. We will use the lm() function, which stands for linear model. This will employ the “formula” syntax that you previously saw when faceting in ggplot. The basic syntax of a formula is response ~ covariate, where response and covariate are the names of the variables in question. In this case, with prestige (note that the variable is lowercase, while the dataset is capitalized) as the response and education as the covariate: lm(prestige ~ education, data = Prestige) ## ## Call: ## lm(formula = prestige ~ education, data = Prestige) ## ## Coefficients: ## (Intercept) education ## -10.73 5.36 You’ll notice that didn’t give us very much. If you’ve previously used statistical programs like Stata, you might expect a ton of output at this point. It’s all there in R too, but R has a different philosophy about models. R sees the fitted model as an object in its own right—like a data frame, a function, or anything else you load or create in R. Therefore, to analyze regression results in R, you will typically save the regression results to a variable. Like any other variable, you’ll want to give your regression results meaningful names. I typically call them fit_ to indicate a fitted model, followed by some memorable description. fit_educ &lt;- lm(prestige ~ education, data = Prestige) When you do this, the output doesn’t get printed. To see the default output, just run the variable name, just like you would to see the content of a data frame: fit_educ ## ## Call: ## lm(formula = prestige ~ education, data = Prestige) ## ## Coefficients: ## (Intercept) education ## -10.73 5.36 For a more detailed readout, use the summary() method: summary(fit_educ) ## ## Call: ## lm(formula = prestige ~ education, data = Prestige) ## ## Residuals: ## Min 1Q Median 3Q Max ## -26.040 -6.523 0.661 6.743 18.164 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -10.732 3.677 -2.92 0.0043 ## education 5.361 0.332 16.15 &lt;2e-16 ## ## Residual standard error: 9.1 on 100 degrees of freedom ## Multiple R-squared: 0.723, Adjusted R-squared: 0.72 ## F-statistic: 261 on 1 and 100 DF, p-value: &lt;2e-16 This prints out a whole boatload of information, including inferential statistics that we’re going to wait until later in the course to discuss how to interpret: The model you ran Basic statistics about the distribution of the residuals For each coefficient: Parameter estimate Standard error estimate Test statistic for a hypothesis test of equality with zero \\(p\\)-value associated with the test statistic \\(\\hat{\\sigma}\\) (called the “residual standard error”, a term seemingly unique to R) \\(R^2\\) and an “adjusted” variant that accounts for the number of variables in the model \\(F\\) statistic, degrees of freedom, and associated \\(p\\)-value for a hypothesis test that every coefficient besides the intercept equals zero Strangely, summary() doesn’t give you the sample size. For that you must use nobs(): nobs(fit_educ) ## [1] 102 You can use a fitted model object to make predictions for new data. For example, let’s make a basic data frame of education levels. my_data &lt;- data_frame(education = 8:16) my_data ## # A tibble: 9 × 1 ## education ## &lt;int&gt; ## 1 8 ## 2 9 ## 3 10 ## 4 11 ## 5 12 ## 6 13 ## 7 14 ## 8 15 ## 9 16 To calculate the predicted level of prestige for each education level, use predict(): predict(fit_educ, newdata = my_data) ## 1 2 3 4 5 6 7 8 9 ## 32.155 37.516 42.877 48.238 53.599 58.959 64.320 69.681 75.042 When using predict(), it is crucial that the newdata have the same column names as in the data used to fit the model. You can also extract a confidence interval for each prediction: predict(fit_educ, newdata = my_data, interval = &quot;confidence&quot;, level = 0.95) ## fit lwr upr ## 1 32.155 29.615 34.695 ## 2 37.516 35.393 39.639 ## 3 42.877 41.024 44.730 ## 4 48.238 46.441 50.034 ## 5 53.599 51.627 55.571 ## 6 58.959 56.632 61.287 ## 7 64.320 61.525 67.116 ## 8 69.681 66.353 73.010 ## 9 75.042 71.142 78.942 One of the problems with summary() and predict() is that they return inconveniently shaped output. The output of summary() is particularly hard to deal with. The broom package provides three utilities to help get model output into shape. The first is tidy(), which makes a tidy data frame out of the regression coefficients and the associated inferential statistics: tidy(fit_educ) ## term estimate std.error statistic p.value ## 1 (Intercept) -10.7320 3.67709 -2.9186 4.3434e-03 ## 2 education 5.3609 0.33199 16.1478 1.2863e-29 The second is glance(), which provides a one-row data frame containing overall model characteristics (e.g., \\(R^2\\) and \\(\\hat{\\sigma}\\)): glance(fit_educ) ## r.squared adj.r.squared sigma statistic p.value df logLik AIC ## 1 0.7228 0.72003 9.1033 260.75 1.2863e-29 2 -369 744.01 ## BIC deviance df.residual ## 1 751.88 8287 100 The third is augment(), which “augments” the original data—or new data you supply, as in predict()—with information from the model, such as predicted values. # Lots of output, so only printing first 10 rows head(augment(fit_educ), 10) ## .rownames prestige education .fitted .se.fit .resid .hat ## 1 gov.administrators 68.8 13.11 59.549 1.19689 9.2509 0.017287 ## 2 general.managers 69.1 12.26 54.992 1.03332 14.1076 0.012885 ## 3 accountants 63.4 12.77 57.726 1.12584 5.6736 0.015295 ## 4 purchasing.officers 56.8 11.42 50.489 0.92936 6.3108 0.010422 ## 5 chemists 73.5 14.62 67.644 1.57269 5.8559 0.029846 ## 6 physicists 77.6 15.64 73.112 1.86034 4.4879 0.041763 ## 7 biologists 72.6 15.09 70.164 1.70291 2.4363 0.034993 ## 8 architects 78.1 15.44 72.040 1.80254 6.0600 0.039208 ## 9 civil.engineers 73.1 14.52 67.108 1.54561 5.9920 0.028827 ## 10 mining.engineers 68.8 14.64 67.751 1.57814 1.0487 0.030053 ## .sigma .cooksd .std.resid ## 1 9.1010 0.00924267 1.02511 ## 2 9.0372 0.01587881 1.55981 ## 3 9.1311 0.00306360 0.62807 ## 4 9.1269 0.00255745 0.69688 ## 5 9.1296 0.00656112 0.65310 ## 6 9.1375 0.00552702 0.50362 ## 7 9.1458 0.00134578 0.27244 ## 8 9.1280 0.00941104 0.67914 ## 9 9.1287 0.00662109 0.66793 ## 10 9.1485 0.00021198 0.11697 augment(fit_educ, newdata = my_data) ## education .fitted .se.fit ## 1 8 32.155 1.28013 ## 2 9 37.516 1.07023 ## 3 10 42.877 0.93407 ## 4 11 48.238 0.90555 ## 5 12 53.599 0.99397 ## 6 13 58.959 1.17319 ## 7 14 64.320 1.40897 ## 8 15 69.681 1.67763 ## 9 16 75.042 1.96574 Notice that you get back more information for the data used to fit the model than for newly supplied data. The most important is .fitted, the predicted value. See ?augment.lm for what all the various output represents. One last note on plotting regression lines with ggplot. Use geom_smooth(method = &quot;lm&quot;). ggplot(Prestige, aes(x = education, y = prestige)) + geom_point() + geom_smooth(method = &quot;lm&quot;) To get rid of the confidence interval: ggplot(Prestige, aes(x = education, y = prestige)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) The marginal mass function, if you don’t recall, is \\(f_X(x) = \\sum_{y \\in \\mathcal{Y}} f_{XY} (x, y)\\). In the continuous case, the marginal density function is \\(f_X(x) = \\int f_{XY} (x, y) \\, dy\\).↩ This problem only gets worse as we move from bivariate into multivariate analysis, a phenomenon called the curse of dimensionality.↩ In fact, since the SSE function is strictly convex, these conditions are sufficient for global minimization.↩ This is a bit of an abuse of notation, since previously I used \\(X\\) and \\(Y\\) to refer to the random variables and now I’m using them to refer to vectors of sample data. Sorry.↩ For random variables \\(A\\) and \\(B\\), \\(E[f(A, B)] = E_A[ E_B[f(A, B) \\,|\\, A] ] = E_B [ E_A[f(A, B) \\,|\\, B] ]\\).↩ What I am describing here is mean square consistency, which is stronger than the broadest definitions of consistency in statistical theory.↩ "],
["matrix.html", "6 Matrix Algebra: A Crash Course 6.1 Vector Operations 6.2 Matrix Operations 6.3 Matrix Inversion 6.4 Solving Linear Systems 6.5 Appendix: Matrices in R", " 6 Matrix Algebra: A Crash Course Some material in this chapter is adapted from notes Hye Young You wrote for the math boot camp for the political science PhD program at Vanderbilt. Matrix algebra is an essential tool for understanding multivariate statistics. You are probably already familiar with matrices, at least informally. The data representations we have worked with so far—each row an observation, each column a variable—are formatted like matrices. An introductory treatment of matrix algebra is a semester-long college course. We don’t have that long, or even half that long. This chapter gives you the bare minimum you need to understand to get up and running with the matrix algebra we need for OLS with multiple covariates. If you want to use advanced statistical methods in your research and haven’t previously taken a matrix algebra or linear algebra course, I recommend taking some time this summer to catch up. For example, MIT has its undergraduate linear algebra course available online, including video lectures. 6.1 Vector Operations A vector is an ordered array. To denote a vector \\(v\\) of \\(k\\) elements, we write \\(\\mathbf{v} = (v_1, v_2, \\ldots, v_k)\\), or sometimes \\[ \\mathbf{v} = \\begin{pmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_k \\end{pmatrix}. \\] Notice the convention of using a lowercase bold letter to denote a vector. We will usually be dealing with vectors of real numbers. To denote the fact that \\(\\mathbf{v}\\) is a vector of \\(k\\) real numbers, we write \\(\\mathbf{v} \\in \\mathbb{R}^k\\). A vector can be multiplied by a scalar \\(c \\in \\mathbb{R}\\), producing what you would expect: \\[ c \\mathbf{v} = \\begin{pmatrix} c v_1 \\\\ c v_2 \\\\ \\vdots \\\\ c v_k \\end{pmatrix} \\] You can also add and subtract two vectors of the same length.18 \\[ \\begin{aligned} \\mathbf{u} + \\mathbf{v} &amp;= \\begin{pmatrix} u_1 + v_1 \\\\ u_2 + v_2 \\\\ \\vdots \\\\ u_k + v_k \\end{pmatrix}, \\\\ \\mathbf{u} - \\mathbf{v} &amp;= \\begin{pmatrix} u_1 - v_1 \\\\ u_2 - v_2 \\\\ \\vdots \\\\ u_k - v_k \\end{pmatrix}. \\end{aligned} \\] A special vector is the zero vector, which contains—you guessed it—all zeroes. We write \\(\\mathbf{0}_k\\) to denote the zero vector of length \\(k\\). When the length of the zero vector is clear from the context, we may just write \\(\\mathbf{0}\\). The last important vector operation is the dot product. The dot product of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\), written \\(\\mathbf{u} \\cdot \\mathbf{v}\\), is the sum of the products of the entries: \\[ \\mathbf{u} \\cdot \\mathbf{v} = u_1 v_1 + u_2 v_2 + \\cdots + u_k v_k = \\sum_{m=1}^k u_m v_m. \\] An important concept for regression analysis is the linear independence of a collection of vectors. Let \\(\\mathbf{v}_1, \\ldots, \\mathbf{v}_J\\) be a collection of \\(J\\) vectors, each of length \\(k\\). We call \\(\\mathbf{u}\\) a linear combination of \\(\\mathbf{v}_1, \\ldots, \\mathbf{v}_J\\) if there exist real numbers \\(c_1, \\ldots, c_J\\) such that \\[ \\mathbf{u} = c_1 \\mathbf{v}_1 + \\cdots + c_J \\mathbf{v}_J = \\sum_{j=1}^J c_j \\mathbf{v}_j. \\] A collection of vectors is linearly independent if the only solution to \\[ c_1 \\mathbf{v}_1 + \\cdots + c_J \\mathbf{v}_J = \\mathbf{0} \\] is \\(c_1 = 0, \\ldots, c_J = 0\\). Otherwise, we call the vectors linearly dependent. Some fun facts about linear independence: If any vector in \\(\\mathbf{v}_1, \\ldots, \\mathbf{v}_J\\) is a linear combination of the others, then these vectors are linearly dependent. A collection of \\(J\\) vectors of length \\(k\\) cannot be linearly independent if \\(J &gt; k\\). In other words, given vectors of length \\(k\\), the most that can be linearly independent of each other is \\(k\\). If any \\(\\mathbf{v}_j = \\mathbf{0}\\), then \\(\\mathbf{v}_1, \\ldots, \\mathbf{v}_J\\) are linearly dependent. (Why?) Examples: \\[ \\begin{gathered} \\mathbf{v}_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\mathbf{v}_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\mathbf{v}_3 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}; \\\\ \\mathbf{v}_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\mathbf{v}_2 = \\begin{pmatrix} 14 \\\\ 12 \\\\ 0 \\end{pmatrix}, \\mathbf{v}_3 = \\begin{pmatrix} 0 \\\\ -1 \\\\ 0 \\end{pmatrix}; \\\\ \\mathbf{v}_1 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}, \\mathbf{v}_2 = \\begin{pmatrix} 1 \\\\ 4 \\\\ 9 \\end{pmatrix}, \\mathbf{v}_3 = \\begin{pmatrix} 1 \\\\ 8 \\\\ 27 \\end{pmatrix}. \\end{gathered} \\] 6.2 Matrix Operations A matrix is a two-dimensional array of numbers, with entries in rows and columns. We call a matrix with \\(n\\) rows and \\(m\\) columns an \\(n \\times m\\) matrix. For example, the following is a \\(2 \\times 3\\) matrix: \\[ \\mathbf{A} = \\begin{bmatrix} 99 &amp; 73 &amp; 2 \\\\ 13 &amp; 40 &amp; 41 \\end{bmatrix} \\] Notice the convention of using an uppercase bold letter to denote a matrix. Given a matrix \\(\\mathbf{A}\\), we usually write \\(a_{ij}\\) to denote the entry in the \\(i\\)’th row and \\(j\\)’th column. In the above example, we have \\(a_{13} = 2\\). You can think of a vector \\(\\mathbf{v} \\in \\mathbb{R}^k\\) as a \\(1 \\times k\\) row matrix or as a \\(k \\times 1\\) column matrix. Throughout this book, I will treat vectors as column matrices unless otherwise noted. Like vectors, matrices can be multipled by a scalar \\(c \\in \\mathbb{R}\\). \\[ c \\mathbf{A} = \\begin{bmatrix} c a_{11} &amp; c a_{12} &amp; \\cdots &amp; c a_{1m} \\\\ c a_{21} &amp; c a_{22} &amp; \\cdots &amp; c a_{2m} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ c a_{n1} &amp; c a_{n2} &amp; \\cdots &amp; c a_{nm} \\end{bmatrix} \\] Matrices of the same dimension (i.e., both with the same number of rows \\(n\\) and columns \\(m\\)) can be added … \\[ \\mathbf{A} + \\mathbf{B} = \\begin{bmatrix} a_{11} + b_{11} &amp; a_{12} + b_{12} &amp; \\cdots &amp; a_{1m} + b_{1m} \\\\ a_{21} + b_{21} &amp; a_{22} + b_{22} &amp; \\cdots &amp; a_{2m} + b_{2m} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} + b_{n1} &amp; a_{n2} + b_{n2} &amp; \\cdots &amp; a_{nm} + b_{nm} \\\\ \\end{bmatrix} \\] … and subtracted … \\[ \\mathbf{A} - \\mathbf{B} = \\begin{bmatrix} a_{11} - b_{11} &amp; a_{12} - b_{12} &amp; \\cdots &amp; a_{1m} - b_{1m} \\\\ a_{21} - b_{21} &amp; a_{22} - b_{22} &amp; \\cdots &amp; a_{2m} - b_{2m} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} - b_{n1} &amp; a_{n2} - b_{n2} &amp; \\cdots &amp; a_{nm} - b_{nm} \\\\ \\end{bmatrix} \\] Sometimes you will want to “rotate” an \\(n \\times m\\) matrix into an \\(m \\times n\\) one, so that the first row becomes the first column, the second row becomes the second column, and so on. This is called the transpose. I write the transpose of \\(\\mathbf{A}\\) as \\(\\mathbf{A}^\\top\\), though you will often also see it written \\(\\mathbf{A}&#39;\\). For example: \\[ \\mathbf{A} = \\begin{bmatrix} 99 &amp; 73 &amp; 2 \\\\ 13 &amp; 40 &amp; 41 \\end{bmatrix} \\qquad \\Leftrightarrow \\qquad \\mathbf{A}^\\top = \\begin{bmatrix} 99 &amp; 13 \\\\ 73 &amp; 40 \\\\ 2 &amp; 41 \\end{bmatrix} \\] Some of the most commonly invoked properties of the transpose are: \\[ \\begin{aligned} (\\mathbf{A}^\\top)^\\top &amp;= \\mathbf{A}, \\\\ (c \\mathbf{A})^\\top &amp;= c \\mathbf{A}^\\top, \\\\ (\\mathbf{A} + \\mathbf{B})^\\top &amp;= \\mathbf{A}^\\top + \\mathbf{B}^\\top, \\\\ (\\mathbf{A} - \\mathbf{B})^\\top &amp;= \\mathbf{A}^\\top - \\mathbf{B}^\\top. \\end{aligned} \\] A matrix is square if it has the same number of rows as columns, i.e., it is \\(n \\times n\\). Every matrix is special, but some kinds of square matrix are especially special. A symmetric matrix is equal to its transpose: \\(\\mathbf{A} = \\mathbf{A}^\\top\\). Example: \\[ \\begin{bmatrix} 1 &amp; 10 &amp; 100 \\\\ 10 &amp; 2 &amp; 0.1 \\\\ 100 &amp; 0.1 &amp; 3 \\end{bmatrix}\\] A diagonal matrix contains zeroes everywhere except along the main diagonal: if \\(i \\neq j\\), then \\(a_{ij} = 0\\). A diagonal matrix is symmetric by definition. Example: \\[ \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 3 \\end{bmatrix}\\] The \\(n \\times n\\) identity matrix, written \\(\\mathbf{I}_n\\) (or just \\(\\mathbf{I}\\) when the size is clear from context), is the \\(n \\times n\\) diagonal matrix where each diagonal entry is 1. Example: \\[ \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\\] And last we come to matrix multiplication. Whereas matrix addition and subtraction are pretty intuitive, matrix multiplication is not. Let \\(\\mathbf{A}\\) be an \\(n \\times m\\) matrix and \\(\\mathbf{B}\\) be an \\(m \\times p\\) matrix. (Notice that the number of columns of \\(\\mathbf{A}\\) must match the number of rows of \\(\\mathbf{B}\\).) Then \\(\\mathbf{C} = \\mathbf{A} \\mathbf{B}\\) is an \\(n \\times p\\) matrix whose \\(ij\\)’th element is the dot product of the \\(i\\)’th row of \\(\\mathbf{A}\\) and the \\(j\\)’th column of \\(\\mathbf{B}\\): \\[ c_{ij} = a_{i1} b_{1j} + a_{i2} b_{2j} + \\cdots + a_{im} b_{mj}. \\] Some examples might make this more clear. \\[ \\begin{gathered} \\mathbf{A} = \\begin{bmatrix} 2 &amp; 10 \\\\ 0 &amp; 1 \\\\ -1 &amp; 5 \\end{bmatrix}, \\mathbf{B} = \\begin{bmatrix} 1 &amp; 4 \\\\ -1 &amp; 10 \\end{bmatrix} \\\\ \\mathbf{A} \\mathbf{B} = \\begin{bmatrix} 2 \\cdot 1 + 10 \\cdot (-1) &amp; 2 \\cdot 4 + 10 \\cdot 10 \\\\ 0 \\cdot 1 + 1 \\cdot (-1) &amp; 0 \\cdot 4 + 1 \\cdot 10 \\\\ (-1) \\cdot 1 + 5 \\cdot (-1) &amp; (-1) \\cdot 4 + 5 \\cdot 10 \\end{bmatrix} = \\begin{bmatrix} -8 &amp; 108 \\\\ -1 &amp; 10 \\\\ -6 &amp; 46 \\end{bmatrix} \\end{gathered} \\] And here’s one that you’ll start seeing a lot of soon. \\[ \\begin{gathered} \\mathbf{A} = \\begin{bmatrix} 1 &amp; x_{11} &amp; x_{12} \\\\ 1 &amp; x_{21} &amp; x_{22} \\\\ &amp; \\vdots \\\\ 1 &amp; x_{N1} &amp; x_{N2} \\end{bmatrix}, \\mathbf{B} = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\end{bmatrix} \\\\ \\mathbf{A} \\mathbf{B} = \\begin{bmatrix} \\beta_0 + \\beta_1 x_{11} + \\beta_2 x_{12} \\\\ \\beta_0 + \\beta_1 x_{21} + \\beta_2 x_{22} \\\\ \\vdots \\\\ \\beta_0 + \\beta_1 x_{N1} + \\beta_2 x_{N2} \\end{bmatrix} \\end{gathered} \\] Some important properties of matrix multiplication: Matrix multiplication is associative: \\((\\mathbf{A} \\mathbf{B}) \\mathbf{C} = \\mathbf{A} (\\mathbf{B} \\mathbf{C})\\). Matrix multiplication is distributive: \\(\\mathbf{A} (\\mathbf{B} + \\mathbf{C}) = \\mathbf{A} \\mathbf{B} + \\mathbf{A} \\mathbf{C}\\). For any \\(n \\times m\\) matrix \\(\\mathbf{A}\\), we have \\(\\mathbf{A} \\mathbf{I}_m = \\mathbf{I}_n \\mathbf{A} = \\mathbf{A}\\). In this way, the identity matrix is kind of like the matrix equivalent of the number one. (More on this when we get to matrix inversion.) Matrix multiplication is not commutative. In other words, \\(\\mathbf{A} \\mathbf{B} \\neq \\mathbf{B} \\mathbf{A}\\) except in very special cases (e.g., one of them is the identity matrix). This is obvious when we’re dealing with non-square matrices. Let \\(\\mathbf{A}\\) be \\(n \\times m\\) and \\(\\mathbf{B}\\) be \\(m \\times p\\), so that \\(\\mathbf{A} \\mathbf{B}\\) exists. Then \\(\\mathbf{B} \\mathbf{A}\\) doesn’t even exist unless \\(n = p\\). Even then, if \\(n \\neq m\\), then \\(\\mathbf{A} \\mathbf{B}\\) is \\(n \\times n\\) and \\(\\mathbf{B} \\mathbf{A}\\) is \\(m \\times m\\), so they can’t possibly be the same. For an example that \\(\\mathbf{A} \\mathbf{B} \\neq \\mathbf{B} \\mathbf{A}\\) even for square matrices: \\[\\begin{gathered} \\mathbf{A} = \\begin{bmatrix} 1 &amp; 0 \\\\ 2 &amp; 0 \\end{bmatrix}, \\mathbf{B} = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix}, \\\\ \\mathbf{A} \\mathbf{B} = \\begin{bmatrix} 1 &amp; 0 \\\\ 2 &amp; 0 \\end{bmatrix}, \\mathbf{B} \\mathbf{A} = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix}. \\end{gathered} \\] The transpose of the product is the product of the transposes … but the other way around: \\((\\mathbf{A} \\mathbf{B})^\\top = \\mathbf{B}^\\top \\mathbf{A}^\\top\\). This is intuitive, if you think about it. Suppose \\(\\mathbf{A}\\) is \\(n \\times m\\) and \\(\\mathbf{B}\\) is \\(m \\times p\\). Then \\(\\mathbf{A} \\mathbf{B}\\) is \\(n \\times p\\), so \\((\\mathbf{A} \\mathbf{B})^\\top\\) should be \\(p \\times n\\). Therefore, \\(\\mathbf{B}^\\top\\) must come first. 6.3 Matrix Inversion We’ve covered matrix addition, subtraction, and multiplication. What about division? Let’s think about division of real numbers for a second. We know that any division problem can be rewritten as a multiplication problem, \\[ \\frac{a}{b} = a \\times b^{-1}, \\] where \\(b^{-1}\\) is the unique real number such that \\[ b \\times b^{-1} = 1. \\] Similarly, in matrix algebra, we say that the \\(n \\times n\\) matrix \\(\\mathbf{C}\\) is an inverse of the \\(n \\times n\\) matrix \\(\\mathbf{A}\\) if \\(\\mathbf{A} \\mathbf{C} = \\mathbf{C} \\mathbf{A} = \\mathbf{I}_n\\). Some basic properties of matrix inverses: If \\(\\mathbf{C}\\) is an inverse of \\(\\mathbf{A}\\), then \\(\\mathbf{A}\\) is an inverse of \\(\\mathbf{C}\\). This is immediate from the definition. If \\(\\mathbf{C}\\) and \\(\\mathbf{D}\\) are both inverses of \\(\\mathbf{A}\\), then \\(\\mathbf{C} = \\mathbf{D}\\). Proof: If \\(\\mathbf{C}\\) and \\(\\mathbf{D}\\) are inverses of \\(\\mathbf{A}\\), then we have \\[ \\begin{aligned} \\mathbf{A} \\mathbf{C} = \\mathbf{I} &amp;\\Leftrightarrow \\mathbf{D} (\\mathbf{A} \\mathbf{C}) = \\mathbf{D} \\mathbf{I} \\\\ &amp;\\Leftrightarrow (\\mathbf{D} \\mathbf{A}) \\mathbf{C} = \\mathbf{D} \\\\ &amp;\\Leftrightarrow \\mathbf{I} \\mathbf{C} = \\mathbf{D} \\\\ &amp;\\Leftrightarrow \\mathbf{C} = \\mathbf{D}. \\end{aligned} \\] As a consequence of this property, we write the inverse of \\(\\mathbf{A}\\), when it exists, as \\(\\mathbf{A}^{-1}\\). The inverse of the inverse of \\(\\mathbf{A}\\) is \\(\\mathbf{A}\\): \\((\\mathbf{A}^{-1})^{-1} = \\mathbf{A}\\). If the inverse of \\(\\mathbf{A}\\) exists, then the inverse of its transpose is the transpose of the inverse: \\((\\mathbf{A}^\\top)^{-1} = (\\mathbf{A}^{-1})^\\top\\). Matrix inversion inverts scalar multiplication: if \\(c \\neq 0\\), then \\((c \\mathbf{A})^{-1} = (1/c) \\mathbf{A}^{-1}\\). The identity matrix is its own inverse: \\(\\mathbf{I}_n^{-1} = \\mathbf{I}_n\\). Some matrices are not invertible; i.e., their inverse does not exist. As a simple example, think of \\[ \\mathbf{A} = \\begin{bmatrix} 0 &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix}. \\] It’s easy to see that, for any \\(2 \\times 2\\) matrix \\(\\mathbf{B}\\), we have \\[ \\mathbf{A} \\mathbf{B} = \\begin{bmatrix} 0 &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix} \\neq \\mathbf{I}_2. \\] Therefore, \\(\\mathbf{A}\\) does not have an inverse. Remember that matrix inversion is kind of like division for scalar numbers. In that light, the previous example is a generalization of the principle that you can’t divide by zero. But matrices full of zeroes are not the only ones that aren’t invertible. For instance, it may not be obvious at first glance, but the following matrix is not invertible: \\[ \\mathbf{A} = \\begin{bmatrix} 1 &amp; 2 \\\\ 2 &amp; 4 \\end{bmatrix}. \\] We know that because of the following theorem: A matrix is invertible if and only if its columns are linearly independent. In the above example, the second column is 2 times the first column, so the columns are not linearly independent, so the matrix is not invertible. Consider the general \\(2 \\times 2\\) matrix \\[ \\mathbf{A} = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix}. \\] We have a simple criterion for linear independence here. In particular, the columns of \\(\\mathbf{A}\\) are linearly independent if and only if \\(ad \\neq bc\\), or \\(ad - bc \\neq 0\\). We call this the determinant of the matrix, since it determines whether the matrix is invertible.19 Moreover, if \\(ad - bc \\neq 0\\) we have \\[ \\mathbf{A}^{-1} = \\frac{1}{ad - bc} \\begin{bmatrix} d &amp; -b \\\\ -c &amp; a \\end{bmatrix}. \\] I’ll leave it to you to convince yourself that’s true. For now, let’s try a couple of examples. \\[ \\begin{gathered} \\mathbf{A} = \\begin{bmatrix} 1 &amp; 1 \\\\ 0 &amp; 1 \\end{bmatrix}, \\mathbf{A}^{-1} = \\begin{bmatrix} 1 &amp; -1 \\\\ 0 &amp; 1 \\end{bmatrix}, \\\\ \\mathbf{A} = \\begin{bmatrix} 4 &amp; 6 \\\\ 2 &amp; 4 \\end{bmatrix}, \\mathbf{A}^{-1} = \\begin{bmatrix} 1 &amp; -1.5 \\\\ -0.5 &amp; 1 \\end{bmatrix}, \\\\ \\mathbf{A} = \\begin{bmatrix} 10 &amp; 25 \\\\ 4 &amp; 10 \\end{bmatrix}, \\text{$\\mathbf{A}^{-1}$ does not exist}. \\end{gathered} \\] 6.4 Solving Linear Systems You may remember from high school being asked to solve for \\(x_1\\) and \\(x_2\\) in systems of equations like the following one: \\[ \\begin{aligned} 2 x_1 + x_2 &amp;= 10, \\\\ 2 x_1 - x_2 &amp;= -10. \\end{aligned} \\] Matrix algebra lets us write this whole system as a single equation, \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\), where \\[ \\begin{aligned} \\mathbf{A} &amp;= \\begin{bmatrix} 2 &amp; 1 \\\\ 2 &amp; -1 \\end{bmatrix}, \\\\ \\mathbf{x} &amp;= \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}, \\\\ \\mathbf{b} &amp;= \\begin{bmatrix} 10 \\\\ -10 \\end{bmatrix}. \\end{aligned} \\] This suggests a natural way to solve for \\(\\mathbf{x}\\): \\[\\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}.\\] In fact, the linear system of equations \\(\\mathbf{A} \\mathbf{x} = \\mathbf{b}\\) has a unique solution if and only if \\(\\mathbf{A}\\) is invertible. Otherwise, it has either zero solutions or infinitely many solutions. Example with zero solutions: \\[ \\begin{aligned} x_1 + x_2 &amp;= 1, \\\\ 2 x_1 + 2 x_2 &amp;= 10. \\end{aligned} \\] Example with infinitely many solutions: \\[ \\begin{aligned} x_1 + x_2 &amp;= 1, \\\\ 2 x_1 + 2 x_2 &amp;= 2. \\end{aligned} \\] 6.5 Appendix: Matrices in R We use the matrix() command to create matrices. A &lt;- matrix(c(2, 1, 3, 4), nrow = 2, ncol = 2) A ## [,1] [,2] ## [1,] 2 3 ## [2,] 1 4 Notice that it fills “down” by column. To fill “across” instead, use the byrow argument: B &lt;- matrix(c(2, 1, 3, 4), nrow = 2, ncol = 2, byrow = 2) B ## [,1] [,2] ## [1,] 2 1 ## [2,] 3 4 There are a few utilities for checking the dimension of a matrix. nrow(A) ## [1] 2 ncol(A) ## [1] 2 dim(A) ## [1] 2 2 To extract the \\(i\\)’th row, \\(j\\)’th column, or \\(ij\\)’th element, use square brackets. A[1, ] # 1st row ## [1] 2 3 A[, 2] # 2nd column ## [1] 3 4 A[2, 1] # entry in 2nd row, 1st column ## [1] 1 Notice that when you extract a row or column, R turns it into a vector—the result has only a single dimension. If you dislike this behavior (i.e., you want an extracted column to be a 1-column matrix), use the drop = FALSE option in the square brackets. A[, 2, drop = FALSE] ## [,1] ## [1,] 3 ## [2,] 4 Adding and subtracting matrices works as you’d expect. A + B ## [,1] [,2] ## [1,] 4 4 ## [2,] 4 8 A - B ## [,1] [,2] ## [1,] 0 2 ## [2,] -2 0 As does scalar multiplication. 5 * A ## [,1] [,2] ## [1,] 10 15 ## [2,] 5 20 -1 * B ## [,1] [,2] ## [1,] -2 -1 ## [2,] -3 -4 However, the * operator performs element-by-element multiplication, not matrix multiplication. A * B ## [,1] [,2] ## [1,] 4 3 ## [2,] 3 16 To perform matrix multiplication, use the %*% operator. A %*% B ## [,1] [,2] ## [1,] 13 14 ## [2,] 14 17 To invert a matrix or solve a linear system, use the solve() function. # Invert A solve(A) ## [,1] [,2] ## [1,] 0.8 -0.6 ## [2,] -0.2 0.4 # Solve for x in Ax = (3, 2) solve(A, c(3, 2)) ## [1] 1.2 0.2 Here is a not-so-fun fact about matrix inversion in R: it’s not entirely exact. To see this, let’s invert a matrix with some decimal elements. X &lt;- matrix(c(1.123, 2.345, 3.456, 4.567), 2, 2) Y &lt;- solve(X) Y ## [,1] [,2] ## [1,] -1.53483 1.16145 ## [2,] 0.78808 -0.37741 Now let’s see what we get when we multiply X and Y. X %*% Y ## [,1] [,2] ## [1,] 1.00e+00 1.4798e-16 ## [2,] 8.21e-17 1.0000e+00 That’s not an identity matrix! The issue here is floating point error, the fact that decimal numbers are not stored exactly on computers. Notice that the off-diagonal elements here, which are supposed to be exactly zero, are instead very very tiny numbers, on the order of \\(10^{-16}\\), or 0.0000000000000001. Let’s check that our result is numerically equal to what we expected. By numerically equal, I mean, loosely speaking, that any differences are less than the amount of error you would expect due to floating point error. First we’ll use diag() to generate a \\(2 \\times 2\\) identity matrix, then we’ll compare numerical equality using all.equal(). I &lt;- diag(2) all.equal(X %*% Y, I) ## [1] TRUE Whereas the traditional == operator is stricter, checking for exact equality. X %*% Y == I ## [,1] [,2] ## [1,] TRUE FALSE ## [2,] FALSE FALSE Moral of the story: when comparing decimal numbers, use all.equal() rather than ==. When all.equal() is not TRUE, it returns a message indicating how far apart the numbers are. This is annoying if you want to use all.equal() in, say, an if/else statement. To get around that, we have the isTRUE() function. all.equal(1.0, 1.5) ## [1] &quot;Mean relative difference: 0.5&quot; isTRUE(all.equal(1.0, 1.5)) ## [1] FALSE One last thing. If solve() throws an error that says “reciprocal condition number…” or “system is exactly singular”, that means you tried to invert a matrix that is not invertible. Z &lt;- matrix(c(1, 1, 2, 2), 2, 2) solve(Z) ## Error in solve.default(Z): Lapack routine dgesv: system is exactly singular: U[2,2] = 0 Sad! R will let you add and subtract vectors of different lengths, via a technique called “recycling”. For example c(1, 0) + c(1, 2, 3, 4) will produce c(2, 2, 4, 4). This is kosher in R, but not in mathematical derivations.↩ On the determinants of \\(3 \\times 3\\) and larger matrices, see your friendly local linear algebra textbook. Calculating the determinant becomes exponentially more complicated with the size of the matrix.↩ "],
["ols-matrix.html", "7 Reintroduction to the Linear Model 7.1 The Linear Model in Matrix Form 7.2 The OLS Estimator 7.3 Vector-Valued Random Variables 7.4 Properties of OLS", " 7 Reintroduction to the Linear Model Having learned some matrix algebra, let us now return to the world of statistics. We are going to take what we learned about regression and ordinary least squares in the bivariate case, then generalize it to a setting with potentially many variables. To make that task feasible, we will rely on the tools of matrix algebra that we learned last week. 7.1 The Linear Model in Matrix Form We have a sequence of observations indexed by \\(n \\in \\{1, \\ldots, N\\}\\). Each observation consists of a response, \\(Y_n\\), a real number; and a vector of \\(K\\) covariates, \\[ \\mathbf{x}_n = \\begin{pmatrix} x_{n1} \\\\ x_{n2} \\\\ \\vdots \\\\ x_{nK} \\end{pmatrix}. \\] Just like in bivariate regression, our goal is to estimate the conditional expectation of the response given the covariates, \\(E[Y_n \\,|\\, \\mathbf{x}_n]\\). To make that task feasible, we will assume the relationship is linear, \\[ E[Y_n \\,|\\, \\mathbf{x}_n] = \\beta \\cdot \\mathbf{x}_n, \\] where \\(\\beta\\) is the \\(K \\times 1\\) vector of coefficients, \\[ \\beta = \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_K \\end{pmatrix}. \\] Our data model is \\[ Y_n = \\beta \\cdot \\mathbf{x}_n + \\epsilon_n, \\] where \\(\\epsilon_n\\) is “white noise” error that is uncorrelated with the covariates. (More on this in a second.) This data model looks a little bit different than our bivariate linear model, which you’ll recall was \\[ Y_n = \\alpha + \\beta x_n + \\epsilon_n. \\] What happened to \\(\\alpha\\), the intercept? When working with the multivariate linear model, it will make our lives easiest to treat the intercept like any other coefficient. Specifically, we will assume \\(x_{n1} = 1\\) for all \\(n\\), and we will treat \\(\\beta_1\\) as the intercept. With \\(K = 2\\), our multivariate model becomes \\[ \\begin{aligned} Y_n &amp;= \\beta \\cdot \\mathbf{x}_n + \\epsilon_n \\\\ &amp;= \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\end{pmatrix} \\cdot \\begin{pmatrix} 1 \\\\ x_{n2} \\end{pmatrix} + \\epsilon_n \\\\ &amp;= \\beta_1 + \\beta_2 x_{n2} + \\epsilon_n, \\end{aligned} \\] which is the same as our bivariate regression model, replacing the intercept \\(\\alpha\\) with \\(\\beta_1\\), the slope \\(\\beta\\) with \\(\\beta_2\\), and the covariate \\(x_n\\) with \\(x_{n2}\\). If we were to stack up all of our data, we would have \\(N\\) equations, \\[ \\begin{aligned} Y_1 &amp;= \\beta \\cdot \\mathbf{x}_1 + \\epsilon_1, \\\\ Y_2 &amp;= \\beta \\cdot \\mathbf{x}_2 + \\epsilon_2, \\\\ &amp;\\vdots \\\\ Y_N &amp;= \\beta \\cdot \\mathbf{x}_N + \\epsilon_N. \\end{aligned} \\] Like any system of linear equations, we can write this one more easily in matrix form. Let \\(\\mathbf{Y}\\) be the \\(N \\times 1\\) vector that collects the response, \\[ \\mathbf{Y} = \\begin{pmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_N \\end{pmatrix}. \\] Let \\(\\mathbf{X}\\) be the \\(N \\times K\\) matrix that collects the covariates, \\[ \\mathbf{X} = \\begin{bmatrix} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1K} \\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2K} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{N1} &amp; x_{N2} &amp; \\cdots &amp; x_{NK} \\end{bmatrix} = \\begin{bmatrix} 1 &amp; x_{12} &amp; \\cdots &amp; x_{1K} \\\\ 1 &amp; x_{22} &amp; \\cdots &amp; x_{2K} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{N2} &amp; \\cdots &amp; x_{NK} \\end{bmatrix}. \\] The \\(n\\)’th row of \\(\\mathbf{X}\\), which we will write \\(\\mathbf{x}_n\\) (lowercase), contains the covariates for the \\(n\\)’th observation. The \\(k\\)’th column of \\(\\mathbf{X}\\), which we will write \\(\\mathbf{X}_k\\) (uppercase), contains the value of the \\(k\\)’th covariate for every observation. Finally, we will collect the error terms in an \\(N \\times 1\\) vector, \\[ \\epsilon = \\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_N \\end{pmatrix}. \\] We can now write a model of the full data, \\[ \\mathbf{Y} = \\mathbf{X} \\beta + \\epsilon. \\] It is worth pausing to clarify what is known and unknown here. The covariate matrix \\(\\mathbf{X}\\) and the response vector \\(\\mathbf{Y}\\) are known. They are our data. The regression parameters \\(\\beta\\) are unknown. They are what we are trying to learn from the data. The error term \\(\\epsilon\\) is also unknown. We can think of each observation of \\(Y_n\\) as being a combination of “signal”, \\(\\mathbf{x}_n \\cdot \\beta\\), and “noise”, \\(\\epsilon_n\\). The fundamental problem is that we don’t know exactly what the signal is and what the noise is. 7.2 The OLS Estimator Consider the linear model with three covariates, \\[ Y_n = \\beta_1 x_{n1} + \\beta_2 x_{n2} + \\beta_3 x_{n3} + \\epsilon_n. \\] Let’s do like we did with bivariate regression, and imagine estimating the parameters of the model by least squares. Let \\((b_1, b_2, b_3)\\) denote an estimate of the parameters.20 We will set up the sum of squared errors as a function of the parameters, \\[ {\\mathop{\\rm SSE}\\nolimits}(b_1, b_2, b_3) = \\sum_n (Y_n - b_1 x_{n1} - b_2 x_{n2} - b_3 x_{n3})^2. \\] Just as we did to derive the bivariate OLS estimator, let’s begin by taking the partial derivative of the SSE with respect to the first regression coefficient, then equalizing it to zero. \\[ \\frac{\\partial {\\mathop{\\rm SSE}\\nolimits}}{\\partial b_1} = -2 \\sum_n x_{n1} (Y_n - b_1 x_{n1} - b_2 x_{n2} - b_3 x_{n3}) = 0. \\] Dividing each side by \\(-2\\) and rearranging terms gives us \\[ \\sum_n x_{n1} (b_1 x_{n1} + b_2 x_{n2} + b_3 x_{n3}) = \\sum_n x_{n1} Y_n. \\] If we break up the left-hand sum into three individual sums, we get \\[ \\left( \\sum_n x_{n1}^2 \\right) b_1 + \\left( \\sum_n x_{n1} x_{n2} \\right) b_2 + \\left( \\sum_n x_{n1} x_{n3} \\right) b_3 = \\sum_n x_{n1} Y_n, \\] which is a linear condition on \\((b_1, b_2, b_3)\\). If we go through the same steps with \\(\\partial {\\mathop{\\rm SSE}\\nolimits}/ \\partial b_2\\) and \\(\\partial {\\mathop{\\rm SSE}\\nolimits}/ \\partial b_3\\), we obtain the linear system \\[ \\begin{aligned} \\left( \\sum_n x_{n1}^2 \\right) b_1 + \\left( \\sum_n x_{n1} x_{n2} \\right) b_2 + \\left( \\sum_n x_{n1} x_{n3} \\right) b_3 &amp;= \\sum_n x_{n1} Y_n, \\\\ \\left( \\sum_n x_{n2} x_{n1} \\right) b_1 + \\left( \\sum_n x_{n2}^2 \\right) b_2 + \\left( \\sum_n x_{n2} x_{n3} \\right) b_3 &amp;= \\sum_n x_{n2} Y_n, \\\\ \\left( \\sum_n x_{n3} x_{n1} \\right) b_1 + \\left( \\sum_n x_{n3} x_{n2} \\right) b_2 + \\left( \\sum_n x_{n3}^2 \\right) b_3 &amp;= \\sum_n x_{n3} Y_n. \\end{aligned} \\] This is a linear system of three equations in three unknowns, namely \\((b_1, b_2, b_3)\\). We can write it as \\(\\mathbf{A} \\mathbf{b} = \\mathbf{c}\\), where \\(\\mathbf{b}\\) is the \\(3 \\times 1\\) column vector we are trying to solve for. You’ll remember from last week that we use matrix algebra to solve linear systems like this one. Let’s take a closer look at the coefficient matrix we have here, \\[ \\mathbf{A} = \\begin{bmatrix} \\sum_n x_{n1}^2 &amp; \\sum_n x_{n1} x_{n2} &amp; \\sum_n x_{n1} x_{n3} \\\\ \\sum_n x_{n2} x_{n1} &amp; \\sum_n x_{n2}^2 &amp; \\sum_n x_{n2} x_{n3} \\\\ \\sum_n x_{n3} x_{n1} &amp; \\sum_n x_{n3} x_{n2} &amp; \\sum_n x_{n3}^2 \\end{bmatrix} \\] Notice that each \\(ij\\)’th element is \\[ a_{ij} = \\sum_n x_{ni} x_{nj} = \\mathbf{X}_i \\cdot \\mathbf{X}_j, \\] the dot product of the \\(i\\)’th and \\(j\\)’th columns of our \\(\\mathbf{X}\\) matrix. Of course, the \\(i\\)’th column of \\(\\mathbf{X}\\) is the \\(i\\)’th row of \\(\\mathbf{X}^\\top\\). If the \\(ij\\)’th entry of \\(\\mathbf{A}\\) is the dot product of the \\(i\\)’th row of \\(\\mathbf{X}^\\top\\) and the \\(j\\)’th column of \\(\\mathbf{X}\\), that means \\[ \\mathbf{A} = \\mathbf{X}^\\top \\mathbf{X}. \\] Similarly, let’s take a look at our right-hand side, \\[ \\mathbf{c} = \\begin{bmatrix} \\sum_n x_{n1} Y_n \\\\ \\sum_n x_{n2} Y_n \\\\ \\sum_n x_{n3} Y_n \\end{bmatrix}. \\] Each \\(i\\)’th entry of \\(\\mathbf{c}\\) is \\[ c_i = \\sum_n x_{ni} Y_n = \\mathbf{X}_i \\cdot \\mathbf{Y}. \\] the dot product of the \\(i\\)’th column of \\(\\mathbf{X}\\) (i.e., the \\(i\\)’th column of \\(\\mathbf{X}^\\top\\)) and the vector \\(\\mathbf{Y}\\). Therefore, we have \\[ \\mathbf{c} = \\mathbf{X}^\\top \\mathbf{Y}. \\] Our linear system of equations, \\(\\mathbf{A} \\mathbf{b} = \\mathbf{c}\\), is equivalent to \\[ (\\mathbf{X}^\\top \\mathbf{X}) \\mathbf{b} = \\mathbf{X}^\\top \\mathbf{Y}. \\] Consequently, the solution to the system is \\[ \\mathbf{b} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{Y}. \\] Although we got here via the \\(3 \\times 3\\) case, this formula works for any number of covariates. The OLS estimator of the linear model coefficients from covariate matrix \\(\\mathbf{X}\\) and response vector \\(\\mathbf{Y}\\) is \\[ \\hat{\\beta}_{{\\text{OLS}}}(\\mathbf{X}, \\mathbf{Y}) = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{Y}. \\] When you see this formula, your hackles should be raised. Wait a minute, you ought to be saying. How do we know the inverse of \\(\\mathbf{X}^\\top \\mathbf{X}\\) exists? That’s an excellent question! Luckily, there’s a simple condition: \\(\\mathbf{X}^\\top \\mathbf{X}\\) is invertible if and only if the columns of \\(\\mathbf{X}\\) are linearly independent. The linear independence condition isn’t just a technical thing that we need to satisfy. It goes to the heart of what we’re doing in linear regression. If the columns of \\(\\mathbf{X}\\) aren’t linearly independent, then the question you’re asking of OLS—to learn something about the coefficients from the data—is ill-defined. Imagine you have a linear dependency between two variables, so one is just a scalar multiple of the other. For example, a regression of a person’s weight on their height in inches and height in centimeters. Or a regression of whether it rains on temperature Fahrenheit and temperature Celsius. It is absurd to think that the relationship between temperature and rain might be different depending on how you measure it. But that’s exactly what you’re asking for when you run this regression—separate estimates for the effect of degrees Fahrenheit and the effect of degrees Celsius. 7.3 Vector-Valued Random Variables Before we can talk about the properties of OLS in the multivariate case, we need to refresh ourselves on how basic statistical operations (expected value and variance) translate when we’re dealing with vectors of random variables. Let \\(A\\) and \\(B\\) be random variables with means \\(\\mu_A = E[A]\\) and \\(\\mu_B = E[B]\\) respectively. Let \\(C\\) be the column vector whose first value is \\(A\\) and whose second value is \\(B\\): \\[ C = \\begin{pmatrix} A \\\\ B \\end{pmatrix}. \\] As a function of random variables, \\(C\\) is itself a random variable. Unlike those we’ve encountered before, though, it is a vector-valued random variable. Assume \\(A\\) and \\(B\\) take values in the finite sets \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\) respectively. The expected value of \\(C\\) is \\[ \\begin{aligned} E[C] &amp;= \\sum_{a \\in \\mathcal{A}} \\sum_{b \\in \\mathcal{B}} \\begin{pmatrix} a \\\\ b \\end{pmatrix} \\Pr(A = a, B = b) \\\\ &amp;= \\begin{pmatrix} \\mu_A \\\\ \\mu_B \\end{pmatrix}. \\end{aligned} \\] I encourage you to prove this on your own—the proof just relies on simple facts about vector addition and joint probability that we’ve already covered in this class. It is easiest to prove in the finite case, but it remains true that \\(E[C] = (\\mu_A, \\mu_B)\\) in the more general case. You might expect the variance of \\(C\\) to be a vector too. You would be wrong—it’s a \\(2 \\times 2\\) matrix. \\[ \\begin{aligned} V[C] &amp;= E\\left[(C - E[C]) (C - E[C])^\\top \\right] \\\\ &amp;= E \\left[ \\begin{pmatrix} A - \\mu_A \\\\ B - \\mu_B \\end{pmatrix} \\begin{pmatrix} A - \\mu_A &amp; B - \\mu_B \\end{pmatrix} \\right] \\\\ &amp;= E \\left[ \\begin{bmatrix} (A - \\mu_A)^2 &amp; (A - \\mu_A) (B - \\mu_B) \\\\ (A - \\mu_A) (B - \\mu_B) &amp; (B - \\mu_B)^2 \\end{bmatrix} \\right] \\\\ &amp;= \\begin{bmatrix} E[(A - \\mu_A)^2] &amp; E[(A - \\mu_A) (B - \\mu_B)] \\\\ E[(A - \\mu_A) (B - \\mu_B)] &amp; E[(B - \\mu_B)^2] \\end{bmatrix} \\\\ &amp;= \\begin{bmatrix} V[A] &amp; {\\mathop{\\rm Cov}\\nolimits}[A, B] \\\\ {\\mathop{\\rm Cov}\\nolimits}[A, B] &amp; V[B] \\end{bmatrix}. \\end{aligned} \\] This is what we call the variance matrix, or variance-covariance matrix, of a vector-valued random variable. The \\(i\\)’th element along the main diagonal gives us the variance of the \\(i\\)’th element of the vector. The \\(ij\\)’th off-diagonal element gives us the covariance of the \\(i\\) and \\(j\\)’th elements. Consequently, since \\({\\mathop{\\rm Cov}\\nolimits}[A, B] = {\\mathop{\\rm Cov}\\nolimits}[B, A]\\), the variance matrix is always symmetric. 7.4 Properties of OLS Just like in the bivariate case, the “good” properties of OLS depend on whether the process that generated our data satisfies particular assumptions. The key assumption, which we call strict exogeneity, is \\[ E[\\epsilon \\,|\\, \\mathbf{X}] = \\mathbf{0}. \\] In other words, the error term must be uncorrelated with the covariates. Remember that the error for the \\(n\\)’th observation, \\(\\epsilon_n\\), collects everything that affects \\(Y_n\\) but is not included in \\(\\mathbf{x}_n\\). So what we’re saying when we impose this condition is either that there’s nothing else out there that affects \\(\\mathbf{Y}\\) besides \\(\\mathbf{X}\\) (unlikely!), or that anything else that affects \\(\\mathbf{Y}\\) is uncorrelated with \\(\\mathbf{X}\\) (also unlikely, but slightly less so!). In the ’90s and ’00s, as more data became available and computing power increased, political scientists labored under the delusion that the way to make strict exogeneity hold was to throw every covariate you could imagine into each regression. This approach was statistically illiterate (Clarke 2005) and scholars have since begun to favor design-based approaches. The basic idea is to collect data with relatively little unobservable heterogeneity, whether through experiments or through careful observational work, rather than to try to eliminate it through endless controls. We’ll talk more about design when we get to causal inference, and it will be a major source of discussion in Stat III. For now, let us proceed imagining that strict exogeneity holds. Then, just as in the bivariate case, OLS is unbiased. In fact, it’s even easier to prove now. First, notice that under strict exogeneity, we have \\[ \\begin{aligned} E[\\mathbf{Y} \\,|\\, \\mathbf{X}] &amp;= E[\\mathbf{X} \\beta + \\epsilon \\,|\\, \\mathbf{X}] \\\\ &amp;= \\mathbf{X} \\beta + E[\\epsilon \\,|\\, \\mathbf{X}] \\\\ &amp;= \\mathbf{X} \\beta. \\end{aligned} \\] It follows that \\[ \\begin{aligned} E[\\hat{\\beta}_{{\\text{OLS}}}(\\mathbf{X}, \\mathbf{Y}) \\,|\\, \\mathbf{X}] &amp;= E[(\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{Y} \\,|\\, \\mathbf{X}] \\\\ &amp;= (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top E[\\mathbf{Y} \\,|\\, \\mathbf{X}] \\\\ &amp;= (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top (\\mathbf{X} \\beta) \\\\ &amp;= (\\mathbf{X}^\\top \\mathbf{X})^{-1} (\\mathbf{X}^\\top \\mathbf{X}) \\beta \\\\ &amp;= \\beta, \\end{aligned} \\] which is the definition of unbiasedness. Unbiasedness is a small-sample property. No matter the sample size, if strict exogeneity holds, the OLS estimator is unbiased. OLS also has some asymptotic (or large-sample) properties under strict exogeneity that we won’t prove, but are worth mentioning: OLS is consistent. Informally, what this means is that as \\(N\\) grows larger, the distribution of the OLS estimator becomes tighter around the population parameter \\(\\beta\\). In other words, with a sufficiently large sample, it becomes highly unlikely that you will draw a sample \\((\\mathbf{X}, \\mathbf{Y})\\) such that \\(\\hat{\\beta}_{{\\text{OLS}}}(\\mathbf{X}, \\mathbf{Y})\\) is far from the true value. Of course, you can’t know that the OLS estimate from any particular sample is close to the truth. But you’re much more likely to get an estimate close to the truth if \\(N = 100{,}000\\) than if \\(N = 10\\). OLS is asymptotically normal. Informally, what this is means is that if \\(N\\) is large enough, the sampling distribution of \\(\\hat{\\beta}_{{\\text{OLS}}}\\) (i.e., its distribution across different possible samples) is roughly normal. This makes the computation of inferential statistics fairly simple in large samples. More on this in two weeks. Unbiasedness and consistency are nice, but frankly they’re kind of dime-a-dozen. Lots of estimators are unbiased and consistent. Why is OLS so ubiquitous? The reason is that it is efficient, at least under a particular condition on the error term. Unlike unbiasedness and consistency, efficiency is defined with reference to other estimators. Given some class or collection of estimators, one is efficient if it has the lowest standard errors—i.e., it is the least sensitive to sampling variation, and thereby the most likely to come close to the true parameter value. The condition we need to hold is that we have spherical errors: \\[ V[\\epsilon \\,|\\, \\mathbf{X}] = \\sigma^2 \\mathbf{I}_N = \\begin{bmatrix} \\sigma^2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\sigma^2 \\end{bmatrix}. \\] Spherical errors summarizes two important conditions: The variance of each \\(\\epsilon_n\\)—i.e., the expected “spread” of the points around the regression line—is the same for every observation. This is also known as homoskedasticity. For \\(n \\neq m\\), there is no correlation between \\(\\epsilon_n\\) and \\(\\epsilon_m\\). In other words, the fact that \\(Y_n\\) lies above the regression line doesn’t tell us anything about whether \\(Y_m\\) lies above or below the regression line. This is also known as no autocorrelation. Spherical errors holds if each \\(\\epsilon_n\\) is independent and identically distributed, though it is possible for non-i.i.d. errors to satisfy the condition. The illustration below compares spherical and non-spherical errors. Notice that in the right-hand graph, the distribution of errors around the regression line is uneven—the spread is much greater at greater values of the covariate. According to the Gauss-Markov theorem, if the errors are spherical, then OLS is the best linear unbiased estimator (BLUE) of the linear model parameters \\(\\beta\\). By “best,” we mean that it is efficient—any other linear unbiased estimator has larger standard errors. In other words, under the spherical error condition, any estimator \\(\\hat{\\beta}\\) with a smaller standard errors than OLS must either be: Biased: \\(E[\\hat{\\beta}] \\neq \\beta\\). Nonlinear: \\(\\hat{\\beta}\\) cannot be written as a linear function of \\(Y\\). Much later in the course, we will encounter ridge regression, a linear estimator that has lower standard errors than OLS. The Gauss-Markov theorem tells us that we’re making a tradeoff when we use ridge regression—that we’re taking on some bias in exchange for the reduction in variance. References "],
["specification.html", "8 Specification Issues 8.1 Categorical Variables 8.2 Interaction Terms 8.3 Quadratic and Logarithmic Terms 8.4 Appendix: Nonstandard Specifications in R", " 8 Specification Issues I lied to you about the linear model last week. Like the grade-school teachers who told you everyone thought the world was flat before Columbus proved them wrong, I had good intentions—but it was a lie nonetheless. I claimed that the linear model assumed that the conditional expectation of the response was a linear function of the covariates. That is false. A data model is a linear model, can be estimated consistently and without bias by OLS, and all that good stuff, as long as it is linear in the parameters. For example, the following is a linear model. \\[ Y_n = \\beta_1 + \\beta_2 x_n + \\beta_3 x_n^2 + \\beta_4 x_n^7 + \\epsilon_n. \\] The conditional expectation of \\(Y_n\\) is a nonlinear function of \\(x_n\\) (holding \\(\\beta\\) fixed) but a linear function of \\(\\beta\\) (holding \\(x_n\\) fixed). Therefore, assuming strict exogeneity holds, OLS is an unbiased, consistent, asymptotically normal estimator of \\(\\beta\\). The following is not a linear model. \\[ Y_n = 2^{\\beta_1} + 2^{\\beta_2} x_n + \\epsilon_n. \\] Holding \\(\\beta\\) fixed, this is a linear function of the covariate \\(x_n\\). But, holding \\(x_n\\) fixed, this is not a linear function of \\(\\beta\\). OLS is not an appropriate estimator for the parameters of this model. This week, we will talk about linear models with non-standard covariate specifications—those that aren’t just a linear function of continuous variables. 8.1 Categorical Variables Using the linear model, we write the conditional expectation for the \\(n\\)’th response as \\[ E[Y_n \\,|\\, \\mathbf{x}_n] = \\mathbf{x}_n \\cdot \\beta + \\epsilon_n, \\] where \\(\\mathbf{x}_n\\) is the vector of \\(K\\) covariates (including the intercept) and \\(\\beta\\) is the vector of \\(K\\) coefficients we wish to estimate. This makes sense with numerical variables, but not so much with categorical variables. For example, think of the relationship between party identification and one’s vote in the 2016 presidential election. Suppose our response variable is one’s vote (1 for Trump, 0 for non-Trump), and our party ID variable records whether the respondent is a Republican, Democrat, or independent. The resulting linear model equation, \\[ \\text{Trump}_n = \\beta_1 + \\beta_2 \\text{Party ID}_n + \\epsilon_n, \\] doesn’t really make sense, because party ID isn’t a number.21 To incorporate a categorical variable into the linear model, we break each category into its own binary variable. For example, with our party ID variable, we go from \\[ \\text{Party ID} = \\begin{pmatrix} \\text{R} \\\\ \\text{R} \\\\ \\text{I} \\\\ \\text{I} \\\\ \\text{D} \\\\ \\text{D} \\end{pmatrix} \\] to \\[ \\text{Republican} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\text{Independent} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\text{Democratic} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix}. \\] These are called dummy variables or, preferably, indicator variables. Having turned our categorical variable into a set of indicators, you may be tempted to rewrite the model as \\[ \\text{Trump}_n = \\beta_1 + \\beta_2 \\text{Republican}_n + \\beta_3 \\text{Independent}_n + \\beta_4 \\text{Democratic}_n + \\epsilon_n. \\] But take a look at the matrix of covariates, or design matrix, that would result if we set up the model this way: \\[ \\mathbf{X} = \\begin{bmatrix} 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] The columns of the design matrix are linearly dependent: the constant term is equal to the sum of three party ID indicators. (A useful exercise is to calculate \\(\\mathbf{X}^\\top \\mathbf{X}\\) and confirm that its columns are linearly dependent too.) This means we can’t include all three when estimating \\(\\beta\\) via OLS—we have to drop one category. In one sense, which category we drop is immaterial—our regression will make the same predictions either way. However, in order to interpret the results of a regression on categorical variables, it is important that we know what the categories are, and which one has been dropped. For example, imagine we drop the Republican category, so we have the following linear model: \\[ \\text{Trump}_n = \\beta_1 + \\beta_2 \\text{Independent}_n + \\beta_3 \\text{Democratic}_n + \\epsilon_n. \\] For a Republican voter, the Independent and Democratic variables will both equal zero, so we will have \\[ E[\\text{Trump}_n \\,|\\, \\text{Party ID}_n = \\text{R}] = \\beta_1. \\] In other words, the intercept will be the predicted probability that a Republican votes for Trump. For an Independent voter, we will have \\[ E[\\text{Trump}_n \\,|\\, \\text{Party ID}_n = \\text{I}] = \\beta_1 + \\beta_2. \\] So the coefficient on the Independent indicator is not the predicted probability that an Independent votes for Trump. Instead, it is the difference in probability of a Trump vote between Independents and the baseline category (in this case, Republicans). If Independents are less likely than Republicans to vote for Trump, the coefficient on Independent will be negative. If Independents are more likely than Republicans to vote for Trump, the coefficient on Independent will be positive. If Independents are equally likely as Republicans to vote for Trump, the coefficient on Independent will be zero. Similarly, for a Democratic voter, we have \\[ E[\\text{Trump}_n \\,|\\, \\text{Party ID}_n = \\text{D}] = \\beta_1 + \\beta_3. \\] The interpretation of the coefficient on Democratic is the same as for the coefficient on Independent. Take a look at the following results of a hypothetical regression. Coefficient Estimate (Intercept) 0.9 Independent -0.4 Democratic -0.75 Republicans have a 90% chance of voting for Trump. We see that by looking at the intercept, since Republicans are the omitted category. We see from the coefficient on Independent that an Independent’s chance of voting for Trump is 40% lower than a Republican’s. This means that an Independent has a 50% chance of voting for Trump. Similarly, we see from the coefficient on Democratic that a Democrat’s chance of voting for Trump is 75% lower than a Republican’s, for a 15% chance overall. Had we instead omitted Independent, we would get different coefficients, but the same predictions. Coefficient Estimate (Intercept) 0.5 Republican 0.4 Democratic -0.35 Same story, different numbers, had we omitted Democratic. Coefficient Estimate (Intercept) 0.15 Republican 0.75 Independent 0.35 Given that the results are substantively the same no matter what, does it matter which category we choose to drop? Yes, for the purpose of communicating your results. The omitted category should serve as a meaningful baseline. For this example, all of our three categories are substantively meaningful, so any choice will do. But imagine replacing our part ID variable with a race variable that has the following categories: White Black Hispanic Asian Other You may be tempted to make “Other” the excluded category, so that you obtain a coefficient for each of the specific racial groups. But that’s actually the worst choice possible. The coefficient on the White variable would then represent the difference in probability of voting for Trump between a white voter and a voter in the “other” category—which is hard to interpret. Whereas if we instead omitted the Black category, the coefficient on the White variable would represent the difference between white and black voters. When in doubt, I recommend omitting whichever category is largest in the data. Now let’s introduce covariates into the mix. Consider a regression of Trump vote on party ID (Republican as omitted category) and age, producing the following results. Coefficient Estimate (Intercept) 0.8 Independent -0.4 Democratic -0.75 Age 0.002 Remember what the coefficient of 0.002 on Age means: if we compared one voter to another who was otherwise identical (in this case, same Party ID) except five years older, we would expect the latter voter to have a 1% greater chance of voting for Trump. More specifically, we have three different regression lines—one for each group: \\[ \\begin{aligned} E[\\text{Trump}_n \\,|\\, \\text{Party ID}_n = \\text{R}, \\text{Age}_n] &amp;= 0.8 + 0.002 \\text{Age}_n, \\\\ E[\\text{Trump}_n \\,|\\, \\text{Party ID}_n = \\text{I}, \\text{Age}_n] &amp;= 0.4 + 0.002 \\text{Age}_n, \\\\ E[\\text{Trump}_n \\,|\\, \\text{Party ID}_n = \\text{D}, \\text{Age}_n] &amp;= 0.05 + 0.002 \\text{Age}_n. \\end{aligned} \\] Notice that the slope is the same in each regression line. Only the intercept varies across groups. When we include a categorical variable in a regression model, it’s like allowing the intercept to differ across categories. 8.2 Interaction Terms When political scientists or economists describe their regression results, they will often talk about the marginal effects of different variables. Formally, the marginal effect of the \\(k\\)’th covariate, \\(x_{nk}\\), is \\[ {\\frac{\\partial E[Y_n \\,|\\, \\mathbf{x}_n]}{\\partial x_{nk}}}, \\] the partial derivative of the conditional expectation with respect to the \\(k\\)’th covariate. The marginal effect answers the following question: Suppose we have two observations that differ in the \\(k\\)’th covariate by one unit, but are otherwise identical. How much greater, or less, would we expect the response to be for the observation with the one-unit-greater value of \\(x_{nk}\\)? If we were sure the relationship we were modeling were causal, we could phrase the above question more succinctly. We could ask: Given a one-unit change in the \\(k\\)’th covariate, holding all else fixed, what change in the response should we expect? But we haven’t yet gotten to the point where we can make our claims causal. Hence I will often refer to so-called marginal effects, since I don’t want the “effect” terminology to deceive us into thinking we’re drawing causal inferences. So-called marginal effects are just a nice way to summarize the relationship between individual covariates and the conditional expectation. The bare-bones linear model has the (sometimes appealing, sometimes not) feature that it assumes constant marginal effects. For each covariate \\(x_{nk}\\), we have \\[ {\\frac{\\partial E[Y_n \\,|\\, \\mathbf{x}_n]}{\\partial x_{nk}}} = \\beta_k, \\] the coefficient on that covariate. This encodes two critical assumptions: The marginal effect of the \\(k\\)’th covariate does not depend on the value of any other covariates. The marginal effect of the \\(k\\)’th covariate does not depend on its own value. It is easy to think of scenarios where each of these might be questionable. Imagine a study of individual voters’ choices in U.S. House races, where we model voting for the incumbent as a function of how often the voter goes to church. The marginal effect of religiosity is probably different if the incumbent is a Republican than if the incumbent is a Democrat. Imagine a study of individual voters’ turnout decisions, where we model turnout as a function of the voter’s ideology. Suppose ideology is measured on a 7-point scale, where 1 is most liberal and 7 is most conservative. We know the most ideologically extreme voters are the most likely to turn out. So, all else equal, we’d expect moving from 1 to 2 (very liberal to pretty liberal) to decrease one’s probability of voting, but we’d expect moving from 6 to 7 (pretty conservative to very conservative) to increase one’s probability of voting. Let’s start with the first case, where the (so-called) marginal effect of one variable depends on the value of another variable. To allow for this in our models, we include the product of the two covariates in our model. For example, suppose we are interested in whether the relationship between education and voting for Trump is different between whites and non-whites. We would include three terms in the model (plus an intercept): education, an indicator for white, and their product. \\[ \\text{Trump}_n = \\beta_1 + \\beta_2 \\text{Education}_n + \\beta_3 \\text{White}_n + \\beta_4 (\\text{Education}_n \\times \\text{White}_n) + \\epsilon_n. \\] The so-called marginal effect of education is now \\[ {\\frac{\\partial E[\\text{Trump}_n \\,|\\, \\mathbf{x}_n]}{\\partial \\text{Education}_n}} = \\beta_2 + \\beta_4 \\text{White}_n. \\] This equation tells us three things. \\(\\beta_2\\) is the marginal effect of education for non-white voters. \\(\\beta_4\\) is the difference between the marginal effect of education for white voters and the effect for non-white voters. \\(\\beta_2 + \\beta_4\\) is the marginal effect of education for white voters. Another way to think of it is that we have two regression lines: \\[ \\begin{aligned} E[\\text{Trump}_n \\,|\\, \\text{White}_n = 0, \\text{Education}_n] &amp;= \\beta_1 + \\beta_2 \\text{Education}_n, \\\\ E[\\text{Trump}_n \\,|\\, \\text{White}_n = 1, \\text{Education}_n] &amp;= (\\beta_1 + \\beta_3) + (\\beta_2 + \\beta_4) \\text{Education}_n. \\end{aligned} \\] We saw before that including a categorical variable is like allowing a different intercept for each category. Including an interaction with a categorical variable is like allowing a different slope for each category. At this point, you might ask, why not just run two separate regressions? I can think of at least two reasons not to. You might want to include other covariates whose effects you don’t think are dependent on race (e.g., age). If you ran separate regressions, you would estimate race-dependent effects for every covariate, at a potential loss of efficiency. You might want to formally test the hypothesis that the effect of education is equal for whites and non-whites. This is easiest to do if you have a single model. Next week we will talk about the tools you would need to undertake this sort of test. One frequent source of confusion with interaction terms is whether you need to include lower-order terms in the model. For example, if we are only interested in how the effect of education differs with race, why can’t we just include education and its product with race in the specification? The equations above give you the answer. Leaving the white indicator out of the model is like fixing \\(\\beta_3 = 0\\). This means you’re forcing the regression lines for whites and non-whites to have the same intercept, which there’s no good reason to do. If you’re not yet persuaded on the necessity of including constitutive terms of interactions in your regressions, see Braumoeller (2004). For an example of interaction terms, imagine the following example. Suppose education is measured in years of schooling. Coefficient Estimate (Intercept) 0.3 Education 0.01 White 0.4 Education * White -0.03 We would interpret these in the following way. A hypothetical non-white voter with zero years of education has a 30% chance of voting for Trump. For each additional year of education, the probability of voting for Trump goes up by 1%. A hypothetical white voter with zero years of education has a 70% (0.3 + 0.4) chance of voting for Trump. For each additional year of education, the probability of voting for Trump goes down by 2% (0.01 - 0.03). What about an interaction between two continuous variables? For example, imagine an interaction between age and education in our model of voting for Trump. Coefficient Estimate (Intercept) 0.4 Education -0.02 Age 0.002 Education * Age 0.0002 One simple way to interpret the effect of each variable is to hold the other one fixed at various values. For example, for a 20-year-old, we have \\[ \\begin{aligned} E[\\text{Trump}_n \\,|\\, \\text{Age}_n = 20, \\text{Education}_n] &amp;= 0.44 - 0.16 \\text{Education}_n, \\end{aligned} \\] whereas for an 80-year-old, we have \\[ \\begin{aligned} E[\\text{Trump}_n \\,|\\, \\text{Age}_n = 80, \\text{Education}_n] &amp;= 0.56 - 0.04 \\text{Education}_n. \\end{aligned} \\] These results would seem to imply that (1) older people have a higher baseline probability of voting for Trump, and (2) the magnitude of the negative relationship between education and voting for Trump is weaker for older voters. Always remember: when in doubt, take the partial derivative of \\(Y_n\\) (or, more precisely, its conditional expectation) with respect to the variable you’re interested in. For example, here we have \\[ {\\frac{\\partial E[\\text{Trump}_n \\,|\\, \\mathbf{x}_n]}{\\partial \\text{Education}_n}} = -0.02 + 0.0002 \\text{Age}_n. \\] 8.3 Quadratic and Logarithmic Terms We use interaction terms when the marginal effect of one variable depends on the value of another variable. But sometimes the marginal effect of a variable depends on its own value. The most stark example is a “U-shaped” relationship, such as we expect between ideology and voter turnout. We call this kind of relationship non-monotonic, since it is neither increasing everywhere nor decreasing everywhere. However, even with a monotonic relationship, the marginal effect might depend on the value of the variable. (In other words, while every linear function is monotonic, not every monotonic function is linear.) For example, think of a hockey-stick shaped relationship. If we model the relationship between years of education and voting for Trump as linear, then we impose the assumption that the difference between voters with 16 years of education and 12 years of education (college versus high-school graduates) is the same as between those with 24 and 20 years of education (got the PhD slowly versus got the PhD quickly). Depending on our sample and the goal of our study, that may not be a reasonable assumption (or approximation). We typically use quadratic models for non-monotonic relationships. In the example above, this would entail a regression model like \\[ \\text{Turnout}_n = \\beta_1 + \\beta_2 \\text{Ideology}_n + \\beta_3 \\text{Ideology}_n^2 + \\epsilon_n. \\] Under this model, the marginal effect of Ideology is \\[ {\\frac{\\partial E[\\text{Turnout}_n \\,|\\, \\text{Ideology}_n]}{\\partial \\text{Ideology}_n}} = \\beta_2 + 2 \\beta_3 \\text{Ideology}_n. \\] If \\(\\beta_3\\) is positive, that means the effect of Ideology increases with the value of Ideology, representing a U-shaped relationship. If \\(\\beta_3\\) is negative, that means the effect of Ideology decreases with the value of Ideology, representing an inverse-U-shaped relationship. If \\(beta_3\\) is zero, that means the effect of Ideology is constant. The other main way to model a nonlinear relationship in a single variable is with a logarithmic model. Remember that, in the standard bivariate linear model, \\[ Y_n = \\beta_1 + \\beta_2 x_n + \\epsilon_n, \\] we say that a 1-unit difference in \\(x_n\\) is associated with a \\(\\beta_2\\)-unit difference in \\(Y_n\\). If we were to instead model the natural logarithm of the response, \\[ \\log Y_n = \\beta_1 + \\beta_2 x_n + \\epsilon_n, \\] then we would say that a 1-unit difference in \\(x_n\\) is associated with a \\(\\beta_2\\)-percent difference in \\(Y_n\\). So, for example, if \\(x_n\\) and \\(Y_n\\) are exactly proportional to each other (tripling \\(x_n\\) leads to a tripling of \\(Y_n\\), for example), we would have \\(\\beta_2 = 1\\). Conversely, if we were to model the response as a function of the natural logarithm of the covariate, \\[ Y_n = \\beta_1 + \\beta_2 \\log x_n + \\epsilon_n, \\] then we would say a 1-percent difference in \\(x_n\\) is associated with a \\(\\beta_2\\)-unit difference in \\(Y_n\\). Finally, in a full log-log model, \\[ \\log Y_n = \\beta_1 + \\beta_2 \\log x_n + \\epsilon_n, \\] we would say that a 1-percent difference in \\(x_n\\) is associated with a \\(\\beta_2\\)-percent difference in \\(Y_n\\). How do you decide which logarithmic model, if any, to use? You may let theory be your guide—develop expectations, on the basis of your substantive knowledge, about whether the relevant changes in conditional expectation will be in terms of levels or proportions. Or you may be inductive—make scatterplots of all four possibilities, and choose the specification under which the relationship is closest to linear. A final note on logarithmic models. The logarithm of a number \\(c \\leq 0\\) does not exist. Therefore, a logarithmic model is only appropriate for a strictly positive response/covariate. For non-negative variables that include zeroes, some people try to “fix” this by doing \\(\\log(Y_n + 1)\\), but in this situation it is generally better to follow the procedure advocated by Burbidge, Magee, and Robb (1988). 8.4 Appendix: Nonstandard Specifications in R We will use the following packages: library(&quot;tidyverse&quot;) library(&quot;broom&quot;) library(&quot;forcats&quot;) library(&quot;interplot&quot;) forcats contains convenience functions for factors, which are R’s way of representing categorical variables. interplot is for plotting marginal effects from interactive models. Once again, we will work with the occupational prestige data from the car package. We will also convert it to a tibble, so that it will more closely resemble the kind of data frame we would get had we read it in with read_csv(). library(&quot;car&quot;) data(Prestige) Prestige &lt;- as_tibble(Prestige) Prestige ## # A tibble: 102 × 6 ## education income women prestige census type ## * &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;fctr&gt; ## 1 13.11 12351 11.16 68.8 1113 prof ## 2 12.26 25879 4.02 69.1 1130 prof ## 3 12.77 9271 15.70 63.4 1171 prof ## 4 11.42 8865 9.11 56.8 1175 prof ## 5 14.62 8403 11.68 73.5 2111 prof ## # ... with 97 more rows 8.4.1 Categorical Variables You will notice that the type column of the prestige data is listed as a &lt;fctr&gt;, which stands for factor. A factor is R’s representation of a categorical variable. Let’s take a closer look. Prestige$type ## [1] prof prof prof prof prof prof prof prof prof prof prof prof prof prof ## [15] prof prof prof prof prof prof prof prof prof prof prof prof prof bc ## [29] prof prof wc prof wc &lt;NA&gt; wc wc wc wc wc wc wc wc ## [43] wc wc wc wc wc wc wc wc wc wc &lt;NA&gt; bc wc wc ## [57] wc bc bc bc bc bc &lt;NA&gt; bc bc bc &lt;NA&gt; bc bc bc ## [71] bc bc bc bc bc bc bc bc bc bc bc bc bc bc ## [85] bc bc bc bc bc bc bc bc bc bc bc prof bc bc ## [99] bc bc bc bc ## Levels: bc prof wc This looks kind of like—but isn’t quite—a character variable. The most noticeable difference is that R tells us its levels: the set of categories available. We can extract these directly with the levels() function. levels(Prestige$type) ## [1] &quot;bc&quot; &quot;prof&quot; &quot;wc&quot; This brings us to an important difference between read.csv() (with a dot, the built-in R function) and read_csv() (with an underscore, the tidyverse version). The old way, read.csv(), by default treats any column of character strings as a factor. The tidyverse way, read_csv(), never creates factors by default—you must make them explicitly. To create a factor variable, you can use the factor() function: example_vector &lt;- c(4, 1, 3, 3, 1) factor(example_vector, # vector to convert to factor levels = 1:4, # possible values of the vector labels = c(&quot;ONE&quot;, &quot;two&quot;, &quot;Three&quot;, &quot;FOUR!&quot;)) # label corresponding to each value ## [1] FOUR! ONE Three Three ONE ## Levels: ONE two Three FOUR! Returning to the prestige data, let’s run a regression of occupational prestige on occupational category. fit_type &lt;- lm(prestige ~ type, data = Prestige) fit_type ## ## Call: ## lm(formula = prestige ~ type, data = Prestige) ## ## Coefficients: ## (Intercept) typeprof typewc ## 35.53 32.32 6.72 lm() automatically converts the factor into a set of indicators, and automatically omits one category from the design matrix. In particular, it omits whichever level is listed first (which may not be the first level to appear in the data!). If you want to have a different category omitted, you need to reorder the levels, placing the category you want to omit first. You can do that with fct_relevel() from the forcats package. Let’s make white-collar (wc) the omitted category. Prestige$type &lt;- fct_relevel(Prestige$type, &quot;wc&quot;) levels(Prestige$type) ## [1] &quot;wc&quot; &quot;bc&quot; &quot;prof&quot; fit_type_relevel &lt;- lm(prestige ~ type, data = Prestige) We can confirm by checking out the model fit statistics that which category we omit makes no difference to the overall fit of the model, or the predicted values. glance(fit_type) ## r.squared adj.r.squared sigma statistic p.value df logLik AIC ## 1 0.69763 0.69126 9.4986 109.59 2.1168e-25 3 -358.15 724.29 ## BIC deviance df.residual ## 1 734.63 8571.3 95 glance(fit_type_relevel) # Should be the same ## r.squared adj.r.squared sigma statistic p.value df logLik AIC ## 1 0.69763 0.69126 9.4986 109.59 2.1168e-25 3 -358.15 724.29 ## BIC deviance df.residual ## 1 734.63 8571.3 95 # fitted() extracts the fitted value for each observation all.equal(fitted(fit_type), fitted(fit_type_relevel)) ## [1] TRUE One more thing—it’s rare you should need to do this, but you can use model.matrix() to extract the design matrix for a fitted regression model. X &lt;- model.matrix(fit_type) dim(X) ## [1] 98 3 head(X) ## (Intercept) typeprof typewc ## gov.administrators 1 1 0 ## general.managers 1 1 0 ## accountants 1 1 0 ## purchasing.officers 1 1 0 ## chemists 1 1 0 ## physicists 1 1 0 8.4.2 Interaction Terms The syntax for an interactive model is pretty intuitive. Let’s look at the joint effect of education and income on occupational prestige. fit_interactive &lt;- lm(prestige ~ education * income, data = Prestige) fit_interactive ## ## Call: ## lm(formula = prestige ~ education * income, data = Prestige) ## ## Coefficients: ## (Intercept) education income education:income ## -2.21e+01 5.37e+00 3.94e-03 -1.96e-04 Notice that lm() automatically included the lower-order terms for us, so we didn’t have to remember which terms to put in. This is particularly useful when you are interacting with a categorical variable that has many categories, or when you are including higher-order interactions. If you want to plot the (so-called) marginal effect of education as a function of income, you can use the handy function from the interplot package. interplot(fit_interactive, var1 = &quot;education&quot;, var2 = &quot;income&quot;) We see that the marginal effect of education on prestige is high for low-income occupations, but almost nil for an occupation that earns around $25,000/year. (The bars represent a confidence interval—of course, we haven’t done any inferential statistics yet.) 8.4.3 Quadratic and Logarithmic Models The syntax for a quadratic model is a bit weird. You would think you could use a formula like y ~ x + x^2, but that won’t work. Instead, you have to write y ~ x + I(x^2), as in the following example. fit_quad &lt;- lm(prestige ~ education + I(education^2) + income, data = Prestige) fit_quad ## ## Call: ## lm(formula = prestige ~ education + I(education^2) + income, ## data = Prestige) ## ## Coefficients: ## (Intercept) education I(education^2) income ## 10.97951 0.77477 0.15373 0.00128 The easiest way to visualize the results of a quadratic model is to create a synthetic dataset, where you vary the relevant variable across its range while holding all the other variables fixed at the same value. Then plug the synthetic dataset into the model to get predicted values. synthetic_data &lt;- data_frame( education = seq(min(Prestige$education), max(Prestige$education), length.out = 100), income = mean(Prestige$income) ) synthetic_data &lt;- augment(fit_quad, newdata = synthetic_data) head(synthetic_data) ## education income .fitted .se.fit ## 1 6.3800 6797.9 30.855 2.2842 ## 2 6.4769 6797.9 31.122 2.1959 ## 3 6.5737 6797.9 31.391 2.1104 ## 4 6.6706 6797.9 31.663 2.0280 ## 5 6.7675 6797.9 31.939 1.9486 ## 6 6.8643 6797.9 32.217 1.8723 ggplot(synthetic_data, aes(x = education, y = .fitted)) + geom_line() In this case, within the range spanned by the sample data, the estimated quadratic relationship is only barely nonlinear. Finally, to run a logarithmic model, just put log() around the variables you want to log. fit_log &lt;- lm(log(prestige) ~ log(income) + education, data = Prestige) fit_log ## ## Call: ## lm(formula = log(prestige) ~ log(income) + education, data = Prestige) ## ## Coefficients: ## (Intercept) log(income) education ## 0.3373 0.2991 0.0789 To visualize the resulting relationship, you can use the same technique as for quadratic models. References "],
["inference.html", "9 Drawing Inferences 9.1 The Basics of Hypothesis Testing 9.2 Variance of OLS 9.3 Single Variable Hypotheses 9.4 Multiple Variable Hypotheses 9.5 Appendix: Full Derivation of OLS Variance 9.6 Appendix: Regression Inference in R", " 9 Drawing Inferences You can think of regression as a descriptive statistic or data reduction method—a simple way to summarize trends and relationships in multivariate data. But for better or worse, most social scientists view regression as a tool for hypothesis testing. This week, we will learn what it is that we’re going when we gaze at the “stars” that accompany our regression output. 9.1 The Basics of Hypothesis Testing Remember the general procedure for testing against a null hypothesis. Choose a test statistic and significance level. Derive the sampling distribution of the test statistic under the null hypothesis. Calculate the value of the test statistic for our sample. Compare the sample test statistic to the sampling distribution under the null hypothesis. If the probability of obtaining a result as least as extreme as ours is at or below the significance level, reject the null hypothesis. Imagine the null hypothesis is true. Given that, imagine 100 labs run independent tests of the null hypothesis, each using a significance level of 0.05. If they follow the procedure above, on average 5 of the labs will reject the null hypothesis, and 95 will fail to reject the null hypothesis. What if the null hypothesis is false? What percentage of the labs will falsely reject it? That’s the power of the test, and it depends on a number of factors: how far off the null hypothesis is, what size sample each lab is drawing, and the significance level. Before we get into hypothesis tests for regression, let’s refresh ourselves on how we draw inferences from a random sample about the population mean. Suppose we have a sequence of \\(N\\) i.i.d. draws of the random variable \\(X\\), which we will denote \\(X_1, \\ldots, X_N\\), and we are interested in testing the null hypothesis \\[ H_0 : E[X] = \\mu_0. \\] Let \\(\\bar{X}\\) denote the sample mean and \\(S_X\\) denote the sample standard deviation. Define the \\(t\\) statistic as \\[ t = \\frac{\\bar{X} - \\mu_0}{S_X / \\sqrt{N}}. \\] The denominator of the \\(t\\) statistic is the standard error—our estimate of the standard deviation of the sampling distribution under the null hypothesis. The greater the standard error, the more the statistic varies across samples, and thus the less reliable it is in any given sample. Naturally enough, our standard errors decrease with our sample size; the more data we have, the more reliably we can draw inferences. If \\(X\\) is known to be normally distributed—an assumption that, in the realms political scientists deal with, is usually implausible—then the sampling distribution of \\(t\\) under the null hypothesis is \\(t_{N - 1}\\), the Student’s \\(t\\) distribution with \\(N - 1\\) degrees of freedom. If \\(X\\) is not known to be normally distributed, but our sample size is “large” (in practice, \\(N \\geq 30\\)), we can rely on the Central Limit Theorem. As \\(N \\to \\infty\\), the distribution of \\(t\\) under the null hypothesis is approximately \\(N(0, 1)\\), the normal distribution with mean zero and variance one. 9.2 Variance of OLS Now let us return to the world of the linear model, \\[ \\mathbf{Y} = \\mathbf{X} \\beta + \\epsilon, \\] and the OLS estimator of \\(\\beta\\), \\[ \\hat{\\beta}_{\\text{OLS}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{Y}. \\] In order to draw inferences on OLS results, the first thing we need to know is the variance of the OLS estimator. You will remember from Stat 1 that the variance of the sample mean, \\(\\bar{X}\\), is \\[ V[\\bar{X}] = \\frac{V[X]}{N}, \\] whose square root ends up in the denominator of the \\(t\\)-test statistic. We will do something similar for OLS. Throughout this week, we will maintain the following two assumptions on the error term: Strict exogeneity: \\(E[\\epsilon \\,|\\, \\mathbf{X}] = \\mathbf{0}\\). Spherical errors: \\(V[\\epsilon \\,|\\, \\mathbf{X}] = \\sigma^2 \\mathbf{I}_N\\), where \\(\\sigma^2 &gt; 0\\). Without the first assumption, OLS is hopeless to begin with. Without the second assumption, OLS is unbiased and consistent, but not efficient. In a couple of weeks, we will discuss how to draw inferences in the presence of non-spherical errors. The OLS estimator is a \\(K \\times 1\\) vector, so its variance won’t be a single number—it will be a \\(K \\times K\\) matrix, \\[ V[\\hat{\\beta}] = \\begin{bmatrix} V[\\hat{\\beta}_1] &amp; {\\mathop{\\rm Cov}\\nolimits}[\\hat{\\beta}_1, \\hat{\\beta}_2] &amp; \\cdots &amp; {\\mathop{\\rm Cov}\\nolimits}[\\hat{\\beta}_1, \\hat{\\beta}_K] \\\\ {\\mathop{\\rm Cov}\\nolimits}[\\hat{\\beta}_2, \\hat{\\beta}_1] &amp; V[\\hat{\\beta}_2] &amp; \\cdots &amp; {\\mathop{\\rm Cov}\\nolimits}[\\hat{\\beta}_2, \\hat{\\beta}_K] \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ {\\mathop{\\rm Cov}\\nolimits}[\\hat{\\beta}_K, \\hat{\\beta}_1] &amp; {\\mathop{\\rm Cov}\\nolimits}[\\hat{\\beta}_K, \\hat{\\beta}_2] &amp; \\cdots &amp; V[\\hat{\\beta}_K] \\\\ \\end{bmatrix}. \\] Specifically, the variance of the OLS estimator (treating the covariates as fixed) is \\[ V[\\hat{\\beta} \\,|\\, \\mathbf{X}] = \\sigma^2 (\\mathbf{X}^\\top \\mathbf{X})^{-1}. \\] See the appendix to this chapter—or any graduate-level econometrics textbook—for how we derive this result. If we knew \\(\\sigma^2\\), the variance of the error term, then we could just use the above formula to draw inferences. Realistically, though, we will need to estimate \\(\\sigma^2\\). We will do so using the residual variance, \\[ \\hat{\\sigma}^2 = \\frac{\\sum_n (Y_n - \\mathbf{x}_n \\cdot \\hat{\\beta})^2}{N - K} = \\frac{\\text{SSE}}{N - K}. \\] Why do we divide by \\(N - K\\)? Remember that when we estimate the variance of a random variable, we divide the squared deviations from the mean by \\(N - 1\\) to correct for the degree of freedom we used to estimate the sample mean. The resulting estimator is unbiased. Similarly, when estimating the residual variance of a linear regression model, we need to correct for the \\(K\\) degrees of freedom we used to estimate the model coefficients. Hence we must divide by \\(N - K\\) in order for \\(\\hat{\\sigma}^2\\) to be unbiased. Under the spherical error assumption, our estimate of the variance of OLS will therefore be the \\(K \\times K\\) matrix \\[ \\hat{\\Sigma} = \\hat{\\sigma}^2 (\\mathbf{X}^\\top \\mathbf{X})^{-1}. \\] A very important note. If the errors are not spherical, this formula produces a biased and inconsistent estimate of the sampling variability of OLS. This is true even though the OLS estimate itself is unbiased and consistent. In other words, if we use OLS in the presence of non-spherical errors: Our estimates will not be systematically biased away from the population parameter, and the probability of our estimate being any meaningful distance away from the population parameter goes to zero as our sample size increases. Our hypothesis tests will not perform as advertised—typically, they will lead us to reject the null hypothesis more often than we should—and this problem does not go away as our sample size increases. For the remainder of this week, we will proceed under the assumption of spherical errors. In a couple of weeks, we will discuss how to draw inferences appropriately when this assumption fails to hold. 9.3 Single Variable Hypotheses Consider a null hypothesis about the population value of a single coefficient, of the form \\[ H_0 : \\beta_k = b, \\] where \\(b\\) is a fixed constant. Usually, though not always, political scientists concern themselves with null hypotheses of the form \\(\\beta_k = 0\\); i.e., the \\(k\\)’th variable has zero (so-called) marginal effect on the response. We will test this hypothesis using the familiar \\(t\\)-statistic. The estimated standard error of \\(\\hat{\\beta}_k\\) is \\[ {\\mathop{\\rm SE}\\nolimits}(\\hat{\\beta}_k) = \\sqrt{\\hat{\\Sigma}_{kk}}, \\] where \\(\\hat{\\Sigma}_{kk}\\) denotes the \\(k\\)’th element of the diagonal of \\(\\hat{\\Sigma} = \\hat{\\sigma}^2 (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\). The “standard errors” that appear alongside your regression output are calculating by taking the square root of the diagonal of this matrix. The \\(t\\) statistic for the test of the null hypothesis \\(H_0\\) is in the familiar “estimate divided by standard error” form, \\[ t = \\frac{\\hat{\\beta}_k - b}{{\\mathop{\\rm SE}\\nolimits}(\\hat{\\beta}_k)} = \\frac{\\hat{\\beta}_k - b}{\\sqrt{\\hat{\\Sigma}_{kk}}}. \\] Our mode of inference from there depends on the sample size and on whether we are willing to make a normality assumption. If \\(\\epsilon\\) is normally distributed, then the sampling distribution of our test statistic is \\(t_{N - K}\\), the \\(t\\) distribution with \\(N - K\\) degrees of freedom. As our sample size grows large, the sampling distribution of our test statistic is approximately \\(N(0, 1)\\), the normal distribution with mean zero and variance one. This follows from the Central Limit Theorem, and it holds even if \\(\\epsilon\\) is not normally distributed. So if you have a small sample, the validity of the standard hypothesis test depends on a normality assumption that may or may not be palatable, depending on the circumstances. Other techniques exist for this situation, but they are beyond the scope of this course. Of course, if your sample is so small that you need to resort to non-standard hypothesis testing techniques in order to draw appropriate inferences, you should probably go back to the drawing board on your study design. Regardless of your sample size, regression software will compare your test statistics to \\(t_{N - K}\\) to calculate \\(p\\)-values and the results of hypothesis tests. This is innocuous even if you don’t assume normality, since if \\(N\\) is large the \\(t_{N - K}\\) distribution is approximately the same as the \\(N(0, 1)\\) distribution (with infinitesimally fatter tails). Our method of constructing confidence intervals for a single parameter is also analogous to what we do with the sample mean. Let \\(z_{\\alpha}\\) be the critical value of the sampling distribution of our test statistic for our chosen significance level \\(\\alpha\\). For example, the critical value of \\(N(0, 1)\\) for significance \\(\\alpha = 0.05\\) is \\(z_{\\alpha} = 1.96\\). Then the \\((1 - \\alpha)\\)-confidence interval around \\(\\hat{\\beta}_k\\) is \\[ {\\mathop{\\rm CI}\\nolimits}_{1 - \\alpha}(\\hat{\\beta}_k) = [\\hat{\\beta}_k - z_{\\alpha} {\\mathop{\\rm SE}\\nolimits}(\\hat{\\beta}_k), \\hat{\\beta}_k + z_{\\alpha} {\\mathop{\\rm SE}\\nolimits}(\\hat{\\beta}_k)]. \\] 9.4 Multiple Variable Hypotheses It is common, especially (though not exclusively) when working with categorical variables or higher-order terms, to have hypotheses involving multiple variables. For example, think of our model from last week, \\[ \\text{Trump}_n = \\beta_1 + \\beta_2 \\text{Independent}_n + \\beta_3 \\text{Democratic}_n + \\beta_4 \\text{Age}_n + \\epsilon_n. \\] Remember that \\(\\beta_2\\) denotes the expected difference between Independents and Republicans (the omitted category) of the same age in their propensity to vote for Trump, and \\(\\beta_3\\) denotes the expected difference between Democrats and Republicans of the same age. If our null hypothesis were that Independents and Republicans of the same age had the same chance of voting for Trump, we would state that as \\[ H_0 : \\beta_2 = 0. \\] But what about the null hypothesis were that Independents and Democrats had the same chance of voting for Trump? We would have to phrase that in terms of multiple coefficients, \\[ H_0 : \\beta_2 = \\beta_3, \\] or equivalently, \\[ H_0 : \\beta_2 - \\beta_3 = 0. \\] Or what if our null hypothesis were that party identification made no difference at all? That would mean Independents and Democrats are both no different than Republicans on average, or in our model notation, \\[ H_0 : \\left\\{ \\begin{aligned} \\beta_2 &amp;= 0, \\\\ \\beta_3 &amp;= 0. \\end{aligned} \\right. \\] Each of these, including the simple single-variable hypothesis, is a linear system in \\(\\beta\\). In other words, we can write each of these hypotheses in the form \\[ H_0 : \\mathbf{R} \\beta - \\mathbf{c} = \\mathbf{0}, \\] where \\(\\mathbf{R}\\) is a fixed \\(r \\times K\\) matrix (where \\(r\\) is the number of restrictions we intend to test) and \\(\\mathbf{c}\\) is a fixed \\(r \\times 1\\) vector. Perhaps the easiest way to test a hypothesis of this form is the Wald test. We form the test statistic \\[ W = (\\mathbf{R} \\beta - \\mathbf{c})^\\top (\\mathbf{R} \\hat{\\Sigma} \\mathbf{R}^\\top)^{-1} (\\mathbf{R} \\beta - \\mathbf{c}), \\] which, despite all the matrices involved, works out to be a scalar. Under \\(H_0\\), the asymptotic sampling distribution of \\(W\\) is \\(\\chi^2_r\\), the chi-squared distribution with \\(r\\) degrees of freedom.22 Regression software like R will usually report an \\(F\\) statistic, since the exact (not asymptotic) distribution of \\(W\\) follows an \\(F\\) distribution in the special case of normal residual error. The Wald test is not just an aggregation of the individual \\(t\\) tests of the coefficients. Two coefficients might each individually be statistically insignificant, yet the Wald test may lead us to reject the null hypothesis that both are zero. Conversely, one of a group of coefficients might be statistically significant, and yet the Wald test may not have us reject the null hypothesis that all are zero. We already saw a couple of examples of how to use the Wald test with a model with a categorical variable. Let’s also quickly consider its use in some other models with less-common specifications. Imagine an interactive model, \\[ Y_n = \\beta_1 + \\beta_2 X_n + \\beta_3 Z_n + \\beta_4 (X_n \\times Z_n) + \\epsilon_n. \\] The interaction term captures how the (so-called) marginal effect of \\(X_n\\) depends on the value of \\(Z_n\\), and vice versa. If your null hypothesis is that the marginal effect of \\(X_n\\) does not depend on \\(Z_n\\), you could use perform a \\(t\\) test of \\[ H_0 : \\beta_4 = 0. \\] But what if your null hypothesis is that the marginal effect of \\(X_n\\) is always zero, regardless of the value of \\(Z_n\\)? Some people make the unfortunate mistake of testing this via the null hypothesis \\[ H_0 : \\beta_2 = 0, \\] but that only means the marginal effect of \\(X_n\\) is zero when \\(Z_n = 0\\). What you want is the composite null \\[ H_0 : \\left\\{ \\begin{aligned} \\beta_2 &amp;= 0, \\\\ \\beta_4 &amp;= 0, \\end{aligned} \\right. \\] or, in matrix form, \\[ \\begin{bmatrix} 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\beta_4 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}. \\] Similarly, think of the quadratic model, \\[ Y_n = \\beta_1 + \\beta_2 X_n + \\beta_3 X_n^2 + \\epsilon_n. \\] The null hypothesis that the (so-called) marginal effect of \\(X_n\\) is constant is equivalent to \\[ H_0 : \\beta_3 = 0. \\] But if we wanted to test against the null hypothesis that the marginal effect of \\(X_n\\) is always zero, we would have to use a composite null, \\[ H_0 : \\left\\{ \\begin{aligned} \\beta_2 &amp;= 0, \\\\ \\beta_3 &amp;= 0. \\end{aligned} \\right. \\] 9.5 Appendix: Full Derivation of OLS Variance We will assume strict exogeneity, \\[ E [\\epsilon \\,|\\, \\mathbf{X}] = \\mathbf{0}, \\] and spherical errors, \\[ V [\\epsilon \\,|\\, \\mathbf{X}] = E [\\epsilon \\epsilon^\\top \\,|\\, \\mathbf{X}] = \\sigma^2 \\mathbf{I}. \\] A useful thing to know is that since \\(\\mathbf{X}^\\top \\mathbf{X}\\) is symmetric, so is its inverse: \\[ [(\\mathbf{X}^\\top \\mathbf{X})^{-1}]^\\top = (\\mathbf{X}^\\top \\mathbf{X})^{-1}. \\] Let’s start by deriving the variance from our formula for a vector-valued random variable, \\[ V[C] = E \\left[ (C - E[C]) (C - E[C])^\\top \\right]. \\] For the OLS estimator \\(\\hat{\\beta}\\), we have \\[ \\begin{aligned} V[\\hat{\\beta} \\,|\\, \\mathbf{X}] &amp;= E \\left[ \\left(\\hat{\\beta} - E[\\hat{\\beta}]\\right) \\left(\\hat{\\beta} - E[\\hat{\\beta}]\\right)^\\top \\,|\\, \\mathbf{X} \\right] \\\\ &amp;= E \\left[ \\left(\\hat{\\beta} - \\beta\\right) \\left(\\hat{\\beta} - \\beta\\right)^\\top \\,|\\, \\mathbf{X} \\right] \\\\ &amp;= E \\left[ \\hat{\\beta} \\hat{\\beta}^\\top - 2 \\beta \\hat{\\beta}^\\top + \\beta \\beta^\\top \\,|\\, \\mathbf{X} \\right] \\\\ &amp;= E \\left[ \\hat{\\beta} \\hat{\\beta}^\\top \\,|\\, \\mathbf{X} \\right] - 2 \\beta E \\left[ \\hat{\\beta}^\\top \\,|\\, \\mathbf{X} \\right] + \\beta \\beta^\\top \\\\ &amp;= E \\left[ (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{Y} \\mathbf{Y}^\\top \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\,|\\, \\mathbf{X} \\right] - \\beta \\beta^\\top \\\\ &amp;= (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top E \\left[ \\mathbf{Y} \\mathbf{Y}^\\top \\,|\\, \\mathbf{X} \\right] \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1} - \\beta \\beta^\\top. \\end{aligned} \\] Since \\(\\mathbf{Y} = \\mathbf{X} \\beta + \\epsilon\\), we have \\[ \\begin{aligned} E \\left[ \\mathbf{Y} \\mathbf{Y}^\\top \\,|\\, \\mathbf{X} \\right] &amp;= E \\left[ (\\mathbf{X} \\beta + \\epsilon) (\\mathbf{X} \\beta + \\epsilon)^\\top \\,|\\, \\mathbf{X} \\right] \\\\ &amp;= E \\left[ \\mathbf{X} \\beta \\beta^\\top \\mathbf{X}^\\top + 2 \\epsilon \\beta^\\top \\mathbf{X}^\\top + \\epsilon \\epsilon^\\top \\,|\\, \\mathbf{X} \\right] \\\\ &amp;= \\mathbf{X} \\beta \\beta^\\top \\mathbf{X}^\\top + 2 \\underbrace{E[\\epsilon \\,|\\, \\mathbf{X}]}_{= \\mathbf{0}} \\beta^\\top \\mathbf{X}^\\top + E[\\epsilon \\epsilon^\\top \\,|\\, \\mathbf{X}] \\\\ &amp;= \\mathbf{X} \\beta \\beta^\\top \\mathbf{X}^\\top + \\sigma^2 \\mathbf{I}. \\end{aligned} \\] Continuing from above, we have \\[ \\begin{aligned} V[\\hat{\\beta} \\,|\\, \\mathbf{X}] &amp;= (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top E \\left[ \\mathbf{Y} \\mathbf{Y}^\\top \\,|\\, \\mathbf{X} \\right] \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1} - \\beta \\beta^\\top \\\\ &amp;= (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top ( \\mathbf{X} \\beta \\beta^\\top \\mathbf{X}^\\top + \\sigma^2 \\mathbf{I} ) \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1} - \\beta \\beta^\\top \\\\ &amp;= (\\mathbf{X}^\\top \\mathbf{X})^{-1} (\\mathbf{X}^\\top \\mathbf{X}) \\beta \\beta^\\top (\\mathbf{X}^\\top \\mathbf{X}) (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\\\ &amp;\\quad + \\sigma^2 (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1} - \\beta \\beta^\\top \\\\ &amp;= \\beta \\beta^\\top + \\sigma^2 (\\mathbf{X}^\\top \\mathbf{X})^{-1} - \\beta \\beta^\\top \\\\ &amp;= \\sigma^2 (\\mathbf{X}^\\top \\mathbf{X})^{-1}. \\end{aligned} \\] A slightly easier way to get there would be to apply two of the helpful properties of variance. You’ll remember from the study of scalar-valued random variables that, for a random variable \\(A\\) and scalars \\(c\\) and \\(d\\), \\[ \\begin{aligned} V[c A] &amp;= c^2 V[A], \\\\ V[A + d] &amp;= V[A]. \\end{aligned} \\] Similarly, for an \\(m \\times 1\\) vector random variable \\(\\mathbf{A}\\), a fixed \\(n \\times m\\) matrix \\(\\mathbf{C}\\), and a fixed \\(m \\times 1\\) vector \\(\\mathbf{d}\\), we have \\[ \\begin{aligned} V[\\mathbf{C} \\mathbf{A}] &amp;= \\mathbf{C} V[\\mathbf{A}] \\mathbf{C}^\\top, \\\\ V[\\mathbf{A} + \\mathbf{d}] &amp;= V[\\mathbf{A}]. \\end{aligned} \\] Consequently, \\[ \\begin{aligned} V \\left[ \\hat{\\beta} \\,|\\, \\mathbf{X} \\right] &amp;= V[ (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{Y} \\,|\\, \\mathbf{X} ] \\\\ &amp;= (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top V [\\mathbf{Y} \\,|\\, \\mathbf{X}] \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\\\ &amp;= (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top V [\\mathbf{X} \\beta + \\epsilon \\,|\\, \\mathbf{X}] \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\\\ &amp;= (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top V [\\epsilon \\,|\\, \\mathbf{X}] \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\\\ &amp;= (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top (\\sigma^2 \\mathbf{I}) \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\\\ &amp;= \\sigma^2 (\\mathbf{X}^\\top \\mathbf{X})^{-1}. \\end{aligned} \\] 9.6 Appendix: Regression Inference in R As usual, we will rely on the tidyverse and broom packages. We will also be using the car package, not only for data but also for its hypothesis testing functions. library(&quot;tidyverse&quot;) library(&quot;broom&quot;) library(&quot;car&quot;) Using the Prestige data, let us regress occupational prestige on the type of occupation (blue collar, white collar, or professional) and its average income and education. data(&quot;Prestige&quot;, package = &quot;car&quot;) fit &lt;- lm(prestige ~ type + education + income, data = Prestige) summary() prints the “regression table” containing the following information: Estimate of each coefficient. Estimated standard error of each coefficient estimate. \\(t\\) statistic for each coefficient estimate, for the test against the null hypothesis that the population value of the corresponding coefficient is zero (\\(H_0 : \\beta_k = 0\\)). \\(p\\) value (two-tailed)23 for the aforementioned hypothesis test. summary(fit) ## ## Call: ## lm(formula = prestige ~ type + education + income, data = Prestige) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.953 -4.449 0.168 5.057 18.632 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.622929 5.227525 -0.12 0.91 ## typeprof 6.038971 3.866855 1.56 0.12 ## typewc -2.737231 2.513932 -1.09 0.28 ## education 3.673166 0.640502 5.73 1.2e-07 ## income 0.001013 0.000221 4.59 1.4e-05 ## ## Residual standard error: 7.09 on 93 degrees of freedom ## (4 observations deleted due to missingness) ## Multiple R-squared: 0.835, Adjusted R-squared: 0.828 ## F-statistic: 118 on 4 and 93 DF, p-value: &lt;2e-16 This is useful for a quick check on the output, but not so much if you want to use the standard errors for further calculations (e.g., making a plot). To extract the standard errors, use the tidy() function from broom. This returns a data frame whose columns correspond to the summary() output. fit_results &lt;- tidy(fit) fit_results ## term estimate std.error statistic p.value ## 1 (Intercept) -0.6229292 5.22752549 -0.11916 9.0540e-01 ## 2 typeprof 6.0389707 3.86685510 1.56173 1.2175e-01 ## 3 typewc -2.7372307 2.51393240 -1.08882 2.7904e-01 ## 4 education 3.6731661 0.64050162 5.73483 1.2052e-07 ## 5 income 0.0010132 0.00022092 4.58628 1.4049e-05 fit_results$std.error ## [1] 5.22752549 3.86685510 2.51393240 0.64050162 0.00022092 To extract the full (estimated) variance matrix, use the vcov() function. vcov(fit) ## (Intercept) typeprof typewc education income ## (Intercept) 2.7327e+01 1.6637e+01 7.39848733 -3.1965e+00 9.9963e-05 ## typeprof 1.6637e+01 1.4953e+01 6.79707308 -2.1239e+00 -4.9843e-06 ## typewc 7.3985e+00 6.7971e+00 6.31985613 -1.1062e+00 1.3108e-04 ## education -3.1965e+00 -2.1239e+00 -1.10618421 4.1024e-01 -4.3335e-05 ## income 9.9963e-05 -4.9843e-06 0.00013108 -4.3335e-05 4.8805e-08 Luckily, you shouldn’t often need to extract the individual standard errors or the variance matrix. R has convenience functions to perform most of the calculations you would care about. To obtain confidence intervals for the regression coefficients, use the confint() function. confint(fit) ## 2.5 % 97.5 % ## (Intercept) -1.1004e+01 9.7579004 ## typeprof -1.6398e+00 13.7177785 ## typewc -7.7294e+00 2.2549408 ## education 2.4013e+00 4.9450753 ## income 5.7449e-04 0.0014519 confint(fit, level = 0.99) # Changing the confidence level ## 0.5 % 99.5 % ## (Intercept) -1.4370e+01 13.1240626 ## typeprof -4.1298e+00 16.2077638 ## typewc -9.3482e+00 3.8737381 ## education 1.9888e+00 5.3575138 ## income 4.3224e-04 0.0015941 confint(fit, &quot;education&quot;) # Only for one coefficient ## 2.5 % 97.5 % ## education 2.4013 4.9451 You may also be interested in testing against null hypotheses other than each individual coefficient being zero. This is where the linearHypothesis() function from the car package comes in. For example, suppose we wanted to test against the null hypothesis that the population coefficient on education is 3. linearHypothesis(fit, &quot;education = 3&quot;) ## Linear hypothesis test ## ## Hypothesis: ## education = 3 ## ## Model 1: restricted model ## Model 2: prestige ~ type + education + income ## ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 94 4737 ## 2 93 4681 1 55.6 1.1 0.3 We care about the last two columns of the bottom row. F gives us the test statistic,24 and Pr(&gt;F) gives us the associated \\(p\\)-value. Here our \\(p\\)-value is 0.3, so we wouldn’t reject the null hypothesis that the population coefficient on education is 3. Two quick notes on the linearHypothesis() function: Make sure to place your hypothesis in quotes. (Or if you have multiple hypotheses, that you use a vector of quoted strings.) The function will not work if you run something like linearHypothesis(fit, education = 3). Make sure the name of the coefficient(s) you’re testing are exactly the same as in the regression output. This requires particular care when you’re dealing with factor variables, interactions, or quadratic terms. Of course, for a univariate hypothesis test like this one, we could have just used the confidence interval to figure out the answer. The real value of linearHypothesis() comes in simultaneously testing hypotheses about multiple coefficients—i.e., the Wald test. For example, let’s test the null hypothesis that the population coefficients on white-collar and professional are the same. linearHypothesis(fit, &quot;typewc = typeprof&quot;) ## Linear hypothesis test ## ## Hypothesis: ## - typeprof + typewc = 0 ## ## Model 1: restricted model ## Model 2: prestige ~ type + education + income ## ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 94 5186 ## 2 93 4681 1 505 10 0.0021 We would reject this null hypothesis except under particularly stringent significance levels (less than 0.002). What about the composite hypothesis that the population coefficients on white-collar and professional both equal zero? To test this, we pass a vector of hypotheses. linearHypothesis(fit, c(&quot;typewc = 0&quot;, &quot;typeprof = 0&quot;)) ## Linear hypothesis test ## ## Hypothesis: ## typewc = 0 ## typeprof = 0 ## ## Model 1: restricted model ## Model 2: prestige ~ type + education + income ## ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 95 5272 ## 2 93 4681 2 591 5.87 0.004 The \\(p\\)-value corresponding to this hypothesis test is 0.004. This illustrates a key feature of composite hypothesis tests. You’ll remember from the original regression output that neither of the occupational indicators were significant on their own. summary(fit) ## ## Call: ## lm(formula = prestige ~ type + education + income, data = Prestige) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.953 -4.449 0.168 5.057 18.632 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.622929 5.227525 -0.12 0.91 ## typeprof 6.038971 3.866855 1.56 0.12 ## typewc -2.737231 2.513932 -1.09 0.28 ## education 3.673166 0.640502 5.73 1.2e-07 ## income 0.001013 0.000221 4.59 1.4e-05 ## ## Residual standard error: 7.09 on 93 degrees of freedom ## (4 observations deleted due to missingness) ## Multiple R-squared: 0.835, Adjusted R-squared: 0.828 ## F-statistic: 118 on 4 and 93 DF, p-value: &lt;2e-16 So, operating at conventional significant levels, we would: Fail to reject the null that the coefficient on professional is zero Fail to reject the null that the coefficient on white-collar is zero Reject the null that the coefficients on both are zero This seems contradictory, but it just illustrates the limits of drawing inferences from a finite sample. What the results of these hypothesis tests are telling us is that we have enough information to conclude that there are differences in prestige across occupational categories, holding average education and income fixed. However, we do not have enough information to identify exactly where those differences are coming—i.e., exactly which of the two categories it is that differs from the omitted baseline. You will notice that, for a single-variable hypothesis \\(H_0 : \\beta_k = b\\), the Wald statistic reduces to the square of the \\(t\\) statistic. Since the asymptotic distribution of the \\(t\\) statistic is standard normal under the null hypothesis, it follows that the asymptotic distribution of its square is \\(\\chi^2_1\\) under the null hypothesis.↩ One-tailed tests, while unproblematic in theory, in practice are usually a signal that the two-tailed test was insignificant and the author is fudging it. Don’t send a bad signal; always use two-tailed tests.↩ Why an \\(F\\) statistic instead of a \\(t\\) statistic? If the random variable \\(Z\\) has a \\(t\\) distribution with \\(n\\) degrees of freedom, then \\(Z^2\\) has an \\(F\\) distribution with \\(1,n\\) degrees of freedom. The \\(F\\) statistic generalizes better to the case of multiple hypotheses.↩ "],
["crisis.html", "10 The Statistical Crisis in Science 10.1 Publication Bias 10.2 \\(p\\)-Hacking 10.3 What to Do 10.4 Appendix: R Simulation", " 10 The Statistical Crisis in Science As a general rule, I do not believe the statistical results reported in political science publications. More to the point, absent compelling evidence to the contrary, I assume that: Reported effect sizes are biased upward in magnitude. The marginal effect of the key variable(s) in the population is likely less dramatic than what is reported. Reported \\(p\\)-values are biased downward. The probability of making a Type I error, in case the null hypothesis were false and one followed the actual procedure that led to the reported results, is greater than what is reported. My position is not one of blind skepticism. It follows, perhaps ironically, from empirical research showing that far more than 5 percent of studies do not replicate upon re-testing (Open Science Collaboration 2015). Ioannidis (2005) puts it bluntly: “Most published research findings are false.” Today we’ll discuss why. I’ll follow Young, Ioannidis, and Al-Ubaydli (2008)’s economic analogy. Consider scientific publication as an economic activity, where researchers “sell” findings to journals in exchange for prestige.25 The demand-side problem is that journals will only “buy” statistically significant findings. Even absent any effects on author behavior, this practice makes published findings a biased sample of actual findings. But of course there are effects on author behavior. The supply-side problem is that authors try to produce “statistically significant” findings instead of scientifically sound findings. There is far more out there on the replication crisis in science than we can cover in a week. Sanjay Srivastava’s faux syllabus, “Everything is fucked”, provides a more comprehensive treatment. 10.1 Publication Bias If you open up an issue of any empirically oriented political science journal, you will not read many abstracts that conclude “We were unable to reject the null hypothesis of no effect.” You probably won’t see any. The prevailing attitude of reviewers and editors is that only significant results are interesting and only interesting results are worth publishing—so only significant results get published. Consequently, published empirical findings are not a representative sample of all empirical findings. Andrew Gelman calls this the statistical significance filter: the publication process only reveals the findings of some studies, namely those that achieve statistical significance. If you draw your beliefs from scientific journals (particularly prestigious ones, as Ioannidis (2008) notes), you will end up with some false ideas about how the world works. Some of these beliefs will be Type I errors: you will reject null hypotheses that are true. Suppose there is a treatment \\(T\\) that has no effect on an outcome \\(Y\\), and 100 labs run separate experiments of the effect of \\(T\\) on \\(Y\\). We would expect about 95 of these experiments to (correctly) fail to reject the null hypotheses, and about 5 to (incorrectly) reject it. But if some of the significant findings get published and none of the insignificant ones do, you will end up incorrectly believing the treatment affects the outcome. But the statistical significance filter has another, less obvious—and thus more pernicious—effect on our inferences. Assume that the null hypothesis is indeed false: that the treatment \\(T\\) has an effect on the outcome \\(Y\\). Suppose once again that 100 labs run separate experiments of the effect of \\(T\\) on \\(Y\\). Depending on the power of the experiments, some proportion of them will (incorrectly) fail to reject the null hypothesis, and the remainder will (correctly) reject it. Because of the statistical significance filter, only the ones that reject the null hypothesis will get published. That’s not so bad, right? Only the studies that reject the null hypothesis get published, but the null hypothesis is wrong! The problem comes in when we want to evaluate the size of the effect—what political scientists like to call “substantive significance.”26 On average, the statistically significant studies will tend to overestimate the magnitude of the effect. Viewing studies through the statistical significance filter, we will correctly infer that there is an effect, but we will systematically overestimate how strong it is. Why does the statistical significance filter result in an overestimate of effect magnitudes? Imagine that the population regression coefficient is real but small, say \\(\\beta_k = 0.1\\). Then the sampling distribution of \\(\\hat{\\beta}_k\\) will look something like the following graph. Since the population parameter is close to zero, we are rather likely to yield a sample estimate close to zero. With a small sample size, sample estimates close to zero are likely to be statistically insignificant. Under the statistical significance filter, only the results in the “tails” of the distribution will end up being published. The first time I read about this result, on Andrew Gelman’s blog, I didn’t believe it. (I should have believed it, because he’s a professional statistician and I’m not.) So I fired up R and ran a simulation to answer: if we only report our estimate of \\(\\beta_k\\) when it’s statistically significant, will we overestimate its magnitude on average? In your R session this week, you will run a version of this same simulation. 10.2 \\(p\\)-Hacking The statistical significance filter is a demand-side problem. The demand (by journals) for “insignificant” findings is too low. This in turn creates supply-side problems. Scientists’ careers depend on their ability to publish their findings. Since there is no demand for insignificant findings, scientists do what they can to conjure up significant results. In the best case scenario, this means devoting effort to projects with a high prior probability of turning up significant, rather than riskier endeavors. In the worst case, it means engaging in vaguely-to-definitely unethical statistical practices in a desperate search for significance. One way to \\(p\\)-hack is to just define the significance level post hoc. XKCD #1478. Luckily, this is pretty transparent. The convention, for better or worse, is a significance level of 0.05, and it’s easy to notice deviations from the convention. Look for the “daggers” in people’s regression tables, or language like “comes close to statistical significance”. Matthew Hankins’ blog post “Still Not Significant” is a comprehensive compendium of the weasel language people use to try to dress up their insignificant findings. See also Pritschet, Powell, and Horne (2016). The more pernicious form of \\(p\\)-hacking is going fishing for something significant after one’s original hypothesis test “fails.” Let us once again imagine a lab performing an experiment. They are interested in the effect of a treatment \\(T\\) on an outcome \\(Y\\). To make it concrete, suppose the treatment is reading a particular editorial, and the outcome is where the respondent places himself or herself on a left-right ideological scale ranging between 0 and 1. The lab spends a lot of time and money recruiting subjects, running the experiment, and tabulating the data. They get their spreadsheet together, load their data into R, test for a treatment effect … and fail to reject the null hypothesis. Damn. All that effort wasted, for a result that can’t be published. But wait! The op-ed was written by a man, and his picture appeared next to it. It seems plausible that it might only have an effect on men, or only one on women. So just to see, the lab re-runs the test once just for men and once just for women. They get a \\(p\\)-value just below \\(0.05\\) for the male subsample! Hooray! This is at least potentially a publishable finding! What’s wrong with this picture? Let’s go back to the formal definition of the significance level. The significance level of a hypothesis test is the probability of rejecting the null hypothesis when the null hypothesis is true. If the null hypothesis is true, and 100 labs run the same experiment on it, we should expect about 5 of them to end up incorrectly rejecting the null hypothesis. Similarly, go back to the formal definition of a \\(p\\)-value. The \\(p\\)-value of a test statistic is the probability of yielding a test statistic at least as extreme when the null hypothesis is true. If the null hypothesis is true, we should expect only about 10 out of 100 labs to end up with \\(p \\leq 0.10\\), 5 out of 100 to have \\(p \\leq 0.05\\), and so on. The problem with this hypothetical procedure—testing post hoc for effects within subgroups after the main test comes back insignificant—is that the stated significance level is not the real significance level. If you run three different tests and reject the null hypothesis if any of them comes back with \\(p \\leq 0.05\\), you will reject the null hypothesis more often than 5% of the time. In our running hypothetical example, the lab’s reported \\(p\\)-value of 0.05 is a lie. XKCD #882. There are many ways to \\(p\\)-hack: Splitting up data by subgroups post hoc Changing the set of variables you control for Changing the operationalization of the covariate of interest or the response variable Changing the time period of the analysis Stopping data collection as soon as \\(p \\leq 0.05\\) What all these have in common is that the final test you report depends on the result of some earlier test you ran. All standard hypothesis tests assume that you didn’t do anything like this—that this was the only test you ran, that your initial results didn’t influence your choice of further tests. It is unethical to report the nominal \\(p\\)-value (i.e., the value your computer spits out) from a \\(p\\)-hacked test, because the true probability of getting a result at least as extreme is greater than the nominal value. 10.3 What to Do At a macro level, we should probably: Assume the magnitudes of published results are exaggerated, and adjust our own beliefs accordingly. Collect new data to replicate published findings, and adjust our beliefs in the direction of the replication results. When reviewing others’ papers, don’t judge on the basis of significance. Try to be “results-blind.” Assess whether the research design is well suited to address the question at hand, not whether it turned up the results the author wanted, or the results you want, or interesting or surprising or counterintuitive results, etc. When writing your own papers, focus on research designs that are clever and novel. Write papers that will be interesting to the political science community regardless of whether the results are statistically significant. And at a more micro level, to avoid \\(p\\)-hacking in your own research: Decide exactly which hypothesis you want to test and which test to run before you collect your data, or at least before running any analysis on it. Report every test you perform on the data, and only highlight results that are robust across tests. Randomly split your sample before performing any tests. Go wild with the first half of the sample looking for an interesting hypothesis. Then test that hypothesis on the other half of the sample (and report the results whether they come out in your favor or not). Equivalently, hack your pilot data and then go out and collect new data to try to replicate your hacked initial hypothesis. Apply a correction for multiple testing problems, or use computational methods (e.g., bootstrap) to calculate the distribution of a data-conditional test statistic under the null hypothesis. 10.4 Appendix: R Simulation Let’s run a simulation to see how publication bias affects scientific output. Imagine a bivariate regression situation where \\(X\\) has a small but real effect on \\(Y\\). Specifically, let the population model be \\[ Y_n = 1 + 0.5 X_n + \\epsilon_n, \\] where \\(X_n\\) is uniformly distributed on \\([0, 1]\\) and \\(\\epsilon_n\\) is normally distributed with mean zero and variance one. We can use the random number generation functions in R to draw a sample from this population model. Let’s draw a sample of \\(N = 50\\) observations. n_obs &lt;- 50 x &lt;- runif(n_obs, min = 0, max = 1) y &lt;- 1 + 0.5 * x + rnorm(n_obs, mean = 0, sd = 1) plot(x, y) We know the true slope is non-zero, though with such a small effect and such a small sample it’s hard to tell from the scatterplot. We can run a regression of \\(Y\\) on \\(X\\) and extract the estimated slope and its \\(p\\)-value (for a test of \\(H_0 : \\beta = 0\\)). library(&quot;tidyverse&quot;) library(&quot;broom&quot;) fit &lt;- lm(y ~ x) tidy(fit) # Full regression table ## term estimate std.error statistic p.value ## 1 (Intercept) 1.21946 0.28893 4.22058 0.00010768 ## 2 x 0.17739 0.46012 0.38553 0.70154746 tidy(fit)[2, c(&quot;estimate&quot;, &quot;p.value&quot;)] # Parts we want ## estimate p.value ## 2 0.17739 0.70155 This just gives us one instance of what we would get from this type of sample from this particular model. What we’re really interested in, though, is the sampling distribution—the distribution of results we would get across many samples from this model. More specifically, we want to answer two questions. If the results of every sample from this model were made public, what would we infer from the full body of evidence? If only the statistically significant results were made public, what we infer from the published body of evidence? To make that task a bit simpler, let’s write a function that collects everything we just did: generates a sample of size n_obs, runs a regression of \\(Y\\) on \\(X\\), and spits back the estimated slope and its \\(p\\)-value. draw_slope_and_p &lt;- function(n_obs) { x &lt;- runif(n_obs, min = 0, max = 1) y &lt;- 1 + 0.5 * x + rnorm(n_obs, mean = 0, sd = 1) fit &lt;- lm(y ~ x) out &lt;- tidy(fit)[2, c(&quot;estimate&quot;, &quot;p.value&quot;)] as.numeric(out) # Convert data frame to vector } Since this function is drawing random numbers, it will return different results every time we run it. draw_slope_and_p(50) ## [1] 0.38730 0.36929 draw_slope_and_p(50) ## [1] 0.21287 0.67439 Now what we want to do is to run this function a whole bunch of times, say 1,000. That lets us imagine what would happen if 1,000 different labs each took a different sample of size \\(N = 50\\) from the population, estimated the slope of the regression line, and tested against the null hypothesis that the slope is zero. To do that, we can use the replicate() function, which is a convenient way to run the same function repeatedly. results_50 &lt;- replicate(1000, draw_slope_and_p(50)) dim(results_50) ## [1] 2 1000 This gives us a \\(2 \\times 1000\\) matrix. Each column is a separate run of draw_slope_and_p(). The first row is the estimated slope; the second row is the associated \\(p\\)-value. First let’s check how often we rejected the null hypothesis. The null hypothesis is false, so we would ideally always reject it. However, we have a small effect size and each individual sample is small, so the effect is hard to detect in any given sample—we will often fail to reject the null hypothesis. Let’s see exactly how often. # Proportion of p-values at or below 0.05 sig_50 &lt;- results_50[2, ] &lt;= 0.05 mean(sig_50) ## [1] 0.16 So the power of our study is not great—most of the time, we’re failing to reject the null hypothesis. Let’s see what the average estimated slope is across the full set of samples. mean(results_50[1, ]) ## [1] 0.52057 This is pretty close to the true value, reflecting the fact that OLS is unbiased. If all 1,000 studies were made public, we would ultimately infer—correctly—that the population slope is roughly 0.5. But what if we only saw the small fraction of studies whose results were statistically significant? # Restrict to columns where p &lt;= 0.05 mean(results_50[1, sig_50]) ## [1] 1.2106 We might be tempted to trust these studies more, since they are the ones that correctly reject the null hypothesis. But in fact, if we throw away those that are statistically insignificant, we end up with a biased picture of the effect size. The average effect size in the significant studies is more than double the true effect size. We can see this if we compare the distribution of all results to that of just the significant results. hist(results_50[1, ]) hist(results_50[1, sig_50]) We can also see that this problem stems in large part from low statistical power. Suppose each lab were taking a sample of \\(N = 200\\) instead of \\(N = 50\\), making their effect estimates more precise and thereby increasing each lab’s chance of (correctly) rejecting the null hypothesis. results_200 &lt;- replicate(1000, draw_slope_and_p(200)) sig_200 &lt;- results_200[2, ] &lt;= 0.05 # Power mean(sig_200) ## [1] 0.535 # Average estimate through the significance filter mean(results_200[1, sig_200]) ## [1] 0.67407 hist(results_200[1, sig_200]) The significance-filtered average is still overinflated, but not nearly as drastically as when we were working with a small sample. References "],
["nonspherical.html", "11 Non-Spherical Errors 11.1 Generalized Least Squares 11.2 Detecting Heteroskedasticity 11.3 Heteroskedasticity of Unknown Form 11.4 Appendix: Implementation in R", " 11 Non-Spherical Errors In our consideration so far of the linear model, \\[ \\mathbf{Y} = \\mathbf{X} \\beta + \\epsilon, \\] we have focused on the ordinary least squares estimator, \\[ \\hat{\\beta}_{\\text{OLS}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{Y}. \\] We have seen that OLS is an unbiased and consistent estimator of the linear model parameters as long as we have strict exogeneity, \\[ E[\\epsilon \\,|\\, \\mathbf{X}] = \\mathbf{0}. \\] However, two more important properties of OLS depend on the assumption of spherical errors, \\[ V[\\epsilon \\,|\\, \\mathbf{X}] = \\sigma^2 \\mathbf{I} = \\begin{bmatrix} \\sigma^2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\sigma^2 \\end{bmatrix}. \\] These properties are That \\(\\hat{\\beta}_{\\text{OLS}}\\) is the minimum-variance unbiased linear estimator of \\(\\beta\\); i.e., that OLS is BLUE. That the variance of OLS is given by the formula \\[ V[\\hat{\\beta}_{\\text{OLS}} \\,|\\, \\mathbf{X}] = \\sigma^2 (\\mathbf{X}^\\top \\mathbf{X})^{-1}. \\] In other words, if the spherical errors assumption fails to hold, there are two problems with OLS. There is a better estimator available—one that is still unbiased, but is less sensitive to sampling variation (i.e., has lower standard errors). The formula we use to estimate the standard errors of OLS is invalid. Our confidence intervals and hypothesis tests will (usually) be overconfident, overstating the precision of our results. This week, we will talk about two ways to proceed in the face of non-spherical errors. The first is to use an estimator other than OLS, specifically one called generalized least squares, or GLS. GLS solves both of the problems listed above (inefficiency and invalid standard errors), but its own validity depends on stringent assumptions that are tough to confirm. The second is to use OLS, but to correct the standard errors. Then our estimates will still be inefficient, relative to the hypothetical ideal GLS estimator, but we will at least be able to draw valid inferences (from large samples). Before we dive into these two methods, let’s remind ourselves what deviations from the spherical errors assumption look like. Spherical errors fails when we have either or both of: Heteroskedasticity: \\(V[\\epsilon_i] \\neq V[\\epsilon_j]\\). Autocorrelation: \\(\\text{Cov}[\\epsilon_i, \\epsilon_j] \\neq 0\\) (for some \\(i \\neq j\\)). Here’s what each of the three cases looks like in bivariate data. Autocorrelation usually arises in time series analysis, which is beyond the scope of this course, so we will focus primarily on heteroskedasticity. 11.1 Generalized Least Squares Suppose strict exogeneity holds, but spherical errors fails, with \\[ V[\\epsilon \\,|\\, \\mathbf{X}] = \\sigma^2 \\Omega = \\sigma^2 \\begin{bmatrix} \\omega_{11} &amp; \\omega_{12} &amp; \\cdots &amp; \\omega_{1N} \\\\ \\omega_{12} &amp; \\omega_{22} &amp; \\cdots &amp; \\omega_{2N} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\omega_{1N} &amp; \\omega_{2N} &amp; \\cdots &amp; \\omega_{NN} \\end{bmatrix}, \\] where \\(\\Omega\\) is known27 and \\(\\sigma^2\\) is unknown. In other words, we know the exact structure of heteroskedasticity and autocorrelation, up to a potentially unknown constant. We derived the OLS estimator by finding the \\(\\hat{\\beta}\\) that minimizes the sum of squared errors, \\[ \\text{SSE} = \\sum_n (Y_n - \\mathbf{x}_n \\cdot \\hat{\\beta})^2 = (\\mathbf{Y} - \\mathbf{X} \\hat{\\beta})^\\top (\\mathbf{Y} - \\mathbf{X} \\hat{\\beta}). \\] We derive its close cousin, the generalized least squares, or GLS, estimator, by minimizing the weighted sum of squared errors, \\[ \\text{WSSE} = (\\mathbf{Y} - \\mathbf{X} \\hat{\\beta})^\\top \\Omega^{-1} (\\mathbf{Y} - \\mathbf{X} \\hat{\\beta}). \\] I’ll spare you the matrix calculus that follows, but suffice to say the resulting estimator is \\[ \\hat{\\beta}_{\\text{GLS}} = (\\mathbf{X}^\\top \\Omega^{-1} \\mathbf{X})^{-1} \\mathbf{X}^\\top \\Omega^{-1} \\mathbf{Y}. \\] It is easy to confirm that OLS is the special case of GLS with \\(\\Omega = \\mathbf{I}\\). Similar to the formula for OLS, the variance of GLS is \\[ V[\\hat{\\beta}_{\\text{GLS}} \\,|\\, \\mathbf{X}] = \\sigma^2 (\\mathbf{X}^\\top \\Omega^{-1} \\mathbf{X})^{-1}. \\] Like OLS, GLS is unbiased and consistent as long as we have strict exogeneity. Even if we get \\(\\Omega\\) wrong—i.e., we misspecify the model of the error variance—GLS will still “work” in the sense of giving us an unbiased and consistent estimate of the coefficients. This is easy to confirm. \\[ \\begin{aligned} E [ \\hat{\\beta}_{\\text{GLS}} \\,|\\, \\mathbf{X} ] &amp;= E [ (\\mathbf{X}^\\top \\Omega^{-1} \\mathbf{X})^{-1} \\mathbf{X}^\\top \\Omega^{-1} \\mathbf{Y} \\,|\\, \\mathbf{X} ] \\\\ &amp;= (\\mathbf{X}^\\top \\Omega^{-1} \\mathbf{X})^{-1} \\mathbf{X}^\\top \\Omega^{-1} E [\\mathbf{Y} \\,|\\, \\mathbf{X} ] \\\\ &amp;= (\\mathbf{X}^\\top \\Omega^{-1} \\mathbf{X})^{-1} \\mathbf{X}^\\top \\Omega^{-1} (\\mathbf{X} \\beta) \\\\ &amp;= (\\mathbf{X}^\\top \\Omega^{-1} \\mathbf{X})^{-1} (\\mathbf{X}^\\top \\Omega^{-1} \\mathbf{X}) \\beta \\\\ &amp;= \\beta. \\end{aligned} \\] If we get \\(\\Omega\\) right, then GLS has two important additional properties. Our estimate of the variance matrix, \\[ \\hat{\\Sigma} = \\frac{\\text{WSSE}}{N - K} (\\mathbf{X}^\\top \\Omega^{-1} \\mathbf{X})^{-1}, \\] is valid for inference using the same methods we discussed in the OLS case. GLS is BLUE, per the Gauss-Markov theorem: there is no other unbiased linear estimator with lower standard errors. The first of these is pretty important; the second is just icing on the cake. The main problem with non-spherical errors is the threat they pose to inference from OLS. GLS fixes that—as long as we know \\(\\Omega\\). More on this soon. An important special case of GLS is when there is heteroskedasticity but no autocorrelation, so that \\[ \\Omega = \\sigma^2 \\begin{bmatrix} \\omega_{11} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\omega_{22} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\omega_{NN} \\end{bmatrix}. \\] This special case is called weighted least squares, since GLS gives us the same answer as if we ran OLS on the following “weighted” data: \\[ \\mathbf{X}^* = \\begin{bmatrix} x_{11} / \\sqrt{\\omega_{11}} &amp; x_{12} / \\sqrt{\\omega_{11}} &amp; \\cdots &amp; x_{1K} / \\sqrt{\\omega_{11}} \\\\ x_{21} / \\sqrt{\\omega_{22}} &amp; x_{22} / \\sqrt{\\omega_{22}} &amp; \\cdots &amp; x_{2K} / \\sqrt{\\omega_{22}} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{N1} / \\sqrt{\\omega_{NN}} &amp; x_{N2} / \\sqrt{\\omega_{NN}} &amp; \\cdots &amp; x_{NK} / \\sqrt{\\omega_{NN}} \\end{bmatrix}, \\mathbf{Y}^* = \\begin{bmatrix} Y_1 / \\sqrt{\\omega_{11}} \\\\ Y_2 / \\sqrt{\\omega_{22}} \\\\ \\vdots \\\\ Y_N / \\sqrt{\\omega_{NN}} \\end{bmatrix}. \\] 11.2 Detecting Heteroskedasticity If you don’t have prior knowledge of the variance structure of the error term, you may be interested in testing whether the homoskedasticity assumption of OLS is viable. In a bivariate regression model, you can usually detect heteroskedasticity via the eye test. Not so much when you have multiple covariates. In this case, you may want to formally test for heteroskedasticity. There are a few such tests, but we will just talk about the Breusch-Pagan test, which was developed by Breusch and Pagan (1980) and refined by Koenker and Bassett Jr (1982). The null hypothesis of the test is that \\(V[\\epsilon_i \\,|\\, \\mathbf{X}] = \\sigma^2\\) for all \\(i = 1, \\ldots, N\\). The test procedure is as follows. Calculate the OLS estimate, \\(\\hat{\\beta}_{\\text{OLS}}\\). Calculate the OLS residuals, \\(\\hat{e} = Y - \\mathbf{X} \\hat{\\beta}_{\\text{OLS}}\\). Let \\(\\hat{u}\\) be the vector of squared residuals, \\(\\hat{u} = (\\hat{e}_1^2, \\ldots, \\hat{e}_N^2)\\). Run a regression of \\(\\hat{u}\\) on \\(\\mathbf{Z}\\), an \\(N \\times q\\) matrix of covariates. Let \\(R_{\\hat{u}}^2\\) denote the \\(R^2\\) of this regression. Reject the null hypothesis if \\(N R_{\\hat{u}}^2\\) exceeds the critical value for a \\(\\chi_{q - 1}^2\\) distribution. In the canonical version of this test, \\(\\mathbf{Z}\\) is equal to \\(\\mathbf{X}\\). A more powerful version is the White test (H. White 1980), in which \\(\\mathbf{Z}\\) contains each variable in \\(\\mathbf{X}\\) as well as all second-order terms (squares and interactions). 11.3 Heteroskedasticity of Unknown Form Suppose we know there is heteroskedasticity but we don’t trust ourselves to properly specify the error variances to run weighted least squares.28 Then we do not have an efficient estimator of \\(\\beta\\). We might be all right with that, but we would really like to have a good estimator for the standard errors of the OLS estimator, so that we can test hypotheses about the coefficients. Happily, we can estimate the variance matrix of the OLS estimator consistently even in the presence of heteroskedasticity. White’s heteroskedasticity-consistent estimator (H. White 1980) of the variance matrix starts by forming a diagonal matrix out of the squared residuals, \\[ \\hat{\\mathbf{U}} = \\begin{bmatrix} \\hat{u}_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\hat{u}_2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\hat{u}_N \\end{bmatrix} = \\begin{bmatrix} \\hat{e}_1^2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\hat{e}_2^2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\hat{e}_N^2 \\end{bmatrix} \\] This lets us form the “meat” of the “sandwich” that is White’s estimator of the OLS variance matrix: \\[ \\hat{\\Sigma}_{\\text{HC}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\hat{\\mathbf{U}} \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{X})^{-1}. \\] You know I love proofs, but I am not even going to attempt to prove that this consistently estimates the (asymptotic) variance matrix of \\(\\hat{\\beta}_{\\text{OLS}}\\). See Greene (2003, 198–99) for a sketch of the proof. White’s estimator is consistent but not unbiased, so we may want to apply a sort of bias correction in small samples. A popular choice is the so-called “HC1” estimator, which corrects for the number of parameters estimated the same way the usual OLS variance estimator does: \\[ \\hat{\\Sigma}_{\\text{HC1}} = \\frac{N}{N - K} \\hat{\\Sigma}_{\\text{HC}} \\] In this scheme, the standard White estimator is called the “HC” or “HC0” estimator. There are many other consistent estimators that apply some or other finite-sample correction; see MacKinnon and White (1985) for the gory details. Because of its association with the , robust option in Stata, people sometimes call the White estimator of the standard errors “robust standard errors”. Don’t do that. In your own work, if you estimate and report heteroskedasticity-consistent standard errors, report that you use the H. White (1980) estimator of the standard errors, and specify which variant (HC0, HC1, and so on). Remember that your goal is to give others enough information to replicate your analysis even if they don’t have your code—“robust standard errors” has too many interpretations to accomplish that. Finally, it is important to know that weighted least squares and heteroskedasticity-consistent standard errors are not mutually exclusive approaches. If you have a suspicion about the error variances that you think can improve the precision of the regression, but you are not totally comfortable with committing to WLS for inference, you can calculate heteroskedasticity-consistent standard errors for a WLS (or GLS) fit. 11.4 Appendix: Implementation in R We’ll be using the following packages: library(&quot;tidyverse&quot;) library(&quot;broom&quot;) library(&quot;car&quot;) library(&quot;lmtest&quot;) library(&quot;sandwich&quot;) We will use data from the car package on professors’ salaries. This is a topic that, ideally, will be of great concern to you in 5ish years. data(Salaries) head(Salaries) ## rank discipline yrs.since.phd yrs.service sex salary ## 1 Prof B 19 18 Male 139750 ## 2 Prof B 20 16 Male 173200 ## 3 AsstProf B 4 3 Male 79750 ## 4 Prof B 45 39 Male 115000 ## 5 Prof B 40 41 Male 141500 ## 6 AssocProf B 6 6 Male 97000 11.4.1 Generalized Least Squares Let us model a professor’s salary as a function of the number of years since she received her PhD. If we glance at the data, we see obvious heteroskedasticity. ggplot(Salaries, aes(x = yrs.since.phd, y = salary)) + geom_point() There is much more variation in the salaries of professors 30+ years into their careers than in those who are fresh out of grad school. If we want the most precise model of the relationship between years since PhD and salary, we might want to place more weight on early-career professors and less on late-career professors. Remember that weighted least squares is the special case of GLS where the off-diagonal elements of \\(\\Omega\\) are zero (i.e., there is heteroskedasticity but no autocorrelation): \\[ \\Omega = \\sigma^2 \\begin{bmatrix} \\omega_{11} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\omega_{22} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\omega_{NN} \\end{bmatrix}. \\] To run weighted least squares in R, we just use the weights argument to the lm() function. As an example, let’s set \\[ \\omega_{nn} = \\frac{1}{\\text{Years Since PhD}_n} \\] for each observation \\(n\\), to place the greatest weight on early-career observations. We will first run a baseline OLS model, then compare to the weighted model. ols_salaries &lt;- lm(salary ~ yrs.since.phd + yrs.service, data = Salaries) summary(ols_salaries) ## ## Call: ## lm(formula = salary ~ yrs.since.phd + yrs.service, data = Salaries) ## ## Residuals: ## Min 1Q Median 3Q Max ## -79735 -19823 -2617 15149 106149 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 89912 2844 31.62 &lt; 2e-16 ## yrs.since.phd 1563 257 6.09 2.8e-09 ## yrs.service -629 254 -2.47 0.014 ## ## Residual standard error: 27400 on 394 degrees of freedom ## Multiple R-squared: 0.188, Adjusted R-squared: 0.184 ## F-statistic: 45.7 on 2 and 394 DF, p-value: &lt;2e-16 wls_salaries &lt;- lm(salary ~ yrs.since.phd + yrs.service, weights = 1 / yrs.since.phd, data = Salaries) summary(wls_salaries) ## ## Call: ## lm(formula = salary ~ yrs.since.phd + yrs.service, data = Salaries, ## weights = 1/yrs.since.phd) ## ## Weighted Residuals: ## Min 1Q Median 3Q Max ## -13520 -4386 -91 4101 16046 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 79672 1460 54.56 &lt; 2e-16 ## yrs.since.phd 1753 242 7.25 2.3e-12 ## yrs.service -289 265 -1.09 0.28 ## ## Residual standard error: 5760 on 394 degrees of freedom ## Multiple R-squared: 0.427, Adjusted R-squared: 0.425 ## F-statistic: 147 on 2 and 394 DF, p-value: &lt;2e-16 Compared to OLS, our WLS model yields a stronger relationship between years since the PhD and the expected value of a professor’s salary. In addition, we estimate a smaller coefficient on years of service, and would no longer reject the null hypothesis of no relationship there. Which model is better, OLS or WLS? It depends on whether our weights correspond to the true relative variance in the error term. In this case, we’ve specified a pretty aggressive weighting scheme—a professor with 1 year of service gets double the weight of a professor with 2 years of service. An alternative weighting scheme would give us different results. The “best” model is the one that best corresponds to the true \\(\\Omega\\), which unfortunately is hard to know in advance. The best we can do is apply theory and judgment in a thoughtful way. In the (relatively unlikely) event that you are pre-specifying \\(\\Omega\\) with autocorrelations, you can use the lm.gls() function from the MASS package. The gls() function from the nlme package performs feasible generalized least squares, whereby we write \\(\\Omega\\) as a function of a few parameters, estimate those parameters, and run the subsequent GLS model. 11.4.2 Breusch-Pagan Test We already used the “eye test” to confirm the presence of heteroskedasticity in the relationship we are modeling, but let’s see how we would use the Breusch-Pagan test to confirm our suspicion. For this we use the bptest() function from the lmtest package. bptest(ols_salaries) ## ## studentized Breusch-Pagan test ## ## data: ols_salaries ## BP = 49.9, df = 2, p-value = 1.5e-11 By default, bptest() uses the same variables as in the original regression in the regression of the squared residuals. To perform the White test, we can use an extra argument to bptest() to specify a different model formula. bptest(ols_salaries, ~ yrs.since.phd * yrs.service + I(yrs.since.phd^2) + I(yrs.service^2), data = Salaries) ## ## studentized Breusch-Pagan test ## ## data: ols_salaries ## BP = 60.5, df = 5, p-value = 9.6e-12 In this case, regardless of which test we use, we reject the null hypothesis of homoskedasticity. 11.4.3 Heteroskedasticity-Consistent Standard Errors To calculate the White estimator and its friends in R, we use the vcovHC() function from the sandwich package. vcv0 &lt;- vcovHC(ols_salaries, type = &quot;HC0&quot;) vcv0 ## (Intercept) yrs.since.phd yrs.service ## (Intercept) 5809137 -340724 111808 ## yrs.since.phd -340724 77168 -75508 ## yrs.service 111808 -75508 91091 To use HC1 (or another one of the finite-sample corrections to the ordinary White estimate), change the type argument to, e.g., type = &quot;HC1&quot;. See ?vcovHC for all the options. To create a “regression table” using our new “robust” standard errors, we can use the coeftest() function from the lmtest package. coeftest(ols_salaries) # Original table ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 89912 2844 31.62 &lt; 2e-16 ## yrs.since.phd 1563 257 6.09 2.8e-09 ## yrs.service -629 254 -2.47 0.014 coeftest(ols_salaries, vcov = vcv0) # With corrected SEs ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 89912 2410 37.30 &lt; 2e-16 ## yrs.since.phd 1563 278 5.63 3.5e-08 ## yrs.service -629 302 -2.08 0.038 Just like ordinary regression tables, the ones made by coeftest() can be “swept” into data frames using the tools in broom: tidy(coeftest(ols_salaries, vcov = vcv0)) ## term estimate std.error statistic p.value ## 1 (Intercept) 89912.2 2410.22 37.3046 2.3298e-131 ## 2 yrs.since.phd 1562.9 277.79 5.6261 3.5014e-08 ## 3 yrs.service -629.1 301.81 -2.0844 3.7766e-02 You may also want to use the White-estimated standard errors to conduct Wald tests of linear hypotheses. You can do that by supplying the relevant estimated variance matrix to the vcov argument of linearHypothesis(): linearHypothesis(ols_salaries, c(&quot;yrs.since.phd = 1500&quot;, &quot;yrs.service = -500&quot;), vcov = vcv0, test = &quot;Chisq&quot;) ## Linear hypothesis test ## ## Hypothesis: ## yrs.since.phd = 1500 ## yrs.service = - 500 ## ## Model 1: restricted model ## Model 2: salary ~ yrs.since.phd + yrs.service ## ## Note: Coefficient covariance matrix supplied. ## ## Res.Df Df Chisq Pr(&gt;Chisq) ## 1 396 ## 2 394 2 0.32 0.85 Finally, remember how earlier we talked about how the WLS estimates are only as good as the weights you choose. If they’re not the true weights, then WLS is not efficient and the standard error estimator is inconsistent. We can’t fix the first problem, but we can fix the second. To wit, you can estimate heteroskedasticity-consistent standard errors for WLS models too. vcv0_wls &lt;- vcovHC(wls_salaries, type = &quot;HC0&quot;) coeftest(wls_salaries) # Original ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 79672 1460 54.56 &lt; 2e-16 ## yrs.since.phd 1753 242 7.25 2.3e-12 ## yrs.service -289 265 -1.09 0.28 coeftest(wls_salaries, vcov = vcv0_wls) # Corrected ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 79672 1474 54.06 &lt; 2e-16 ## yrs.since.phd 1753 245 7.16 3.9e-12 ## yrs.service -289 272 -1.06 0.29 So if you have a good idea about the residual variances but aren’t sure you’ve nailed it down, you can have the best of both worlds—at least in terms of large-sample hypothesis testing. References "],
["panel.html", "12 Clustered and Panel Data 12.1 The Linear Model with Grouped Data 12.2 Clustered Standard Errors 12.3 Random Effects 12.4 Fixed Effects 12.5 Appendix: Implementation in R", " 12 Clustered and Panel Data Grouped data structures, in which we observe individual units within larger groups, are common in political science and other social sciences. Examples include: Cross-national survey data, where we observe individual respondents grouped by country. Block-randomized field experiments. For example, in an experiment where the treatment is administered at the village level, we observe individual outcomes grouped by village. Panel data, where we observe the same units repeatedly over time. This includes panel surveys as well as observational data on states, countries, or other political units over time. Grouped data presents problems and opportunities. At the root of both is the idea of unobserved heterogeneity—that some variation across groups might be due to unobservable features of the groups. To return to our running example, suppose you are interested in the correlates of voting for Donald Trump in the 2016 general election. You observe survey respondents grouped by state. Vote choice might be affected by: Individual characteristics like one’s age, gender, race, income, and education. State characteristics29 like the unemployment rate, undocumented population, and exposure to trade with China. Some state characteristics that might affect vote choice are difficult or impossible to measure. For example, some states have a more cosmopolitan culture, while others have a more traditional culture. A 50-year-old white man living in Connecticut is, we would expect, less likely to have voted for Trump than a 50-year-old white man living in Alabama. This is unobserved heterogeneity—characteristics that we do not observe, and therefore cannot control for, but which affect the response. If we are not careful in dealing with grouped data, unobserved heterogeneity can be a major problem. The spherical errors assumption is usually not tenable for grouped data, since observations will be correlated with others in the same group. OLS will therefore be inefficient and yield invalid inferences, as we saw last week. Even worse, if group-level sources of unobserved heterogeneity are correlated with individual characteristics—if, say, younger voters are more likely to live in cosmopolitan states—OLS may also be biased and inconsistent. On the other hand, if we deal with grouped data properly, we can enhance the credibility of our inferences. We can eliminate the influence of variation across groups, allowing us to focus on the comparisons within groups that we are usually most interested in. Before we get started, a note of caution. One week allows us just enough time to scratch the surface of how to analyze grouped data. If you work with grouped data in your dissertation or other research, you should think carefully about your data structure and potential sources of unobserved heterogeneity. The methods we discuss this week may not solve your problems. As further references, I recommend Wooldridge (2002) and Gelman and Hill (2006). 12.1 The Linear Model with Grouped Data We need to change our notation a bit to reflect the arrangement of observations into groups. Let there be \\(G\\) groups indexed by \\(g = 1, \\ldots, G\\). Within each group, we have \\(N\\) observations indexed by \\(n = 1, \\ldots, N\\).30 (In the special case of panel data, where each group is a unit observed over time, the standard notation is to use \\(t = 1, \\ldots, T\\) instead.) We will now index individual “rows” of the data by \\(gn\\), which stands for the \\(n\\)’th observation within group \\(g\\). Unit-level \\(Y_{gn}\\): response for the \\(gn\\)’th observation \\(\\mathbf{x}_{gn}\\): vector of \\(K\\) covariates for the \\(gn\\)’th observation \\(\\epsilon_{gn}\\): random shock to the \\(gn\\)’th response Group-level \\(\\mathbf{Y}_g\\): vector of \\(N\\) responses for the \\(g\\)’th group \\(\\mathbf{X}_g\\): \\(N \\times K\\) matrix of covariates for the \\(g\\)’th group \\(\\epsilon_g\\): vector of \\(N\\) random shocks for the \\(g\\)’th group Full data \\(\\mathbf{Y}\\): vector of all \\(GN\\) responses, where \\[\\mathbf{Y} = \\begin{bmatrix} \\mathbf{Y}_1 \\\\ \\mathbf{Y}_2 \\\\ \\vdots \\\\ \\mathbf{Y}_G \\end{bmatrix}.\\] \\(\\mathbf{X}\\): \\(GN \\times K\\) matrix of covariates, where \\[\\mathbf{X} = \\begin{bmatrix} \\mathbf{X}_1 \\\\ \\mathbf{X}_2 \\\\ \\vdots \\\\ \\mathbf{X}_G \\end{bmatrix}.\\] \\(\\mathbf{D}\\): \\(GN \\times G\\) matrix of group membership indicators. \\(\\epsilon\\): vector of all \\(GN\\) random shocks If we assume a standard linear model for each \\(gn\\)’th response, \\[ Y_{gn} = \\mathbf{x}_{gn} \\cdot \\beta + \\epsilon_{gn}, \\] we end up with the familiar matrix equation \\[ \\mathbf{Y} = \\mathbf{X} \\beta + \\epsilon. \\] You may look at this equation and think, “This looks like something we’ve used OLS on. What’s wrong with OLS?” Two reasons. First, at a minimum, we are unlikely to have spherical errors in grouped data. Observations in the same group—students in the same classroom, voters in the same state, LAPOP respondents in the same country—are likely to have residual correlation. There are unmeasured factors that commonly affect their responses. As in any non-spherical error model, this means OLS will yield invalid inferences. Moreover, because we are talking about autocorrelation and not just heteroskedasticity, the standard error correction that we encountered last week won’t do the trick. Second, depending on the nature of the group-specific effects, we may also have a failure of strict exogeneity. I hope I have impressed on you by now that this is a major problem. To see why we have a failure of strict exogeneity, let us return to the example of voters living in states with cosmopolitan versus traditional cultures. If we cannot measure cosmopolitanism (as I am assuming), then it ends up in the error term of our model: \\[ \\epsilon_{gn} = \\text{Cosmpolitanism}_g + \\text{Other Stuff}_{gn}. \\] But we know that younger and more educated voters are more likely to live in cosmopolitan areas. So if our covariate matrix includes age and education, we have \\[ E[\\epsilon \\,|\\, \\mathbf{X}] \\neq \\mathbf{0}. \\] We will proceed from the easiest problems to the hardest. We will first consider a standard error correction and an efficiency improvement for the case where errors are correlated within groups but strict exogeneity still holds. We will then identify an unbiased estimator in the case where strict exogeneity fails. 12.2 Clustered Standard Errors Imagine the following policy experiment. The federal government randomly selects half of the states to receive a grant intended to improve high school education, while the other half do not receive it. We observe some indicator of educational quality (e.g., graduation rates) at the school district level, where school districts are grouped within states. So our model looks like \\[ \\text{Quality}_{gn} = \\beta_1 + \\beta_2 \\text{Grant}_g + \\epsilon_{gn}. \\] We want to know whether receiving the grant affected quality. What we cannot measure is how well the states used the money. If some states used it better than others, then we would expect the error term to be correlated across school districts within the state. In a state where the money was used wisely, we would expect most of the schools to do “better than expected”—to have positive residuals, in the language of regression. Conversely, in a state where the money was squandered, we would expect most of the schools to do worse than we would otherwise predict. This is one example of the general phenomenon Moulton (1986; 1990) identifies—that there is often substantial correlation in the random errors within groups, especially when we are looking at the effects of variables that only vary at the group level. One way I think about it is that, with grouped data and group-level covariates, the effective number of observations is less than the nominal number of observations. If we use the OLS standard errors, we are pretending to have more data than we really do. Luckily, we can correct these “clustered” errors in a manner similar to what we did last week with heteroskedasticity of unknown form. The most we can assume on \\(\\Omega = V[\\epsilon \\,|\\, \\mathbf{X}]\\) is Heteroskedasticity of unknown form, within and across groups. Autocorrelation of unknown form within groups. No autocorrelation across groups. Under these assumptions, \\(\\Omega\\) has the block-diagonal form \\[ \\Omega = \\begin{bmatrix} \\Omega_1 &amp; \\mathbf{0} &amp; \\cdots &amp; \\mathbf{0} \\\\ \\mathbf{0} &amp; \\Omega_2 &amp; \\cdots &amp; \\mathbf{0} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{0} &amp; \\mathbf{0} &amp; \\cdots &amp; \\Omega_G \\end{bmatrix}, \\] where each \\(\\Omega_g\\) is a symmetric \\(N \\times N\\) matrix. These may differ from each other—each group may have its own special autocorrelation structure. If we knew \\(\\Omega\\) exactly, we could use GLS to efficiently estimate \\(\\beta\\) and obtain correct standard errors. That is unlikely. Instead, we will use OLS to obtain our coefficient estimates, then we will correct the standard errors so as to be approximately correct in large samples. The cluster-robust variance estimator is \\[ \\hat{\\Sigma}_{\\text{CR}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\left( \\sum_{g=1}^G \\mathbf{X}_g^\\top \\hat{e}_g \\hat{e}_g^\\top \\mathbf{X}_g \\right) (\\mathbf{X}^\\top \\mathbf{X})^{-1}, \\] where \\(\\hat{e}_g\\) is the \\(N \\times 1\\) vector of OLS residuals for the \\(g\\)’th group. Like White’s estimator for heteroskedasticity, this is a “sandwich” estimator. The “meat” of the sandwich here accounts for the within-group correlations in the error term. Also like White’s estimator for heteroskedasticity, the cluster-robust estimator is consistent, but not unbiased. It approaches the truth as \\(G\\), the number of groups, grows large (holding fixed \\(N\\), the number of observations per group), but it may be badly biased in small samples. The consensus seems to be that \\(N = 50\\) is large enough (Cameron and Miller 2015), so you Americanists may go ahead and rejoice. However, if you observe a small number of units over a long period of time (e.g., in a study of the historical political economy of Western European countries), the cluster-robust standard error estimator will be severely biased. Beck and Katz (1995) provide a similar estimator for such data, except with the summation in the middle of the “sandwich” taken over the \\(N\\) time periods instead of the \\(G\\) groups. The earliest derivation of this estimator was Liang and Zeger (1986), and that is whom you should cite when you use it. Arellano (1987) derives the same estimator but is a bit easier to follow. As with White’s estimator, there are various finite-sample corrections to \\(\\hat{\\Sigma}_{\\text{CR}}\\) that you may want to use; we will not go through those here. 12.3 Random Effects The coverage and notation in this section and the next one closely follow Johnston and DiNardo (1997, chap. 12). Just as Huber-White standard errors do not fix the inefficiency of OLS under heteroskedasticity, cluster-robust standard errors do not fix the inefficiency of OLS under within-group correlation. To get a handle on the efficiency problem, we will make further assumptions about the source of that correlation. We will assume each group \\(g\\) has a (potentially) different intercept. We can incorporate this into our original model, \\[ Y_{gn} = \\mathbf{x}_{gn} \\cdot \\beta + \\epsilon_{gn}, \\] by decomposing the error term into independent group-specific and individual-specific components: \\[ \\epsilon_{gn} = \\underbrace{\\alpha_g}_{\\text{group}} + \\underbrace{\\eta_{gn}}_{\\text{individual}}. \\] In this equation, \\(\\alpha_g\\) represents the difference between the intercept for group \\(g\\) and the overall average, while \\(\\eta_{gn}\\) is an idiosyncratic shock specific to the \\(n\\)’th observation of group \\(g\\). We will assume that \\(\\eta_{gn}\\) is independent and identically distributed across observations, so that the only source of autocorrelation is that observations in the same group share the same \\(\\alpha_g\\). When we decompose the error term like this, we are assuming implicitly that the \\(\\alpha_g\\)’s are uncorrelated with the covariates in the model. Otherwise, strict exogeneity fails and the techniques that follow are useless. Let me put that another way—the random effects model depends on the assumption that the group-specific shocks are uncorrelated with the covariates. We will proceed under this assumption, and return shortly to the questions of testing it and what to do if it fails. The random-intercepts model gives us a convenient structure for the \\(\\Omega\\) matrix in GLS. Let \\(V[\\alpha_g] = \\sigma^2_\\alpha\\) and \\(V[\\eta_{gn}] = \\sigma^2_\\eta\\). The error variance matrix for the \\(g\\)’th group then works out to \\[ \\Omega_g = \\begin{bmatrix} \\sigma_{\\eta}^2 + \\sigma_{\\alpha}^2 &amp; \\sigma_{\\alpha}^2 &amp; \\cdots &amp; \\sigma_{\\alpha}^2 \\\\ \\sigma_{\\alpha}^2 &amp; \\sigma_{\\eta}^2 + \\sigma_{\\alpha}^2 &amp; \\cdots &amp; \\sigma_{\\alpha}^2 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma_{\\alpha}^2 &amp; \\sigma_{\\alpha}^2 &amp; \\cdots &amp; \\sigma_{\\eta}^2 + \\sigma_{\\alpha}^2 \\end{bmatrix}. \\] Since there is no autocorrelation across groups, \\(\\Omega\\) takes the same block-diagonal form as in our discussion of cluster-robust standard errors. If we knew \\(\\sigma^2_\\alpha\\) and \\(\\sigma^2_\\eta\\), we could estimate \\(\\beta\\) by GLS. It is unlikely, however, that we would know these in advance. Luckily, there is a compromise option available. Feasible GLS, or FGLS, entails using a pair of first-stage regressions to estimate the variance of \\(\\alpha\\) and \\(\\eta\\), then plugging these into the GLS formula. I won’t go through all the math, but the basic idea is as follows. Calculate the average response \\(\\bar{Y}_g\\) and average covariate vector \\(\\bar{\\mathbf{x}}_g\\) for each group. Estimate \\(\\sigma^2_\\alpha\\), the between-group variance, using the residual variance of a regression of \\(\\bar{\\mathbf{Y}}\\) on \\(\\bar{\\mathbf{X}}\\). Estimate \\(\\sigma^2_\\eta\\), the within-group variance, using the residual variance of a regression of \\(\\mathbf{Y}\\) on \\(\\mathbf{X}\\) and \\(\\mathbf{D}\\), the full set of group dummies. We will see the importance of this regression very shortly. Form the matrix \\(\\hat{\\Omega}\\) by plugging \\(\\hat{\\sigma}^2_\\alpha\\) and \\(\\hat{\\sigma}^2_\\eta\\) into the formulas above, then run GLS using \\(\\hat{\\Omega}\\) as the weighting matrix. This gives us the random effects estimator, \\[ \\hat{\\beta}_{\\text{RE}} = (\\mathbf{X}^\\top \\hat{\\Omega}^{-1} \\mathbf{X})^{-1} \\mathbf{X}^\\top \\hat{\\Omega}^{-1} \\mathbf{Y}. \\] See Johnston and DiNardo (1997, 392–95) for full formulas and details. FGLS is consistent but not unbiased, so the random effects model may not be a good idea in small samples. If our specification of the error structure is correct, it is asymptotically efficient—as the sample size increases, no other estimator has lower standard errors. 12.4 Fixed Effects What if the group-specific intercepts are correlated with the covariates? Then, in order to maintain strict exogeneity, we must pull them out of the error term and into the covariate matrix. This is easy to do—we can rewrite the full model as \\[ \\mathbf{Y} = \\mathbf{X} \\beta + \\mathbf{D} \\alpha + \\eta, \\] where \\(\\alpha\\) is the \\(G \\times 1\\) vector of group-specific intercepts. This suggests that we run OLS on our covariates plus the full set of group membership indicators. (As with any set of indicators, we need to omit one category.) The first \\(K\\) elements of this regression constitute the fixed effects estimator. Most textbook treatments of fixed effect estimators go through a whole rigmarole about computation, because it used to be challenging to invert a \\((K + G) \\times (K + G)\\) matrix when \\(G\\) was moderately large. This is no longer true,31 so you can safely ignore most of the hand-wringing about computational difficulties. The standard errors of the fixed effects estimator are usually higher than those of the random effects estimator, since estimating \\(G\\) additional parameters uses a lot of degrees of freedom. This leads us to the following pair of observations. If the random effects assumption is met (group-specific effects are uncorrelated with covariates), then the random effects and fixed estimators are both consistent, but fixed effects is less efficient. If the random effects assumption is not met, then the random effects estimator is inconsistent while the fixed effects estimator is consistent. The typical test for whether fixed effects are necessary comes from Hausman (1978). Under the null hypothesis that both estimators are consistent (and thus fixed effects are unnecessary and inefficient), the test statistic \\[ H = (\\hat{\\beta}_{\\text{RE}} - \\hat{\\beta}_{\\text{FE}})^\\top (\\hat{\\Sigma}_{\\text{FE}} - \\hat{\\Sigma}_{\\text{RE}})^{-1} (\\hat{\\beta}_{\\text{RE}} - \\hat{\\beta}_{\\text{FE}}) \\] asymptotically has a \\(\\chi^2\\) distribution with \\(K\\) degrees of freedom. The other main drawback of fixed effects estimation is that you cannot estimate the effects of variables that do not vary within groups. (Why not?) See Greene (2003, sec. 13.5) for estimation strategies with panel data and time-invariant covariates. One final note: Arellano (1987) shows that the cluster-robust variance matrix estimator can be used with fixed effects. Cameron and Miller (2015) recommend doing so, at least when the number of groups is large. 12.5 Appendix: Implementation in R The methods introduced here can be implemented via the plm package. library(&quot;plm&quot;) We will use the Produc dataset from plm, a riveting collection of economic statistics about the U.S. states from 1970 to 1986. data(Produc, package = &quot;plm&quot;) head(Produc) ## state year region pcap hwy water util pc gsp emp unemp ## 1 ALABAMA 1970 6 15033 7325.8 1655.7 6051.2 35794 28418 1010.5 4.7 ## 2 ALABAMA 1971 6 15502 7525.9 1721.0 6255.0 37300 29375 1021.9 5.2 ## 3 ALABAMA 1972 6 15972 7765.4 1764.8 6442.2 38670 31303 1072.3 4.7 ## 4 ALABAMA 1973 6 16406 7907.7 1742.4 6756.2 40084 33430 1135.5 3.9 ## 5 ALABAMA 1974 6 16763 8025.5 1734.8 7002.3 42057 33749 1169.8 5.5 ## 6 ALABAMA 1975 6 17316 8158.2 1752.3 7405.8 43972 33604 1155.4 7.7 The functions in plm assume that your data are organized like Produc, with the grouping variable in the first column and the identification variable (time, in the case of panel data) in the second column. See the plm package vignette on how to get datasets not organized this way into line. We will treat unemployment (unemp) as our response and public capital stock (pcap) and private capital stock (pc) as our covariates. As a benchmark, let’s use OLS. fit_ols &lt;- lm(unemp ~ pcap + pc, data = Produc) summary(fit_ols) ## ## Call: ## lm(formula = unemp ~ pcap + pc, data = Produc) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.559 -1.622 -0.337 1.213 11.595 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.20e+00 1.08e-01 57.57 &lt;2e-16 ## pcap 1.01e-05 5.52e-06 1.82 0.069 ## pc 2.56e-06 2.56e-06 1.00 0.319 ## ## Residual standard error: 2.2 on 813 degrees of freedom ## Multiple R-squared: 0.0352, Adjusted R-squared: 0.0328 ## F-statistic: 14.8 on 2 and 813 DF, p-value: 4.79e-07 The “pooling” estimator implemented by plm() ought to give us the same results. fit_pooling &lt;- plm(unemp ~ pcap + pc, data = Produc, model = &quot;pooling&quot;) summary(fit_pooling) ## Pooling Model ## ## Call: ## plm(formula = unemp ~ pcap + pc, data = Produc, model = &quot;pooling&quot;) ## ## Balanced Panel: n=48, T=17, N=816 ## ## Residuals : ## Min. 1st Qu. Median 3rd Qu. Max. ## -3.560 -1.620 -0.337 1.210 11.600 ## ## Coefficients : ## Estimate Std. Error t-value Pr(&gt;|t|) ## (Intercept) 6.20e+00 1.08e-01 57.57 &lt;2e-16 ## pcap 1.01e-05 5.52e-06 1.82 0.069 ## pc 2.56e-06 2.56e-06 1.00 0.319 ## ## Total Sum of Squares: 4060 ## Residual Sum of Squares: 3920 ## R-Squared: 0.0352 ## Adj. R-Squared: 0.0328 ## F-statistic: 14.8141 on 2 and 813 DF, p-value: 4.79e-07 We can obtain the cluster-robust variance matrix estimate via vcovHC(). Make sure to specify method = &quot;arellano&quot; so as to get the usual estimator. It is not entirely clear to me which of the various finite-sample adjustments corresponds to the defaults in Stata. crvm_pooling &lt;- vcovHC(fit_pooling, method = &quot;arellano&quot;, type = &quot;HC1&quot;) summary(fit_pooling, vcov = crvm_pooling) ## Pooling Model ## ## Note: Coefficient variance-covariance matrix supplied: crvm_pooling ## ## Call: ## plm(formula = unemp ~ pcap + pc, data = Produc, model = &quot;pooling&quot;) ## ## Balanced Panel: n=48, T=17, N=816 ## ## Residuals : ## Min. 1st Qu. Median 3rd Qu. Max. ## -3.560 -1.620 -0.337 1.210 11.600 ## ## Coefficients : ## Estimate Std. Error t-value Pr(&gt;|t|) ## (Intercept) 6.20e+00 2.45e-01 25.34 &lt;2e-16 ## pcap 1.01e-05 1.21e-05 0.83 0.41 ## pc 2.56e-06 7.30e-06 0.35 0.73 ## ## Total Sum of Squares: 4060 ## Residual Sum of Squares: 3920 ## R-Squared: 0.0352 ## Adj. R-Squared: 0.0328 ## F-statistic: 6.45144 on 2 and 47 DF, p-value: 0.00334 Notice that our \\(t\\)-statistics are cut in more than half, even though our variables have within-group variation (unlike Moulton (1990)’s example). We can also use plm() to estimate a random-effects model. fit_random &lt;- plm(unemp ~ pcap + pc, data = Produc, model = &quot;random&quot;) summary(fit_random) ## Oneway (individual) effect Random Effect Model ## (Swamy-Arora&#39;s transformation) ## ## Call: ## plm(formula = unemp ~ pcap + pc, data = Produc, model = &quot;random&quot;) ## ## Balanced Panel: n=48, T=17, N=816 ## ## Effects: ## var std.dev share ## idiosyncratic 3.09 1.76 0.69 ## individual 1.36 1.17 0.31 ## theta: 0.656 ## ## Residuals : ## Min. 1st Qu. Median 3rd Qu. Max. ## -3.640 -1.360 -0.286 1.010 9.730 ## ## Coefficients : ## Estimate Std. Error t-value Pr(&gt;|t|) ## (Intercept) 5.67e+00 2.53e-01 22.42 &lt; 2e-16 ## pcap 1.55e-09 1.12e-05 0.00 0.99989 ## pc 1.59e-05 4.55e-06 3.51 0.00048 ## ## Total Sum of Squares: 2920 ## Residual Sum of Squares: 2800 ## R-Squared: 0.0421 ## Adj. R-Squared: 0.0398 ## F-statistic: 17.88 on 2 and 813 DF, p-value: 2.52e-08 And, finally, a fixed-effects model, which plm() calls the &quot;within&quot; model. fit_fixed &lt;- plm(unemp ~ pcap + pc, data = Produc, model = &quot;within&quot;) summary(fit_fixed) ## Oneway (individual) effect Within Model ## ## Call: ## plm(formula = unemp ~ pcap + pc, data = Produc, model = &quot;within&quot;) ## ## Balanced Panel: n=48, T=17, N=816 ## ## Residuals : ## Min. 1st Qu. Median 3rd Qu. Max. ## -3.810 -1.170 -0.248 0.947 8.390 ## ## Coefficients : ## Estimate Std. Error t-value Pr(&gt;|t|) ## pcap 2.27e-04 3.18e-05 7.14 2.2e-12 ## pc 4.18e-06 6.57e-06 0.64 0.52 ## ## Total Sum of Squares: 2770 ## Residual Sum of Squares: 2370 ## R-Squared: 0.144 ## Adj. R-Squared: 0.0889 ## F-statistic: 64.2646 on 2 and 766 DF, p-value: &lt;2e-16 We can extract the fixed-effect estimates themselves via fixef(). fixef(fit_fixed) ## ALABAMA ARIZONA ARKANSAS CALIFORNIA COLORADO ## 3.763902 3.201232 5.020980 -24.351816 1.906885 ## CONNECTICUT DELAWARE FLORIDA GEORGIA IDAHO ## 2.222801 5.723806 -2.950385 0.090978 5.926215 ## ILLINOIS INDIANA IOWA KANSAS KENTUCKY ## -7.483176 1.499181 1.176446 1.258062 2.487150 ## LOUISIANA MAINE MARYLAND MASSACHUSETTS MICHIGAN ## 2.394977 6.132823 -0.469424 -0.081008 -2.073261 ## MINNESOTA MISSISSIPPI MISSOURI MONTANA NEBRASKA ## -0.913146 4.790411 0.576112 5.398842 1.033804 ## NEVADA NEW_HAMPSHIRE NEW_JERSEY NEW_MEXICO NEW_YORK ## 6.121925 3.867146 -0.475632 5.875722 -22.416937 ## NORTH_CAROLINA NORTH_DAKOTA OHIO OKLAHOMA OREGON ## 0.588848 3.714982 -5.208327 2.212210 4.913671 ## PENNSYLVANIA RHODE_ISLAND SOUTH_CAROLINA SOUTH_DAKOTA TENNESSE ## -6.740590 6.045561 3.895325 2.942687 1.080415 ## TEXAS UTAH VERMONT VIRGINIA WASHINGTON ## -11.161384 4.233540 5.624496 -0.960964 1.080757 ## WEST_VIRGINIA WISCONSIN WYOMING ## 6.940764 0.191971 3.798969 If we wanted to include time dummies as well, we could specify effect = &quot;twoways&quot; in the fitting function. fit_fixed_2 &lt;- plm(unemp ~ pcap + pc, data = Produc, effect = &quot;twoways&quot;, model = &quot;within&quot;) summary(fit_fixed_2) ## Twoways effects Within Model ## ## Call: ## plm(formula = unemp ~ pcap + pc, data = Produc, effect = &quot;twoways&quot;, ## model = &quot;within&quot;) ## ## Balanced Panel: n=48, T=17, N=816 ## ## Residuals : ## Min. 1st Qu. Median 3rd Qu. Max. ## -3.3300 -0.8030 -0.0449 0.7500 6.0200 ## ## Coefficients : ## Estimate Std. Error t-value Pr(&gt;|t|) ## pcap 9.10e-05 2.62e-05 3.47 0.00054 ## pc -1.27e-05 5.04e-06 -2.51 0.01216 ## ## Total Sum of Squares: 1260 ## Residual Sum of Squares: 1240 ## R-Squared: 0.0164 ## Adj. R-Squared: -0.0689 ## F-statistic: 6.23517 on 2 and 750 DF, p-value: 0.00206 fixef(fit_fixed_2, effect = &quot;time&quot;) ## 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 ## 3.5548 4.2823 3.7710 3.1935 3.8319 6.5160 5.4577 4.9365 3.9381 3.8231 ## 1980 1981 1982 1983 1984 1985 1986 ## 5.1814 5.6677 7.7005 7.6848 5.6401 5.4863 5.3504 phtest() implements the Hausman test. Remember that the null hypothesis is that both estimators are consistent; if we reject it, then the random effects estimator is inconsistent and we must use fixed effects. phtest(fit_random, fit_fixed) ## ## Hausman Test ## ## data: unemp ~ pcap + pc ## chisq = 95.9, df = 2, p-value &lt;2e-16 ## alternative hypothesis: one model is inconsistent References "],
["references.html", "References", " References "]
]
