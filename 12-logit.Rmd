# Binary Response Models {#logit}

Many of the political phenomena we'd like to explain are dichotomous:

*   Whether someone votes.

*   Whether someones votes for a particular candidate.

*   Whether war occurs.

For a long time, the conventional wisdom among political scientists was that special statistical models were required to analyze binary responses like these.
In this section of the course, we'll discuss the reasoning behind this claim, work with the fancier statistical models that are purportedly necessary for binary responses, and discuss why the conventional wisdom about linear models might be wrong.


## The Linear Probability Model

So far when encountering binary responses, we have just modeled them as linear functions of covariates.
For example, think of voting for Trump as a function of the years of education one has received:
$$
\text{Trump}_n = \beta_0 + \beta_1 \text{Education}_n + \epsilon_n.
$$
In this model, $\beta_0$ represents the probability that someone with zero years of education would vote for Trump, and $\beta_1$ is the marginal effect of an additional year of education.

Suppose we yielded the following estimates from this model:
$$
\text{Trump}_n = 1.4 - 0.075 \text{Education}_n + \epsilon_n.
$$
Most Americans have between 8 and 16 years of formal education.
According to this regression equation, we should expect about 80% of those with only an eighth-grade education to vote for Trump, while we should expect only 20% of those with a college degree to vote for Trump.
These sound like reasonable numbers!

The problem is, not *every* American's educational attainment lies within this range.
Some Americans only have four years of formal education, and this model would predict that 110% of them voted for Trump.
Other Americans are crazy enough to pursue 20+ years of education, and this model would predict that --10% or fewer of them voted for Trump.

This illustrates one problem with linear probability models: they can produce impossible predicted probabilities, outside the $[0, 1]$ range that all probabilities must lie within.
In practice, such predictions usually occur for observations far away from the central tendency of the data.
(In fact, as you proved on a previous problem set, the predicted probability for an observation with an average value of all covariates must equal the sample proportion of 1s in the response.)

We can see this problem in action when we draw the OLS line through the scatterplot of some (hypothetical) data on educational attainment and vote choice.

```{r binary-scatterplot, echo=FALSE, message=FALSE}
set.seed(30931)
dat_binary <- tibble(
  education = rnorm(100, mean = 13, sd = 1.75),
  trump_latent = -1.5 * (education - 12) + rlogis(100),
  trump = as.numeric(trump_latent > 0)
)
ggplot(dat_binary, aes(x = education, y = trump)) +
  geom_smooth(method = "lm") +
  geom_point()
```

Another way to put all this is that the assumption of constant marginal effects doesn't make sense for a binary response.
If someone already has a 99% chance of voting for Trump, the most you could possibly raise their chance of voting for him is 1%.
This puts a cap on the magnitude of the effect of any given variable.
An intervention that raises an *average* voter's chance of voting for Trump by 5% cannot possibly have the same effect on our hypothetical Trump die-hard.
This is in contrast to settings with continuous outcomes.
A job training program that raises an average person's annual earnings by 2% is *unlikely* to have the same effect on Jeff Bezos, but it is at least *conceivable* it would do so.

The idea behind logistic regression (and its close friend probit regression) is to model the conditional expectation in a way that gets around these issues.
These models always produce sensible predicted probabilities, and they assume lower-magnitude marginal effects for observations at the extremes.
Instead of regression lines, they draw nice curves like the one below.

```{r binary-scatterplot-redux, echo=FALSE, message=FALSE}
ggplot(dat_binary, aes(x = education, y = trump)) +
  geom_smooth(method = "glm",
              method.args = list(family = binomial(link = "logit"))) +
  geom_point()
```


## The Logistic Regression Model

For a binary response, $Y_n \in \{0, 1\}$, we can think of the linear probability model as the following:
$$
\Pr(Y_n = 1) = \mathbf{x}_n \cdot \beta.
$$
Logistic regression replaces the linear formula on the right-hand side with a nonlinear S-curve, as illustrated above.
The logistic regression model is
$$
\Pr(Y_n = 1) = \frac{e^{\mathbf{x}_n \cdot \beta}}{1 + e^{\mathbf{x}_n \cdot \beta}} = \Lambda(\mathbf{x}_n \cdot \beta),
$$
where $\Lambda(z) = e^z / (1 + e^z)$.

How did we get the above formula?
We can tell a little story to provide a foundation for the logistic regression model.
For each observation $n = 1, \ldots, N$, let $Y_n^*$ denote the *latent* (unobservable) propensity to choose $Y_n = 1$.
We assume that the latent propensity follows a linear model,
$$
Y_n^* = \mathbf{x}_n \cdot \beta + \epsilon_n,
$$
where $\epsilon_n$ is independent and identically distributed across observations.
Furthermore, assume $\epsilon_n$ has a logistic distribution, so that $\Pr(\epsilon_n < z) = \Lambda(z)$ for all real numbers $z$.
Finally, we assume that we observe $Y_n = 1$ if and only if the latent propensity is non-negative:
$$
Y_n = \begin{cases}
0 & Y_n^* < 0, \\
1 & Y_n^* \geq 0.
\end{cases}
$$
The logistic regression model follows from this combination of assumptions.

I hope you notice that the assumptions we've made on the latent propensity $Y_n^*$ are even stronger than the usual linear model assumptions.
We assumed that $\epsilon_n$ was independent across observations, whereas the usual linear model allows for autocorrelation.
We assumed that $\epsilon_n$ was identically distributed across observations, whereas the usual linear model allows for heteroskedasticity.[^hetero-dist]
Finally, we assumed that $\epsilon_n$ has a logistic distribution, whereas the usual linear model does not assume a specific distributional form for the error term.
The verisimilitude of the S-curve comes at the cost of forcing us to make many more assumptions than we do with a linear model.

[^hetero-dist]: The differences actually go even further than allowing for heteroskedasticity.  We can have a homoskedastic linear model where the error variances are the same but their full distributions are different.  For example, in the usual linear model we could have $\epsilon_1 \sim U[0, 1]$ and $\epsilon_2 \sim N(0, 1/12)$, so that $V[\epsilon_1] = V[\epsilon_2] = 1/12$ even though their distributions differ.

Probit regression is the same as logistic regression, except we assume that the error term $\epsilon_n$ in the latent propensity equation has a standard normal distribution.
This results in the model
$$
\Pr(Y_n = 1) = \Phi(\mathbf{x}_n \cdot \beta),
$$
where $\Phi$ is the standard normal CDF.
In practice, logit and probit almost always yield nearly identical results in terms of predicted probabilities and marginal effects.
From here on we will keep working with the logistic model, solely because the logistic CDF is easier to work with mathematically than the normal CDF.

One of the nice things about the linear probability model is that it's easy to calculate marginal effects.
The estimated marginal effect of $x_{nk}$ on $\Pr(Y_n = 1)$ is simply its coefficient, $\beta_k$.
It is not so simple with logistic regression.
First we need the derivative of the logistic CDF.
Surely you remember the quotient rule, which gives us
$$
\Lambda'(z)
= \frac{e^z (1 + e^z) - e^z (e^z)}{(1 + e^z)^2}
= \frac{e^z}{1 + e^z} \times \frac{1}{1 + e^z}
= \Lambda(z) [1 - \Lambda(z)].
$$
And since you surely also remember the chain rule, you can calculate the marginal effect of $x_{nk}$ on $\Pr(Y_n = 1)$ as
$$
\begin{aligned}
\frac{\partial \Pr(Y_n = 1)}{x_{nk}}
&= \Lambda'(\mathbf{x}_k \cdot \beta) \frac{\partial [\mathbf{x}_k \cdot \beta]}{x_{nk}} \\
&= \Lambda(\mathbf{x}_k \cdot \beta) [1 - \Lambda(\mathbf{x}_k \cdot \beta)] \beta_k \\
&= \Pr(Y_n = 1) \Pr(Y_n = 0) \beta_k.
\end{aligned}
$$
This gives us the properties we wanted---that the marginal effect of a variable is lowest in magnitude for those observations that are already highly likely or highly unlikely to have $Y_n = 1$.

*   If $\Pr(Y_n = 1) \approx 0$, then the marginal effect of $x_{nk}$ is approximately zero.

*   If $\Pr(Y_n = 1) = 0.5$, then the marginal effect of $x_{nk}$ is $\beta_k / 4$.

*   If $\Pr(Y_n = 1) \approx 1$, then the marginal effect of $x_{nk}$ is approximately zero.

This gives us a helpful rule for making sense of logistic regression output.
You can divide each coefficient by 4, and that gives you its marginal effect on an observation that's at the 50-50 point.

```{r logit-margeff, echo=FALSE}
dat_logit_margeff <- tibble(p = seq(0, 1, length.out = 101),
                            me = p * (1 - p))
ggplot(dat_logit_margeff, aes(x = p, y = me)) +
  geom_line() +
  scale_x_continuous("Pr(Y = 1 | x)") +
  scale_y_continuous("Marginal effect of x_k",
                     breaks = c(0, 1/8, 1/4),
                     labels = c("0", "beta_k / 8", "beta_k / 4"))
```

Briefly --- for a probit model, the marginal effect is
$$
\frac{\partial \Pr(Y_n = 1)}{x_{nk}} = \phi(\mathbf{x}_n \cdot \beta) \beta_k,
$$
where $\phi$ is the standard normal PDF.
Since $\phi(0) \approx 0.4$, the marginal effect for an observation at the 50-50 point is roughly 0.4 times its coefficient.

For either logit or probit, if you want to calculate the average marginal effect of $X_k$ across your dataset, you may be tempted to calculate the marginal effect for an "average" observation---one with mean or median values for all the covariates.
Resist the temptation.
In both cases, the marginal effect is a nonlinear function of $x_{nk}$, and a not-so-fun fact about nonlinear functions is that $E[f(z)] \neq f(E[z])$.
If you want the average marginal effect, you need to calculate it individually for each observation and then average them:
$$
\text{average m.e. of $X_k$}
= \frac{\beta_k}{N} \sum_{i=1}^N \Lambda(\mathbf{x}_n \cdot \beta) [1 - \Lambda(\mathbf{x}_n \cdot \beta)]
$$

For binary covariates, we will usually focus on the first difference rather than the marginal effect, as it doesn't make much sense to take a derivative with respect to a dichotomous indicator.
Assuming $X_k$ is binary, the first difference for the $n$'th observation is
$$
\Pr(Y_n = 1) - \Pr(Y_n = 0) =
\Lambda(\tilde{\mathbf{x}}_n \cdot \tilde{\beta} + \beta_k)
- \Lambda(\tilde{\mathbf{x}}_n \cdot \tilde{\beta}),
$$
where $\tilde{\mathbf{x}}_n$ is the vector of all covariates but the $k$'th, and $\tilde{\beta}$ is the vector of all coefficients but the $k$'th.
As with marginal effects, to calculate the average first difference across the data, you must calculate it for each observation individually and then average them---no shortcuts.
